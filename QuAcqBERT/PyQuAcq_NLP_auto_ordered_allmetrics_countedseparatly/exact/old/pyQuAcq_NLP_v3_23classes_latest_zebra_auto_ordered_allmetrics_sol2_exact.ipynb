{
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fsvcD4UEQxTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzVaWjnlfb1I"
      },
      "source": [
        "# pyQuAcqV7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jabFMsmEhKBV"
      },
      "source": [
        "## Install necessary tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PZc-E0ZZgiPf"
      },
      "outputs": [],
      "source": [
        "#!pip install ortools\n",
        "!pip install exact\n",
        "#!pip install python-sat\n",
        "!pip install cpmpy\n",
        "!pip install gdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eNPWIR2PRUSq"
      },
      "outputs": [],
      "source": [
        "import cpmpy as cp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXwibIv4hONW"
      },
      "source": [
        "## Define operators"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NL3k9PRNglwU"
      },
      "outputs": [],
      "source": [
        "\n",
        "import operator\n",
        "# Here define constraints\n",
        "# this is the relation: a+b!=c\n",
        "def someAndDiff(a,b,c):\n",
        "  return operator.ne( operator.add(a,b) , c )\n",
        "\n",
        "def alldiff():\n",
        "  pass\n",
        "\n",
        "def allequ():\n",
        "  pass\n",
        "\n",
        "def eqDist1(x,y):\n",
        "\n",
        "  return operator.eq(operator.abs(operator.sub(x,y)), 1)\n",
        "\n",
        "def neDist1(x,y):\n",
        "\n",
        "  return operator.ne(operator.abs(operator.sub(x,y)), 1)\n",
        "\n",
        "\n",
        "\n",
        "def eqDist(a,b,c,d):\n",
        "  return operator.eq( operator.abs(operator.sub(a,b)) ,  operator.abs(operator.sub(c,d)))\n",
        "\n",
        "def neDist(a,b,c,d):\n",
        "  return operator.ne( operator.abs(operator.sub(a,b)) ,  operator.abs(operator.sub(c,d)))\n",
        "\n",
        "\n",
        "def eqDist3(repeat,b,c):\n",
        "  return operator.eq( operator.abs(operator.sub(repeat,b)) ,  operator.abs(operator.sub(c,repeat)))\n",
        "\n",
        "def neDist3(repeat,b,c):\n",
        "  return operator.ne( operator.abs(operator.sub(repeat,b)) ,  operator.abs(operator.sub(c,repeat)))\n",
        "\n",
        "\n",
        "\n",
        "vals = [1,2,3,4,5]\n",
        "ops = [operator.ge,operator.le,operator.lt,operator.gt,operator.ne,operator.eq]\n",
        "\n",
        "\n",
        "def eqConst(x, const):\n",
        "  return operator.eq(x,const)\n",
        "\n",
        "def geConst(x, const):\n",
        "  return operator.ge(x,const)\n",
        "\n",
        "def leConst(x, const):\n",
        "  return operator.le(x,const)\n",
        "\n",
        "def ltConst(x, const):\n",
        "  return operator.lt(x,const)\n",
        "\n",
        "def gtConst(x, const):\n",
        "  return operator.gt(x,const)\n",
        "\n",
        "def neConst(x, const):\n",
        "  return operator.ne(x,const)\n",
        "\n",
        "\n",
        "def DistgtValue(x,y, const):\n",
        "  return operator.gt(operator.abs(operator.sub(x,y)),const)\n",
        "\n",
        "def DistgeValue(x,y, const):\n",
        "  return operator.ge(operator.abs(operator.sub(x,y)),const)\n",
        "\n",
        "def DistltValue(x,y, const):\n",
        "  return operator.lt(operator.abs(operator.sub(x,y)),const)\n",
        "\n",
        "def DistleValue(x,y, const):\n",
        "  return operator.le(operator.abs(operator.sub(x,y)),const)\n",
        "\n",
        "def DistneValue(x,y, const):\n",
        "  return operator.ne(operator.abs(operator.sub(x,y)),const)\n",
        "\n",
        "def DisteqValue(x,y, const):\n",
        "  return operator.eq(operator.abs(operator.sub(x,y)),const)\n",
        "\n",
        "\n",
        "def eqDist(a,b,c,d):\n",
        "  return operator.eq( operator.abs(operator.sub(a,b)) ,  operator.abs(operator.sub(c,d)))\n",
        "\n",
        "def neDist(a,b,c,d):\n",
        "  return operator.ne( operator.abs(operator.sub(a,b)) ,  operator.abs(operator.sub(c,d)))\n",
        "\n",
        "def eqDist3(repeat,b,c):\n",
        "  return operator.eq( operator.abs(operator.sub(repeat,b)) ,  operator.abs(operator.sub(c,repeat)))\n",
        "\n",
        "def neDist3(repeat,b,c):\n",
        "  return operator.ne( operator.abs(operator.sub(repeat,b)) ,  operator.abs(operator.sub(c,repeat)))\n",
        "\n",
        "\n",
        "def gtDist(a,b,c,d):\n",
        "  return operator.gt( operator.abs(operator.sub(a,b)) ,  operator.abs(operator.sub(c,d)))\n",
        "\n",
        "def geDist(a,b,c,d):\n",
        "  return operator.ge( operator.abs(operator.sub(a,b)) ,  operator.abs(operator.sub(c,d)))\n",
        "\n",
        "def gtDist3(repeat,b,c):\n",
        "  return operator.gt( operator.abs(operator.sub(repeat,b)) ,  operator.abs(operator.sub(c,repeat)))\n",
        "\n",
        "def geDist3(repeat,b,c):\n",
        "  return operator.ge( operator.abs(operator.sub(repeat,b)) ,  operator.abs(operator.sub(c,repeat)))\n",
        "\n",
        "\n",
        "def ltDist(a,b,c,d):\n",
        "  return operator.lt( operator.abs(operator.sub(a,b)) ,  operator.abs(operator.sub(c,d)))\n",
        "\n",
        "def leDist(a,b,c,d):\n",
        "  return operator.le( operator.abs(operator.sub(a,b)) ,  operator.abs(operator.sub(c,d)))\n",
        "\n",
        "def ltDist3(repeat,b,c):\n",
        "  return operator.lt( operator.abs(operator.sub(repeat,b)) ,  operator.abs(operator.sub(c,repeat)))\n",
        "\n",
        "def leDist3(repeat,b,c):\n",
        "  return operator.le( operator.abs(operator.sub(repeat,b)) ,  operator.abs(operator.sub(c,repeat)))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9h-l966Hdt4Z"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3uK3M_AhVHo"
      },
      "source": [
        "## Projection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t9QpBpZRhZ1j"
      },
      "outputs": [],
      "source": [
        "def projection(e,vars_ids):\n",
        "  # this is a projection of Vars on assignement: return the part of the assignement\n",
        "  # including only Vars\n",
        "\n",
        "  ids = [ee[0] for ee in e]\n",
        "  els = { ee[0]: ee for ee in e}\n",
        "  p = []\n",
        "\n",
        "  for id in vars_ids:\n",
        "    if id in ids:\n",
        "      p.append(els[id])\n",
        "\n",
        "  return p"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ofQk4xD5dyDI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMi-6KSkhZvi"
      },
      "source": [
        "## Constraint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "00NGqCUtPKq5"
      },
      "outputs": [],
      "source": [
        "import operator\n",
        "from copy import deepcopy\n",
        "class Constraint:\n",
        "    # this class represents a constraint\n",
        "\n",
        "    def __init__(self ,scope_ids , rel, arity, isCommutative , parameters = None, isNegation=False, isRedundant=False):\n",
        "      \"\"\"\n",
        "          scope_ids : list of variable ids (e.g. [1,2])\n",
        "          rel : the relation of this constraint (e.g. operator.ne)\n",
        "          arity : the arity of the constraint (e.g. 2)\n",
        "          isCommutative : is this constraint commutative (e.g. True (A != B <=> B != A))\n",
        "          parameters : list of parameters of this constraint (e.g. A>1 the parameters = [1])\n",
        "          isNegation : whether this constraint is a negation of the constraint with the same parameters, in other words\n",
        "          all parameters are the same except the relation which will be replaced by the opposit relation, this will be\n",
        "          implemented with returning the not of verify later\n",
        "      \"\"\"\n",
        "      assert isinstance(scope_ids,list), \"The scope must be a list of variable ids\"\n",
        "      for x in scope_ids:\n",
        "        assert isinstance(x,int), \"The scope must be a list of variable ids\"\n",
        "\n",
        "      self.scope_ids = scope_ids\n",
        "\n",
        "      self.isCommutative = isCommutative\n",
        "\n",
        "      #The relation must be an operator\n",
        "      assert callable(rel), \"The relation must be an operator\"\n",
        "      self.rel = rel\n",
        "\n",
        "      # the arity is the number of variables in the scope, an arity could be binary, ternary ...\n",
        "      self.arity = arity\n",
        "\n",
        "      self.parameters = parameters\n",
        "\n",
        "      self.id = id(self)\n",
        "\n",
        "      self.isNegation = isNegation\n",
        "      self.isRedundant = isRedundant\n",
        "\n",
        "    def verify(self, assignement):\n",
        "      # this method checks whether the constraint accepts an assignement (True) or rejects it (False)\n",
        "\n",
        "      # we project the assignement on the scope of the constraint\n",
        "      p = projection(assignement, self.scope_ids)\n",
        "\n",
        "      # get values of vars in the assignement\n",
        "      #print(\"P: \", p)\n",
        "      if(p==[]):\n",
        "        return True # the constraint is not concerned with this assignement\n",
        "\n",
        "      values = [pp[2] for pp in p]\n",
        "      # print(\"values hereeee _________\",values)\n",
        "      # print(\"Self\", self)\n",
        "      # print(\"self.rel\", self.rel)\n",
        "      # print(\"self.scope_ids\", self.scope_ids)\n",
        "\n",
        "      if(len(values) != self.arity and self.arity != None):\n",
        "        # if the number of the values is different from the arity then just return True because\n",
        "        # the constraint is not concerned, in general it is unlikely to get here because we do the projection\n",
        "        # but to avoid errors we keep it here\n",
        "        return True\n",
        "\n",
        "      if(self.parameters == None):\n",
        "        if(len(values)>1):\n",
        "          if(self.isNegation):\n",
        "            return not self.rel(*values) # we check if the relation accepts the values given to the variables, if so return not the result because it is a negation\n",
        "          else:\n",
        "            return self.rel(*values) # we check if the relation accepts the values given to the variables\n",
        "        else:\n",
        "          if(self.isNegation):\n",
        "            return not self.rel(values[0])\n",
        "          else:\n",
        "            return self.rel(values[0])\n",
        "      else:\n",
        "        if(len(values)>1):\n",
        "          if(self.isNegation):\n",
        "            return not self.rel(*values, *self.parameters)\n",
        "          else:\n",
        "            return self.rel(*values, *self.parameters)\n",
        "\n",
        "        else:\n",
        "          if(len(self.parameters)>1):\n",
        "            if(self.isNegation):\n",
        "              return not self.rel(values[0], *self.parameters)\n",
        "            else:\n",
        "              return self.rel(values[0], *self.parameters)\n",
        "          else:\n",
        "            if(self.isNegation):\n",
        "              return not self.rel(values[0], self.parameters[0])\n",
        "            else:\n",
        "              return self.rel(values[0], self.parameters[0])\n",
        "\n",
        "    def prepareCpModelConstraint(self, vars, model):\n",
        "      tmp = []\n",
        "      for  i in self.scope_ids:\n",
        "        tmp.append(vars[i])\n",
        "\n",
        "      if(self.rel == alldiff): #the alldifferent constraint\n",
        "        if(self.isNegation):\n",
        "          model += ~ cp.AllDifferent(tmp)\n",
        "        else:\n",
        "          model += cp.AllDifferent(tmp)\n",
        "\n",
        "      elif(self.rel == allequ):\n",
        "        if(self.isNegation):\n",
        "          model += ~ cp.AllEqual(tmp)\n",
        "        else:\n",
        "          model += cp.AllEqual(tmp)\n",
        "\n",
        "      elif(self.rel == eqDist1):\n",
        "        if(self.isNegation):\n",
        "          model += ~ (abs(tmp[0] - tmp[1]) == 1)\n",
        "        else:\n",
        "          model += abs(tmp[0] - tmp[1]) == 1\n",
        "\n",
        "      elif(self.rel == neDist1):\n",
        "        if(self.isNegation):\n",
        "          model += ~(abs(tmp[0] - tmp[1]) != 1)\n",
        "        else:\n",
        "          model += abs(tmp[0] - tmp[1]) != 1\n",
        "\n",
        "      elif(self.rel in [eqConst, neConst , ltConst , leConst, gtConst, geConst]):\n",
        "        if(self.isNegation):\n",
        "          model += ~(self.rel(*tmp, *self.parameters))\n",
        "        else:\n",
        "          model += self.rel(*tmp, *self.parameters)\n",
        "\n",
        "      else:\n",
        "        if(self.parameters!=None):\n",
        "          if(self.isNegation):\n",
        "            model += ~(self.rel(*tmp, *self.parameters))\n",
        "          else:\n",
        "            model += self.rel(*tmp, *self.parameters)\n",
        "        else:\n",
        "\n",
        "          if(self.isNegation):\n",
        "            model += ~(self.rel(*tmp))  # add the constraint to the model specifying the variables\n",
        "          else:\n",
        "            model += self.rel(*tmp)\n",
        "\n",
        "    def returnCpModelConstraint(self, vars):\n",
        "      tmp = []\n",
        "      for  i in self.scope_ids:\n",
        "        tmp.append(vars[i])\n",
        "\n",
        "      if(self.rel == alldiff): #the alldifferent constraint\n",
        "        if(self.isNegation):\n",
        "          return ~ cp.AllDifferent(tmp)\n",
        "        else:\n",
        "          return cp.AllDifferent(tmp)\n",
        "\n",
        "      elif(self.rel == allequ):\n",
        "        if(self.isNegation):\n",
        "          return ~ cp.AllEqual(tmp)\n",
        "        else:\n",
        "          return cp.AllEqual(tmp)\n",
        "\n",
        "      elif(self.rel == eqDist1):\n",
        "        if(self.isNegation):\n",
        "          return ~ (abs(tmp[0] - tmp[1]) == 1)\n",
        "        else:\n",
        "          return abs(tmp[0] - tmp[1]) == 1\n",
        "\n",
        "      elif(self.rel == neDist1):\n",
        "        if(self.isNegation):\n",
        "          return ~(abs(tmp[0] - tmp[1]) != 1)\n",
        "        else:\n",
        "          return abs(tmp[0] - tmp[1]) != 1\n",
        "\n",
        "      elif(self.rel in [eqConst, neConst , ltConst , leConst, gtConst, geConst]):\n",
        "        if(self.isNegation):\n",
        "          return ~(self.rel(*tmp, *self.parameters))\n",
        "        else:\n",
        "          return self.rel(*tmp, *self.parameters)\n",
        "\n",
        "      else:\n",
        "        if(self.parameters!=None):\n",
        "          if(self.isNegation):\n",
        "            return ~(self.rel(*tmp, *self.parameters))\n",
        "          else:\n",
        "            return self.rel(*tmp, *self.parameters)\n",
        "        else:\n",
        "\n",
        "          if(self.isNegation):\n",
        "            return ~(self.rel(*tmp))  # add the constraint to the model specifying the variables\n",
        "          else:\n",
        "            return self.rel(*tmp)\n",
        "\n",
        "    # def prepareCpModelConstraintReification(self, vars_dict, model, booll):\n",
        "\n",
        "    #   tmp = []\n",
        "    #   for  i in self.scope_ids:\n",
        "    #     tmp.append(vars_dict[i])\n",
        "\n",
        "    #   if(self.rel == alldiff): #the alldifferent constraint\n",
        "    #     if(self.isNegation):\n",
        "    #       model += (~ cp.AllDifferent(tmp)).implies(booll)\n",
        "    #       model += booll.implies((~ cp.AllDifferent(tmp)))\n",
        "\n",
        "\n",
        "    #     else:\n",
        "    #       model += (cp.AllDifferent(tmp)).implies(booll)\n",
        "    #       model += booll.implies(cp.AllDifferent(tmp))\n",
        "\n",
        "\n",
        "\n",
        "    #   elif(self.rel == allequ):\n",
        "    #     if(self.isNegation):\n",
        "    #       model +=  (~cp.AllEqual(tmp)).implies(booll)\n",
        "    #       model +=  booll.implies(~cp.AllEqual(tmp))\n",
        "\n",
        "\n",
        "    #     else:\n",
        "    #       model += cp.AllEqual(tmp).implies(booll)\n",
        "    #       model += booll.implies(cp.AllEqual(tmp))\n",
        "\n",
        "\n",
        "    #   elif(self.rel == eqDist1):\n",
        "    #     if(self.isNegation):\n",
        "    #       model +=  (~(abs(tmp[0] - tmp[1]) == 1)).implies(booll)\n",
        "    #       model +=  booll.implies(~(abs(tmp[0] - tmp[1]) == 1))\n",
        "\n",
        "\n",
        "    #     else:\n",
        "    #       model+= (abs(tmp[0] - tmp[1]) == 1).implies(booll)\n",
        "    #       model +=booll.implies(abs(tmp[0] - tmp[1]) == 1)\n",
        "\n",
        "\n",
        "\n",
        "    #   elif(self.rel == neDist1):\n",
        "    #     if(self.isNegation):\n",
        "    #       model += (~(abs(tmp[0] - tmp[1]) != 1)).implies(booll)\n",
        "    #       model += booll.implies(~(abs(tmp[0] - tmp[1]) != 1))\n",
        "\n",
        "\n",
        "    #     else:\n",
        "    #       model += (abs(tmp[0] - tmp[1]) != 1).implies(booll)\n",
        "    #       model += booll.implies(abs(tmp[0] - tmp[1]) != 1)\n",
        "\n",
        "\n",
        "\n",
        "    #   elif(self.rel in [eqConst, neConst , ltConst , leConst, gtConst, geConst]):\n",
        "    #     if(self.isNegation):\n",
        "    #       model += (~(self.rel(*tmp, *self.parameters))).implies(booll)\n",
        "    #       model += booll.implies(~(self.rel(*tmp, *self.parameters)))\n",
        "\n",
        "\n",
        "    #     else:\n",
        "    #       model += (self.rel(*tmp, *self.parameters)).implies(booll)\n",
        "    #       model += booll.implies(self.rel(*tmp, *self.parameters))\n",
        "\n",
        "\n",
        "\n",
        "    #   else:\n",
        "    #     if(self.parameters!=None):\n",
        "    #       if(self.isNegation):\n",
        "    #         model += (~(self.rel(*tmp, *self.parameters))).implies(booll)\n",
        "    #         model += booll.implies(~(self.rel(*tmp, *self.parameters)))\n",
        "\n",
        "\n",
        "    #       else:\n",
        "    #         model += (self.rel(*tmp, *self.parameters)).implies(booll)\n",
        "    #         model += booll.implies(self.rel(*tmp, *self.parameters))\n",
        "\n",
        "\n",
        "    #     else:\n",
        "    #       if(self.isNegation):\n",
        "    #         model += (~(self.rel(*tmp))).implies(booll)\n",
        "    #         model += booll.implies(~(self.rel(*tmp)))  # add the constraint to the model specifying the variables\n",
        "\n",
        "\n",
        "    #       else:\n",
        "    #         model += (self.rel(*tmp)).implies(booll)\n",
        "    #         model += booll.implies(self.rel(*tmp))\n",
        "\n",
        "\n",
        "    def prepareCpModelConstraintReification(self, vars_dict, model, booll):\n",
        "\n",
        "      tmp = []\n",
        "      for  i in self.scope_ids:\n",
        "        tmp.append(vars_dict[i])\n",
        "\n",
        "      if(self.rel == alldiff): #the alldifferent constraint\n",
        "        if(self.isNegation):\n",
        "          model += cp.all([(~ cp.AllDifferent(tmp)).implies(booll), booll.implies((~ cp.AllDifferent(tmp)))])\n",
        "\n",
        "\n",
        "        else:\n",
        "          model+= cp.all([(cp.AllDifferent(tmp)).implies(booll),booll.implies(cp.AllDifferent(tmp))])\n",
        "\n",
        "\n",
        "\n",
        "      elif(self.rel == allequ):\n",
        "        if(self.isNegation):\n",
        "          model += cp.all([(~cp.AllEqual(tmp)).implies(booll),booll.implies(~cp.AllEqual(tmp))])\n",
        "\n",
        "\n",
        "        else:\n",
        "          model+= cp.all([cp.AllEqual(tmp).implies(booll),booll.implies(cp.AllEqual(tmp))])\n",
        "\n",
        "\n",
        "      elif(self.rel == eqDist1):\n",
        "        if(self.isNegation):\n",
        "          model+= cp.all([(~(abs(tmp[0] - tmp[1]) == 1)).implies(booll),booll.implies(~(abs(tmp[0] - tmp[1]) == 1))])\n",
        "\n",
        "\n",
        "        else:\n",
        "          model+= cp.all([(abs(tmp[0] - tmp[1]) == 1).implies(booll),booll.implies(abs(tmp[0] - tmp[1]) == 1)])\n",
        "\n",
        "\n",
        "\n",
        "      elif(self.rel == neDist1):\n",
        "        if(self.isNegation):\n",
        "          model += cp.all([(~(abs(tmp[0] - tmp[1]) != 1)).implies(booll),booll.implies(~(abs(tmp[0] - tmp[1]) != 1))])\n",
        "\n",
        "        else:\n",
        "          model += cp.all([(abs(tmp[0] - tmp[1]) != 1).implies(booll),booll.implies(abs(tmp[0] - tmp[1]) != 1)])\n",
        "\n",
        "\n",
        "\n",
        "      elif(self.rel in [eqConst, neConst , ltConst , leConst, gtConst, geConst]):\n",
        "        if(self.isNegation):\n",
        "          model+= cp.all([(~(self.rel(*tmp, *self.parameters))).implies(booll),booll.implies(~(self.rel(*tmp, *self.parameters)))])\n",
        "\n",
        "\n",
        "        else:\n",
        "          model += cp.all([(self.rel(*tmp, *self.parameters)).implies(booll),booll.implies(self.rel(*tmp, *self.parameters))])\n",
        "\n",
        "\n",
        "\n",
        "      else:\n",
        "        if(self.parameters!=None):\n",
        "          if(self.isNegation):\n",
        "            model += cp.all([(~(self.rel(*tmp, *self.parameters))).implies(booll),booll.implies(~(self.rel(*tmp, *self.parameters)))])\n",
        "\n",
        "          else:\n",
        "            model += cp.all([(self.rel(*tmp, *self.parameters)).implies(booll),booll.implies(self.rel(*tmp, *self.parameters))])\n",
        "\n",
        "\n",
        "        else:\n",
        "          if(self.isNegation):\n",
        "            model += cp.all([(~(self.rel(*tmp))).implies(booll),booll.implies(~(self.rel(*tmp)))])\n",
        "\n",
        "          else:\n",
        "            model += cp.all([(self.rel(*tmp)).implies(booll),booll.implies(self.rel(*tmp))])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def isEqualTo(self, constraint):\n",
        "\n",
        "      # vars_ids, vars_domains, vars_types Are globals\n",
        "\n",
        "      # if(not isinstance(constraint, Constraint)):\n",
        "      #   return False\n",
        "\n",
        "      if(isinstance(constraint, Conjunction)):\n",
        "        conjScp = set(constraint.scope)\n",
        "        if(set(self.scope_ids) != conjScp):\n",
        "          return False\n",
        "\n",
        "        v = set(self.scope_ids).union(conjScp)\n",
        "\n",
        "        # #print(\"\\n The union of the scope: \", v)\n",
        "        prob_data = {}\n",
        "        for vv in v:\n",
        "          prob_data[vv] = problem_data[vv]\n",
        "          #prob_data[vv][\"domain\"] = (0,len(v))\n",
        "\n",
        "        # # print(\"\\n The prob_data: \", prob_data.keys())\n",
        "\n",
        "        # # print(\"\\n The lens are equal ?: \", len(prob_data.keys())==len(v) )\n",
        "\n",
        "        n1 = Network(\n",
        "                 prob_data,\n",
        "                 [self])\n",
        "\n",
        "        n2 = Network(\n",
        "                      prob_data,\n",
        "                      [constraint])\n",
        "\n",
        "        e = n1.solve()\n",
        "\n",
        "        if(not n2.isAccepted(e)):\n",
        "          return False\n",
        "\n",
        "        e = n2.solve()\n",
        "\n",
        "        if(not n1.isAccepted(e)):\n",
        "          return False\n",
        "\n",
        "        notself = deepcopy(self)\n",
        "        notself.isNegation = True\n",
        "\n",
        "        notconstraint = deepcopy(constraint)\n",
        "        notconstraint.isNegation = True\n",
        "\n",
        "        if(n1.addConstraint(notconstraint).solve() ==[] and n2.addConstraint(notself).solve() == [] ):\n",
        "          return True\n",
        "\n",
        "        return False\n",
        "\n",
        "\n",
        "        # # return listOfSolsEqual(n1.getAllSolutions(), n2.getAllSolutions())\n",
        "\n",
        "        # # elementaryConsts = deepcopy(constraint.elementaryConstraints)\n",
        "        # notc = deepcopy(self)\n",
        "        # notc.isNegation = True\n",
        "\n",
        "        # # # print(\"\\nElementary constraints of the conjunction: \\n\")\n",
        "        # # # for c in elementaryConsts:\n",
        "        # # #   print(c.rel, c.scope_ids)\n",
        "\n",
        "        # # elementaryConsts.append(notc)\n",
        "        # # n = Network(prob_data, elementaryConsts)\n",
        "        # n = Network(prob_data, [constraint,notc])\n",
        "\n",
        "\n",
        "        # # # print(\"Notc: \\n\")\n",
        "        # # # print(notc.rel, notc.scope_ids, notc.isNegation)\n",
        "\n",
        "        # # # print(n.solve())\n",
        "        # # # print(input())\n",
        "        # if(n.solve()==[]):\n",
        "        #   return True\n",
        "        # else:\n",
        "        #   return False\n",
        "\n",
        "      if self.id == constraint.id:\n",
        "        return True\n",
        "      else:\n",
        "\n",
        "        conjScp = set(constraint.scope_ids)\n",
        "        if(set(self.scope_ids) != conjScp):\n",
        "          return False\n",
        "\n",
        "        v = set(self.scope_ids).union(conjScp)\n",
        "\n",
        "        # #print(\"\\n The union of the scope: \", v)\n",
        "        prob_data = {}\n",
        "        for vv in v:\n",
        "          prob_data[vv] = problem_data[vv]\n",
        "          #prob_data[vv][\"domain\"] = (0,len(v))\n",
        "\n",
        "        # # print(\"\\n The prob_data: \", prob_data.keys())\n",
        "\n",
        "        # # print(\"\\n The lens are equal ?: \", len(prob_data.keys())==len(v) )\n",
        "\n",
        "        n1 = Network(\n",
        "                  prob_data,\n",
        "                  [self])\n",
        "\n",
        "        n2 = Network(\n",
        "                      prob_data,\n",
        "                      [constraint])\n",
        "\n",
        "        e = n1.solve()\n",
        "\n",
        "        if(not n2.isAccepted(e)):\n",
        "          return False\n",
        "\n",
        "        e = n2.solve()\n",
        "\n",
        "        if(not n1.isAccepted(e)):\n",
        "          return False\n",
        "\n",
        "        notself = deepcopy(self)\n",
        "        notself.isNegation = True\n",
        "\n",
        "        notconstraint = deepcopy(constraint)\n",
        "        notconstraint.isNegation = True\n",
        "\n",
        "        if(n1.addConstraint(notconstraint).solve() ==[] and n2.addConstraint(notself).solve() == [] ):\n",
        "          return True\n",
        "\n",
        "        return False\n",
        "\n",
        "\n",
        "def listOfSolsEqual(l1,l2):\n",
        "\n",
        "  if(l1 == [] and l2 == []):\n",
        "    return True\n",
        "\n",
        "  if(l1 != [] and l2 == []):\n",
        "    return False\n",
        "\n",
        "  if(l1 == [] and l2 != []):\n",
        "    return False\n",
        "\n",
        "  if len(l1) != len(l2):\n",
        "    return False\n",
        "\n",
        "\n",
        "  cpt1=0\n",
        "  for a in l1:\n",
        "    if a in l2:\n",
        "      cpt1+=1\n",
        "\n",
        "  cpt2=0\n",
        "  for a in l2:\n",
        "    if a in l1:\n",
        "      cpt2+=1\n",
        "  if(cpt1 == len(l1) and cpt2 == len(l2)):\n",
        "    return True\n",
        "\n",
        "  return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FbtW9G8fd3QY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKjgLz63hZjC"
      },
      "source": [
        "## Conjunction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QWXApeqMhZdD"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "class Conjunction():\n",
        "\n",
        "  def __init__(self, constraints, isNegation =False, isRedundant=False):\n",
        "    # a conjunction is the AND of constraints\n",
        "\n",
        "    self.scope_ids = [c.scope_ids for c in constraints]\n",
        "\n",
        "    self.rel = [c.rel for c in constraints]\n",
        "\n",
        "    self.constraints = constraints\n",
        "\n",
        "    self.elementaryConstraints = self.AllElementaryConstraints()\n",
        "\n",
        "    self.scope = self.getScope()\n",
        "\n",
        "    #self.id = sum([c.id for c in self.constraints])\n",
        "    self.id = id(self)\n",
        "\n",
        "    self.isNegation = isNegation\n",
        "    self.isRedundant= isRedundant\n",
        "\n",
        "  def getScope(self):\n",
        "\n",
        "    scopes = []\n",
        "\n",
        "    for c in self.constraints:\n",
        "      if isinstance(c,Constraint):\n",
        "        scopes.append(set(c.scope_ids))\n",
        "      else:\n",
        "        scopes.append(set(c.scope))\n",
        "\n",
        "    ss = list(set.union(*scopes))\n",
        "\n",
        "    return list(np.unique(ss))\n",
        "\n",
        "\n",
        "  def AllElementaryConstraints(self):\n",
        "\n",
        "    elementaryConstraints = []\n",
        "\n",
        "    for c in self.constraints:\n",
        "      if isinstance(c,Constraint):\n",
        "        elementaryConstraints.append(c)\n",
        "      else:\n",
        "        elementaryConstraints+= c.elementaryConstraints\n",
        "\n",
        "    return elementaryConstraints\n",
        "\n",
        "\n",
        "  def isEqualTo(self, conj):\n",
        "\n",
        "    # if(not isinstance(conj, Conjunction)):\n",
        "    #   return False\n",
        "\n",
        "    if(isinstance(conj, Constraint)):\n",
        "      return conj.isEqualTo(self)\n",
        "\n",
        "    if self.id == conj.id:\n",
        "      return True\n",
        "    else:\n",
        "      conjScp = set(conj.scope)\n",
        "      if(set(self.scope) != conjScp):\n",
        "        return False\n",
        "\n",
        "      v = set(self.scope).union(conjScp)\n",
        "\n",
        "      # #print(\"\\n The union of the scope: \", v)\n",
        "      prob_data = {}\n",
        "      for vv in v:\n",
        "        prob_data[vv] = problem_data[vv]\n",
        "        #prob_data[vv][\"domain\"] = (0,len(v))\n",
        "\n",
        "      # # print(\"\\n The prob_data: \", prob_data.keys())\n",
        "\n",
        "      # # print(\"\\n The lens are equal ?: \", len(prob_data.keys())==len(v) )\n",
        "\n",
        "      n1 = Network(\n",
        "                prob_data,\n",
        "                [self])\n",
        "\n",
        "      n2 = Network(\n",
        "                    prob_data,\n",
        "                    [conj])\n",
        "\n",
        "      e = n1.solve()\n",
        "\n",
        "      if(not n2.isAccepted(e)):\n",
        "        return False\n",
        "\n",
        "      e = n2.solve()\n",
        "\n",
        "      if(not n1.isAccepted(e)):\n",
        "        return False\n",
        "\n",
        "      notself = deepcopy(self)\n",
        "      notself.isNegation = True\n",
        "\n",
        "      notconstraint = deepcopy(conj)\n",
        "      notconstraint.isNegation = True\n",
        "\n",
        "      if(n1.addConstraint(notconstraint).solve() ==[] and n2.addConstraint(notself).solve() == [] ):\n",
        "        return True\n",
        "\n",
        "      return False\n",
        "\n",
        "\n",
        "\n",
        "  def verify(self, assignement,liste = []):\n",
        "    # for a conjunction to accept an assignement, all constraints from which\n",
        "    # it is built must accept it (i.e there is an AND between these constraints )\n",
        "\n",
        "    liste1 = []\n",
        "    for c in self.constraints:\n",
        "      if(isinstance(c,Constraint)):\n",
        "        liste1.append(c.verify(assignement))\n",
        "\n",
        "    liste2 = []\n",
        "    for c in self.constraints:\n",
        "      if(not isinstance(c,Constraint)):\n",
        "        liste2.append(c.verify(assignement,liste2))\n",
        "\n",
        "    listtt = liste1 + liste2\n",
        "    if (listtt == []):\n",
        "      return all([listtt]) # if only one constraint doesn't accept the assignement this returns False. It returns True otherwise\n",
        "    else:\n",
        "      res = all(listtt)\n",
        "      listtt = []\n",
        "      return res\n",
        "\n",
        "  def prepareCpModelConstraint(self, vars_dict, model):\n",
        "\n",
        "    # This method adds the constraints in this conjunction to the model\n",
        "    if not self.isNegation:\n",
        "      ec = deepcopy(self.elementaryConstraints)\n",
        "      for c in ec:\n",
        "        model+= cp.all([c.returnCpModelConstraint(vars_dict) for c in ec])\n",
        "    else:\n",
        "      ec = deepcopy(self.elementaryConstraints)\n",
        "      for c in ec:\n",
        "        c.isNegation = True\n",
        "      model+= cp.any([c.returnCpModelConstraint(vars_dict) for c in ec])\n",
        "\n",
        "  # def prepareCpModelConstraintReification(self,vars_dict,model,booll):\n",
        "  #   model+= (cp.all([c.returnCpModelConstraint(vars_dict) for c in self.elementaryConstraints])).implies(booll)\n",
        "  #   model+= booll.implies(cp.all([c.returnCpModelConstraint(vars_dict) for c in self.elementaryConstraints]))\n",
        "\n",
        "  def prepareCpModelConstraintReification(self,vars_dict,model,booll):\n",
        "    model += cp.all([(cp.all([c.returnCpModelConstraint(vars_dict) for c in self.elementaryConstraints])).implies(booll),booll.implies(cp.all([c.returnCpModelConstraint(vars_dict) for c in self.elementaryConstraints])) ])\n",
        "\n",
        "\n",
        "\n",
        "  def isScopeIncludedInY(self,Y, liste = []):\n",
        "  # this method verifies if the scope is included in Y\n",
        "  # the scope of a conjunction is all the variables in the scopes of the constraints from which it is built\n",
        "\n",
        "    scope = self.scope\n",
        "\n",
        "    if set(scope).issubset(set(Y)):\n",
        "      return True\n",
        "\n",
        "    return False\n",
        "\n",
        "\n",
        "  def isScopeIsExactlyY(self,Y):\n",
        "\n",
        "    # this method verifies if the scope is Exactly Y\n",
        "    scope = self.scope\n",
        "    return set(scope) == set(Y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QckGq1LRwjPU"
      },
      "outputs": [],
      "source": [
        "# problem_data = {\n",
        "#     1:{\n",
        "#       \"domain\":(1,5),\n",
        "#        \"type\":\"d\",\n",
        "#        \"name\":\"A\"\n",
        "#     },\n",
        "#     2:{\n",
        "#         \"domain\":(1,5),\n",
        "#        \"type\":\"d\",\n",
        "#        \"name\":\"B\"\n",
        "#     },\n",
        "#     3:{\n",
        "#         \"domain\":(1,5),\n",
        "#        \"type\":\"d\",\n",
        "#        \"name\":\"C\"\n",
        "#     },\n",
        "#     4:{\n",
        "#         \"domain\":(1,5),\n",
        "#        \"type\":\"d\",\n",
        "#        \"name\":\"D\"\n",
        "#     },\n",
        "#     5:{\n",
        "#         \"domain\":(1,5),\n",
        "#        \"type\":\"d\",\n",
        "#        \"name\":\"E\"\n",
        "#     },\n",
        "# }\n",
        "\n",
        "\n",
        "# # c = Conjunction([\n",
        "# #     Constraint([1,2], operator.eq, 2, True),\n",
        "# #     Constraint([1,3], operator.eq, 2, True),\n",
        "# #     Conjunction([\n",
        "# #       Constraint([1,4], operator.eq, 2, True),\n",
        "# #       Constraint([1,5], operator.eq, 2, True),\n",
        "# #     ])\n",
        "# # ])\n",
        "\n",
        "# # n1 = Network(problem_data, [c])\n",
        "# # print(\"before: \", n1.solve())\n",
        "\n",
        "# # c.isNegation = True\n",
        "# # n2 = Network(problem_data, [c])\n",
        "# # print(\"after: \", n2.solve())\n",
        "\n",
        "\n",
        "\n",
        "# c = Conjunction([\n",
        "#     Constraint([1,3], operator.ne, 2, True),\n",
        "#     Constraint([1,3], operator.ge, 2, False),\n",
        "# ])\n",
        "\n",
        "\n",
        "\n",
        "# co = Constraint([1,3], operator.gt, 2, False)\n",
        "\n",
        "\n",
        "# co.isEqualTo(c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FcHff2xtd6IP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGeBS2ePhZXR"
      },
      "source": [
        "## Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xn-63wxVhZRR"
      },
      "outputs": [],
      "source": [
        "from ortools.sat.python import cp_model\n",
        "from itertools import permutations, combinations\n",
        "\n",
        "# This was borrowed from ortools official documentation, it catches all the solutions\n",
        "class VarArraySolutionPrinter(cp_model.CpSolverSolutionCallback):\n",
        "    \"\"\"Print intermediate solutions.\"\"\"\n",
        "\n",
        "    def __init__(self, variables, Limit):\n",
        "        cp_model.CpSolverSolutionCallback.__init__(self)\n",
        "        self.__variables = variables\n",
        "        self.__solution_count = 0\n",
        "        self.__solutions = []\n",
        "        self.Limit = Limit\n",
        "\n",
        "    def on_solution_callback(self):\n",
        "        self.__solution_count += 1\n",
        "\n",
        "        e = []\n",
        "        self.__solution_count+=1\n",
        "        for v in self.__variables.keys():\n",
        "          e.append(\n",
        "              (v, self.__variables[v].Name() , self.Value(self.__variables[v]))\n",
        "          )\n",
        "        #print(f\"Solution {self.__solution_count}: \",e)\n",
        "\n",
        "        self.__solutions.append(e)\n",
        "\n",
        "        if(self.Limit != None):\n",
        "          if(self.__solution_count > self.Limit):\n",
        "             self.StopSearch()\n",
        "\n",
        "    def solution_count(self):\n",
        "        return self.__solution_count\n",
        "\n",
        "    def allSolutions(self):\n",
        "      return self.__solutions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lb7j-uoul48i"
      },
      "outputs": [],
      "source": [
        "class Network():\n",
        "\n",
        "  def __init__(self, problem_data ,constraints):\n",
        "\n",
        "    self.problem_data = problem_data\n",
        "\n",
        "    self.constraints = constraints\n",
        "\n",
        "    self.all_solutions = []\n",
        "\n",
        "  def isAccepted(self, e):\n",
        "    # An assignement e is accepted by a network if it doesn't violate any constraints of the network\n",
        "    violated = None\n",
        "    for c in self.constraints:\n",
        "      if not c.verify(e) :\n",
        "        violated = c\n",
        "\n",
        "    return all( [c.verify(e) for c in self.constraints]), violated\n",
        "\n",
        "  def isSolution(self,e):\n",
        "    # e is a solution if it is complete and accepted\n",
        "    ids = [ee[0] for ee in e]\n",
        "    return self.isAccepted(e) and set(ids) == set(self.problem_data.keys())\n",
        "\n",
        "\n",
        "  def ConstraintsIncludedInY(self,Y):\n",
        "    # This method constructs a network from the constraints\n",
        "    # that have scope included in Y\n",
        "\n",
        "    res = []\n",
        "\n",
        "    for c in self.constraints:\n",
        "      if(isinstance(c,Constraint)):\n",
        "        if set(c.scope_ids).issubset(set(Y)):\n",
        "            res.append(c)\n",
        "      else:\n",
        "        if(c.isScopeIncludedInY(Y)):\n",
        "          res.append(c)\n",
        "\n",
        "    prob_data = {}\n",
        "    for y in Y:\n",
        "      prob_data[y] = self.problem_data[y]\n",
        "\n",
        "\n",
        "    return Network(prob_data,res)\n",
        "\n",
        "\n",
        "  def ConstraintsIsExactlyY(self,Y):\n",
        "    # This method constructs a network from the constraints\n",
        "    # that have scope is Exactly Y\n",
        "\n",
        "    res = []\n",
        "\n",
        "    for c in self.constraints:\n",
        "      if(isinstance(c,Constraint)):\n",
        "        if set(c.scope_ids) == set(Y) :\n",
        "          res.append(c)\n",
        "      else:\n",
        "        if(c.isScopeIsExactlyY(Y)):\n",
        "          res.append(c)\n",
        "\n",
        "\n",
        "    prob_data = {}\n",
        "    for y in Y:\n",
        "      prob_data[y] = self.problem_data[y]\n",
        "\n",
        "\n",
        "    return Network(prob_data,res)\n",
        "\n",
        "\n",
        "  def solve(self):\n",
        "\n",
        "    model1 = cp.Model()\n",
        "\n",
        "    vars = {id: cp.intvar(self.problem_data[id][\"domain\"][0],self.problem_data[id][\"domain\"][1], shape=1, name=self.problem_data[id][\"name\"] )  for id in self.problem_data.keys()}\n",
        "\n",
        "    for c in self.constraints:\n",
        "      c.prepareCpModelConstraint(vars,model1)\n",
        "\n",
        "    for v in vars.keys():\n",
        "        model1 += vars[v] >= vars[v].lb  # this is necessary because cpmpy doesn't instantiate vars that are not used in any constraint\n",
        "\n",
        "\n",
        "    if(model1.solve(solver=\"exact\")):\n",
        "      e = []\n",
        "      for v in vars.keys():\n",
        "        e.append(\n",
        "            (v, vars[v].name, vars[v].value())\n",
        "          )\n",
        "      return e\n",
        "    else:\n",
        "        #print('No solution found.')\n",
        "        return []\n",
        "\n",
        "\n",
        "  def allSolutions(self, TimeLimit = None, SolutionLimit= None):\n",
        "    self.all_solutions = []\n",
        "\n",
        "    model2 = cp.Model()\n",
        "\n",
        "    vars = {id: cp.intvar(self.problem_data[id][\"domain\"][0],self.problem_data[id][\"domain\"][1], shape=1, name=self.problem_data[id][\"name\"] )  for id in self.problem_data.keys()}\n",
        "\n",
        "\n",
        "    for c in self.constraints:\n",
        "      c.prepareCpModelConstraint(vars,model2)\n",
        "\n",
        "    for v in vars.keys():\n",
        "        model2 += vars[v] >= vars[v].lb\n",
        "\n",
        "    model2.solveAll(solver=\"exact\",display = lambda : self.all_solutions.append([(k,vars[k].name, vars[k].value()) for k in vars.keys()]), time_limit= TimeLimit, solution_limit = SolutionLimit)\n",
        "\n",
        "  def getAllSolutions(self, TimeLimit = None, SolutionLimit= None):\n",
        "    # this is the function which will be used in other places.\n",
        "    self.allSolutions(TimeLimit, SolutionLimit)\n",
        "\n",
        "    return self.all_solutions\n",
        "\n",
        "  # def isEquivalentTo(self, N):\n",
        "  #   # If the sets of solutions of two networks T and T2 are equal, then these two networks are equivalent\n",
        "  #   e1 = self.solve()\n",
        "  #   if(not N.isAccepted(e1)):\n",
        "  #     return False\n",
        "\n",
        "  #   e2 = N.solve()\n",
        "  #   if(not self.isAccepted(e2)):\n",
        "  #     return False\n",
        "\n",
        "  #   selfsols = self.getAllSolutions()\n",
        "  #   for solution in selfsols:\n",
        "  #     if not N.isAccepted(solution):\n",
        "  #       return False\n",
        "\n",
        "  #   return True\n",
        "\n",
        "\n",
        "\n",
        "  def networkOfConstraintsThatRejectE(self,e):\n",
        "    # Here we construct a network containing constraints that reject e\n",
        "    tmp = []\n",
        "\n",
        "    Y = set()\n",
        "    for c in self.constraints:\n",
        "      if not c.verify(e):\n",
        "        tmp.append(c)\n",
        "\n",
        "        if(isinstance(c,Conjunction)):\n",
        "          Y = Y.union(set(c.scope))\n",
        "        else:\n",
        "          Y = Y.union(set(c.scope_ids))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    prob_data = {}\n",
        "    for y in Y:\n",
        "      prob_data[y] = self.problem_data[y]\n",
        "\n",
        "\n",
        "\n",
        "    return Network(prob_data, tmp)\n",
        "\n",
        "\n",
        "  def removeConstraintsThatRejectE(self,e):\n",
        "    # we remove the constraints that reject e from the current network\n",
        "    tmp = []\n",
        "\n",
        "    for c in self.constraints:\n",
        "      if c.verify(e):\n",
        "        tmp.append(c)\n",
        "\n",
        "    self.constraints = tmp\n",
        "    return self\n",
        "\n",
        "\n",
        "  def addConstraint(self,c):\n",
        "    # we add  the constraint c to the network\n",
        "    self.constraints.append(c)\n",
        "    return self\n",
        "\n",
        "  def removeListOfConstraints(self, l):\n",
        "    # we remove a list l of constraints from the current network\n",
        "\n",
        "    tmp = []\n",
        "    for c in self.constraints:\n",
        "      #if c not in l:\n",
        "      if not constraintinConstraints(c,l):\n",
        "        tmp.append(c)\n",
        "\n",
        "    self.constraints = tmp\n",
        "\n",
        "    return self\n",
        "\n",
        "  def removeRedundants(self):\n",
        "    tmp = []\n",
        "    for c in self.constraints:\n",
        "      #if c not in l:\n",
        "      if not c.isRedundant:\n",
        "        tmp.append(c)\n",
        "\n",
        "    self.constraints = tmp\n",
        "\n",
        "    return self\n",
        "\n",
        "\n",
        "  def isEqualTo(self,constraint1, constraint):\n",
        "\n",
        "    if(not isinstance(constraint, Constraint)):\n",
        "        return False\n",
        "\n",
        "    if(constraint1.arity == constraint.arity and  constraint1.parameters == constraint.parameters and constraint1.rel == constraint.rel):\n",
        "\n",
        "      if(constraint1.scope_ids == constraint.scope_ids):\n",
        "        return True\n",
        "\n",
        "      else:\n",
        "        if(set(constraint1.scope_ids) == set(constraint.scope_ids)):\n",
        "\n",
        "          if(constraint1.isCommutative and constraint.isCommutative):\n",
        "            return True\n",
        "\n",
        "\n",
        "          v = set(constraint1.scope_ids).union(set(constraint.scope_ids))\n",
        "\n",
        "\n",
        "          prob_data = {}\n",
        "          for vv in v:\n",
        "            prob_data[vv] = problem_data[vv]\n",
        "\n",
        "\n",
        "          n1 = Network(\n",
        "                    prob_data,\n",
        "                    [constraint1])\n",
        "\n",
        "          n2 = Network(\n",
        "                    prob_data,\n",
        "                    [constraint])\n",
        "\n",
        "          return listOfSolsEqual(n1.getAllSolutions(), n2.getAllSolutions())\n",
        "\n",
        "    return False\n",
        "\n",
        "\n",
        "\n",
        "  def removeDuplicates(self):\n",
        "    res = []\n",
        "    tmp = []\n",
        "\n",
        "    for c in self.constraints:\n",
        "      if not self.existAlready2(res,c):\n",
        "        tmp.append(c)\n",
        "\n",
        "    self.constraints = tmp\n",
        "    return self\n",
        "\n",
        "\n",
        "  def existAlready2(self,res,c):\n",
        "    if(self.ConstraintinListOfConstraints(c, res)):\n",
        "      return True\n",
        "    res.append(c)\n",
        "    return False\n",
        "\n",
        "  def ConstraintinListOfConstraints(self,a,listOfConsts):\n",
        "    for l in listOfConsts:\n",
        "      if(a.isEqualTo(l)):\n",
        "        return True\n",
        "\n",
        "    return False\n",
        "\n",
        "  def isEquivalentTo(self, net):\n",
        "\n",
        "    a1 = []\n",
        "    for c in self.constraints:\n",
        "      a1.append(isImplied(c, net.constraints))\n",
        "\n",
        "    a2 = []\n",
        "    for c in net.constraints:\n",
        "      a2.append(isImplied(c,self.constraints))\n",
        "\n",
        "    return all(a1) and all(a2)\n",
        "\n",
        "\n",
        "\n",
        "def listOfSolsEqual(l1,l2):\n",
        "\n",
        "  if(l1 == [] and l2 == []):\n",
        "    return True\n",
        "\n",
        "  if(l1 != [] and l2 == []):\n",
        "    return False\n",
        "\n",
        "  if(l1 == [] and l2 != []):\n",
        "    return False\n",
        "\n",
        "  if len(l1) != len(l2):\n",
        "    return False\n",
        "\n",
        "\n",
        "  cpt1=0\n",
        "  for a in l1:\n",
        "    if a in l2:\n",
        "      cpt1+=1\n",
        "\n",
        "  cpt2=0\n",
        "  for a in l2:\n",
        "    if a in l1:\n",
        "      cpt2+=1\n",
        "  if(cpt1 == len(l1) and cpt2 == len(l2)):\n",
        "    return True\n",
        "\n",
        "  return False\n",
        "\n",
        "def constraintinConstraints(c,l):\n",
        "\n",
        "  for cc in l:\n",
        "    if c.isEqualTo(cc):\n",
        "      return True\n",
        "\n",
        "  return False\n",
        "\n",
        "def isImplied(c,consts):\n",
        "  n = []\n",
        "  n = Network(problem_data, deepcopy(consts))\n",
        "  notc = deepcopy(c)\n",
        "  notc.isNegation = True\n",
        "\n",
        "  n= n.addConstraint(notc)\n",
        "\n",
        "  if(n.solve()==[]):\n",
        "    return True\n",
        "\n",
        "  return False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1beQ7n_UeAqd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVvmtYoCpBcD"
      },
      "source": [
        "## Basis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zL0_Y1orpBSJ"
      },
      "outputs": [],
      "source": [
        "from itertools import permutations, combinations\n",
        "\n",
        "#The language is like this = [(rel,arity, isCommutative, parameters)]\n",
        "\n",
        "\n",
        "class Basis(Network):\n",
        "  # A Basis is also a network, but its constraints are built from a language\n",
        "\n",
        "  def __init__(self,problem_data, language):\n",
        "    super().__init__(problem_data,[])\n",
        "    self.language = language\n",
        "\n",
        "  def build(self):\n",
        "\n",
        "   # we build the set of constraints using permutations of variables of length == arity of relation (operator)\n",
        "    if((operator.lt,2,False,None) in self.language and (operator.gt,2,False,None) in self.language):\n",
        "      self.language.remove((operator.gt,2,False,None))\n",
        "\n",
        "    if((operator.le,2,False,None) in self.language and (operator.ge,2,False,None) in self.language):\n",
        "      self.language.remove((operator.ge,2,False,None))\n",
        "\n",
        "    for l in self.language:\n",
        "      if(l[1] != None): # this is a bounded constraint\n",
        "        if(l[1] == 1): # the unary constraints with a parameter\n",
        "          # if(l[0] in [eqConst, neConst,ltConst, leConst, gtConst, geConst]):\n",
        "          #   for v in self.vars_ids:\n",
        "          #     for param in [1,2,3,4,5]:\n",
        "          #       self.constraints.append(Constraint([v],l[0],l[1],l[2],[param]))\n",
        "\n",
        "          if(l[3] != None):\n",
        "            for v in self.problem_data.keys():\n",
        "              for param in l[3]:\n",
        "                self.constraints.append(Constraint([v],l[0],l[1],l[2],[param]))\n",
        "        else:\n",
        "          if(l[2] == False): # Not commutative, then we need both [a,b] and [b,a] so we do permutations\n",
        "            for p in permutations(self.problem_data.keys(),l[1]):\n",
        "              self.constraints.append(Constraint(list(p),l[0],l[1],l[2]))\n",
        "          else: #Commutative, just [a,b] is enough so we use combinations\n",
        "            if(l[0] in [eqDist, eqDist3, neDist, neDist3]):\n",
        "              for p in combinations(self.problem_data.keys(),l[1]):\n",
        "                aa = np.array(list(p))\n",
        "                if(len(np.unique(aa)) == len(aa)):\n",
        "                  self.constraints.append(Constraint(list(p),l[0],l[1],l[2]))\n",
        "            else:\n",
        "              for p in combinations(self.problem_data.keys(),l[1]):\n",
        "                self.constraints.append(Constraint(list(p),l[0],l[1],l[2]))\n",
        "\n",
        "\n",
        "      else: # this is an unbounded constraint(global)\n",
        "        ll = 2\n",
        "        while( ll<=len(self.problem_data.keys())):\n",
        "          for p in permutations(self.problem_data.keys(),ll):\n",
        "            self.constraints.append(Constraint(list(p),l[0],l[1],l[2]))\n",
        "          ll+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "om78a4u8eFdv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqAFjDQmpBK1"
      },
      "source": [
        "## Send a query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TN5W6W6vpBDi"
      },
      "outputs": [],
      "source": [
        "def ask(e, Target):\n",
        "  global Qc\n",
        "  global Qa\n",
        "  global oldsize\n",
        "  global newsize\n",
        "  global maxwaittimeold\n",
        "  global waittimes\n",
        "  global Examplesizes\n",
        "\n",
        "\n",
        "\n",
        "  now = time.time()\n",
        "\n",
        "  waittimes.append(now-maxwaittimeold)\n",
        "  maxwaittimeold = now\n",
        "\n",
        "  Qc+=1\n",
        "  Examplesizes.append(len(e))\n",
        "\n",
        "  if(newsize > oldsize):\n",
        "    oldsize = newsize\n",
        "    if(Qc > Qa):\n",
        "      Qa = Qc\n",
        "\n",
        "  print(\"Classify this example: \\n\\n\")\n",
        "  print(e)\n",
        "\n",
        "  if set([ee[0] for ee in e]) == set(Target.problem_data.keys()):\n",
        "    # This is a membership query\n",
        "    #print(\"Hereeeee 367 : \", e)\n",
        "    #if e in Target.getAllSolutions(): # not necessary and when we generate just for example 10000 solutions, this will return false if\n",
        "                                       # e is not in the first 10000 even if it is a solution\n",
        "    res, cons = Target.isAccepted(e)\n",
        "    if res: # we don't need all solutions here !!!!!!!!!, just verify if it is accepted or not !\n",
        "      print(\"\\n a COMPLETE query was sent, the answer was YES\")\n",
        "      #print(\"The example was : \\n\\n\", e)\n",
        "      return True, cons\n",
        "    else:\n",
        "      print(\"\\n a COMPLETE query was sent, the answer was NO\")\n",
        "      # print(\"The Negative Example was : \\n\\n\", e)\n",
        "      # print(input())\n",
        "      return False, cons\n",
        "\n",
        "  else:\n",
        "    # This is a partial query\n",
        "    #print(\"Hereeeee 376 : \", e)\n",
        "    res, cons = Target.isAccepted(e)\n",
        "    if res:\n",
        "      print(\"\\n a PARTIAL query was sent, the answer was YES\")\n",
        "      #print(\"The example was : \\n\\n\", e)\n",
        "      return True, cons\n",
        "    else:\n",
        "      print(\"\\n a PARTIAL query was sent, the answer was NO\")\n",
        "      # print(\"The Negative Example was : \\n\\n\", e)\n",
        "      # print(input())\n",
        "      return False, cons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bVfc7VdqeITn"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kz6vcOkOILR1"
      },
      "source": [
        "## Generate Example with cuttoff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJXSgYzpn6y0"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "\n",
        "def solve(n, scope):\n",
        "\n",
        "  st = time.time()\n",
        "  b = n.ConstraintsIncludedInY(scope)\n",
        "\n",
        "  r = b.solve()\n",
        "  if( r == []):\n",
        "    return [], time.time()-st\n",
        "  else:\n",
        "    return projection(r,scope) , time.time()-st\n",
        "    #return r , time.time()-st\n",
        "\n",
        "def getVarIdBdeg(X):\n",
        "  global B\n",
        "  global vars_ids\n",
        "  nbConsts = 0\n",
        "  idd = None\n",
        "  for x in X:\n",
        "    cpt = 0\n",
        "    for c in B.constraints:\n",
        "      if(isinstance(c, Conjunction)):\n",
        "        if(x in c.scope):\n",
        "          cpt+=1\n",
        "      else:\n",
        "        if(x in c.scope_ids):\n",
        "          cpt+=1\n",
        "\n",
        "    if(cpt>nbConsts):\n",
        "      # print(\"\\n New max consts: \", cpt)\n",
        "      # print(\"\\n Var id \", x)\n",
        "      nbConsts = cpt\n",
        "      idd = x\n",
        "\n",
        "  # print(\"\\n Final max consts: \", nbConsts)\n",
        "  # print(\"\\n Var id \", idd)\n",
        "  return idd\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def solveCuttOffCore(n, scope, remainingTime):\n",
        "  Y = scope[:]\n",
        "  best = None\n",
        "  X = list(set(problem_data.keys()).difference(set(Y)))[:]\n",
        "\n",
        "  start = time.time()\n",
        "\n",
        "\n",
        "  while(X != []):\n",
        "\n",
        "\n",
        "\n",
        "    #Y.append(X[0]) # use bdeg heuristic\n",
        "    i = getVarIdBdeg(X)\n",
        "    if(i!=None):\n",
        "      Y.append(i)\n",
        "    else:\n",
        "      Y.append(X[0])\n",
        "    #print(\"\\n\\n Next Y: \", Y)\n",
        "    X = list(set(problem_data.keys()).difference(set(Y)))\n",
        "    #print(\"\\n\\n Next X: \", X)\n",
        "    Y = sorted(Y)\n",
        "    b = n.ConstraintsIncludedInY(Y)\n",
        "\n",
        "    model3 = cp.Model()\n",
        "\n",
        "    vars = {id: cp.intvar(b.problem_data[id][\"domain\"][0],b.problem_data[id][\"domain\"][1], shape=1, name=b.problem_data[id][\"name\"] )  for id in b.problem_data.keys()}\n",
        "\n",
        "    for c in b.constraints:\n",
        "      c.prepareCpModelConstraint(vars,model3)\n",
        "\n",
        "    for v in vars.keys():\n",
        "        model3 += vars[v] >= vars[v].lb  # this is necessary because cpmpy doesn't instantiate vars that are not used in any constraint\n",
        "\n",
        "    if(model3.solve(solver=\"exact\",time_limit=remainingTime) ):\n",
        "      s = []\n",
        "      for v in vars.keys():\n",
        "        s.append(\n",
        "            (v, vars[v].name, vars[v].value())\n",
        "          )\n",
        "    else:\n",
        "      if(model3.cpm_status.exitstatus.name == \"UNSATISFIABLE\"):\n",
        "        return []\n",
        "      else:\n",
        "        return None\n",
        "\n",
        "\n",
        "      # if time.time() - start > remainingTime:\n",
        "      #   return best\n",
        "\n",
        "    for v in X:\n",
        "      #print(\"\\n\\n\\n Inside if leads to arc inconsistency, remainingtime was : \",remainingTime)\n",
        "      #print(\"\\n Inside if leads to arc inconsistency, Time consumed so far : \", time.time() - start)\n",
        "      #print(\"\\nExit condition : \", time.time() - start > remainingTime)\n",
        "      # if time.time() - start > remainingTime:\n",
        "      #   #print(\"HERE 46 will return None\")\n",
        "      #   return None\n",
        "\n",
        "      tmp = n.ConstraintsIsExactlyY(Y+[v])\n",
        "      tmp2 = n.ConstraintsIsExactlyY([v]+Y)\n",
        "\n",
        "      if(tmp.solve()==[] or tmp2.solve==[]):\n",
        "        #leads to arc inconsistency\n",
        "        return []\n",
        "\n",
        "      # #print(\"\\nExit condition : \", time.time() - start > remainingTime)\n",
        "      # if time.time() - start > remainingTime:\n",
        "      #   return None\n",
        "  #best = projection(s,Y)\n",
        "    best = s\n",
        "\n",
        "    # print(\"\\n\\n\\nRemainingtime was : \",remainingTime)\n",
        "    # print(\"\\nTime consumed so far : \", time.time() - start)\n",
        "\n",
        "    # print(\"\\nExit condition : \", time.time() - start > remainingTime)\n",
        "    if time.time() - start > remainingTime:\n",
        "      #print(\"HERE 58 will return None\")\n",
        "      return best\n",
        "  return best\n",
        "\n",
        "def solveCuttOff(n,scope, remainingTime):\n",
        "\n",
        "\n",
        "  started = time.time()\n",
        "  s = solveCuttOffCore(n,scope, remainingTime)\n",
        "\n",
        "  #print(\"\\nAfter solveCuttOffCore:\\n return value : \", s, \" remainingTime: \", remainingTime)\n",
        "  #print(\"It took: \", time.time() - started)\n",
        "\n",
        "  if(s == None):\n",
        "    return (None, remainingTime)\n",
        "  else:\n",
        "    return (s, time.time() - started)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o1mYGrxXIPuU"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from copy import deepcopy\n",
        "\n",
        "redundants = []\n",
        "def generateExample(X,L, B):\n",
        "  global redundants\n",
        "  cuttoff = 1\n",
        "  timer = 0\n",
        "\n",
        "\n",
        "\n",
        "  cpt = 0\n",
        "  while(cpt<len(B.constraints)):\n",
        "    cc = deepcopy(B.constraints[cpt])\n",
        "    cc.isNegation = True\n",
        "    ll = deepcopy(L)\n",
        "    ll = ll.addConstraint(cc)\n",
        "\n",
        "    e,t = solve(ll, cc.scope_ids)\n",
        "\n",
        "    timer+=t\n",
        "\n",
        "    if e==[]:\n",
        "      redd = B.constraints[cpt]\n",
        "      redd.isRedundant = True\n",
        "      redundants.append(redd)\n",
        "      L = L.addConstraint(redd)\n",
        "      B = B.removeListOfConstraints([redd])\n",
        "    else:\n",
        "      rt = cuttoff - timer\n",
        "      if(rt<0):\n",
        "        rt = 0\n",
        "      ep, tp = solveCuttOff(ll, cc.scope_ids, rt)\n",
        "      timer += tp\n",
        "\n",
        "      if ep == None:\n",
        "        return e\n",
        "\n",
        "      if ep == []:\n",
        "        redd = B.constraints[cpt]\n",
        "        redd.isRedundant = True\n",
        "        redundants.append(redd)\n",
        "        L = L.addConstraint(redd)\n",
        "        B = B.removeListOfConstraints([redd])\n",
        "      else:\n",
        "        return ep\n",
        "  print(\"Redundants: \\n\")\n",
        "  for r in redundants:\n",
        "    print(r)\n",
        "\n",
        "  print(\"before remove reudndants: \\n\")\n",
        "  print(len(L.constraints))\n",
        "  #L = L.removeListOfConstraints(redundants)\n",
        "  L = L.removeRedundants()\n",
        "  print(\"after remove reudndants: \\n\")\n",
        "  print(len(L.constraints))\n",
        "  return []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Rz6-fJAeNN7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jDUuzVs1kdT"
      },
      "source": [
        "## FindScope"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PD6rlzo21kPn"
      },
      "outputs": [],
      "source": [
        "def ConstraintinListOfConstraints(a,listOfConsts):\n",
        "\n",
        "  for l in listOfConsts:\n",
        "    if(a.isEqualTo(l)):\n",
        "      return True\n",
        "\n",
        "  return False\n",
        "def areListsOfConstraintsEqual(l1,l2):\n",
        "  if(l1 == [] and l2!=[]):\n",
        "    return False\n",
        "  if(l1 != [] and l2==[]):\n",
        "    return False\n",
        "  if(l1 == [] and l2==[]):\n",
        "    return True\n",
        "  if(len(l1)!=len(l2)):\n",
        "    return False\n",
        "\n",
        "  cpt1  = 0\n",
        "  for l in l1:\n",
        "    if ConstraintinListOfConstraints(l,l2):\n",
        "      cpt1+=1\n",
        "\n",
        "  cpt2  = 0\n",
        "  for l in l2:\n",
        "    if ConstraintinListOfConstraints(l,l1):\n",
        "      cpt2+=1\n",
        "\n",
        "  if(cpt1 == len(l1) and cpt2 == len(l2)):\n",
        "    return True\n",
        "\n",
        "  return False\n",
        "\n",
        "def FindScope(e, R ,Y , B):\n",
        "  global calls\n",
        "  global Target\n",
        "\n",
        "  print(\"\\n\\nFind scope call -------------------------------------------\\n\\n\")\n",
        "  # calls+=1\n",
        "\n",
        "  # print(\"Inside findscope !!!!!! \\n: \")\n",
        "\n",
        "  print(\"R: \", [r for r in R])\n",
        "  print(\"Y: \", [y for y in Y])\n",
        "\n",
        "  # if(calls > 20):\n",
        "  #   print(\"Possible Recusion Error \") # For test purpose, to be removed later\n",
        "  #   return\n",
        "\n",
        "  # print(\"len(B.networkOfConstraintsThatRejectE(projection(e,R)).constraints): \", B.networkOfConstraintsThatRejectE(projection(e,R)).constraints)\n",
        "  # for c in B.networkOfConstraintsThatRejectE(projection(e,R)).constraints:\n",
        "  #   print(c.rel, c.scope_ids)\n",
        "\n",
        "\n",
        "  # print(\"the test\", B.networkOfConstraintsThatRejectE(projection(e,R)).constraints != [])\n",
        "  if B.networkOfConstraintsThatRejectE(projection(e,R)).constraints != [] :\n",
        "    if(ask(projection(e,R), Target)[0]):\n",
        "      # print(\"Here\")\n",
        "      # print(\"In FindScope B[\",Y,\"] before removal: \\n\")\n",
        "      # for c in B.ConstraintsIncludedInY(Y).constraints:\n",
        "      #   print(c.scope_ids, c.rel)\n",
        "      # print(projection(e,R),\" was classified positive\")\n",
        "      B = B.removeConstraintsThatRejectE(projection(e,R))\n",
        "      # print(\"\\nInside FindScope B reduced in size, new size: \",len(B.constraints))\n",
        "      # print(\"In FindScope B[\",Y,\"] After removal: \\n\")\n",
        "      # for c in B.ConstraintsIncludedInY(Y).constraints:\n",
        "      #   print(c.scope_ids, c.rel)\n",
        "    else:\n",
        "      # print(\"Here 2\")\n",
        "      # print(projection(e,R),\" was classified negative\")\n",
        "      return []\n",
        "\n",
        "  if len(Y) == 1:\n",
        "    return Y\n",
        "\n",
        "  l = len(Y)\n",
        "\n",
        "  l2 = l//2\n",
        "\n",
        "  if l == 2 :\n",
        "    Y1 = Y[:l2]\n",
        "    Y2 = Y[l2:]\n",
        "  else:\n",
        "    Y1 = Y[:l2+1]\n",
        "    Y2 = Y[l2+1:]\n",
        "\n",
        "\n",
        "  if(areListsOfConstraintsEqual(B.networkOfConstraintsThatRejectE(projection( e,list(set(R).union(set(Y1))))).constraints , B.networkOfConstraintsThatRejectE(projection( e,list(set(R).union(set(Y))))).constraints ) ):\n",
        "    #print(\"Here !!! 1\")\n",
        "    S1 = []\n",
        "  else:\n",
        "    #print(\"Here !!! 2\")\n",
        "    S1 = FindScope(e, list(set(R).union(set(Y1))), Y2, B )\n",
        "\n",
        "  if( areListsOfConstraintsEqual(B.networkOfConstraintsThatRejectE(projection( e,list(set(R).union(set(S1))))).constraints , B.networkOfConstraintsThatRejectE(projection( e,list(set(R).union(set(Y))))).constraints) ):\n",
        "    #print(\"Here !!! 3\")\n",
        "    S2 = []\n",
        "  else:\n",
        "    #print(\"Here !!! 4\")\n",
        "    S2 = FindScope(e, list(set(R).union(set(S1))), Y1, B)\n",
        "\n",
        "\n",
        "  return list( set(S1).union(set(S2)) )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Vf3XYcleYYB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Czg3LXM51kG0"
      },
      "source": [
        "## Join Networks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BwIRV30TBWc7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0aae5e07-cc71-47a3-9d19-7240b6140af8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-4d80adbd618e>:2: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.*` instead of `tqdm._tqdm_notebook.*`\n",
            "  from tqdm._tqdm_notebook import tqdm_notebook\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from tqdm._tqdm_notebook import tqdm_notebook\n",
        "tqdm_notebook.pandas()\n",
        "\n",
        "def existAlready(res,nc1,nc2):\n",
        "    # conj = joinConstraints([],nc1,nc2)\n",
        "    # conj2 = joinConstraints([],nc2,nc1)\n",
        "    conj = joinConstraints([],nc1,nc2)\n",
        "    if(ConstraintinListOfConstraints(conj, res)):\n",
        "        return True\n",
        "    # if(ConstraintinListOfConstraints(conj2, res)):\n",
        "    #     return True\n",
        "\n",
        "    return False\n",
        "\n",
        "\n",
        "\n",
        "def existAlready2(res,c):\n",
        "\n",
        "    if(ConstraintinListOfConstraints(c, res)):\n",
        "        return True\n",
        "    res.append(c)\n",
        "    return False\n",
        "\n",
        "\n",
        "def inListOfLists(a,listOfLists):\n",
        "\n",
        "  for l in listOfLists:\n",
        "    if(isinstance(l,int)):\n",
        "      if(l == a):\n",
        "        return True\n",
        "    else:\n",
        "       res = inListOfLists(a,l)\n",
        "       if(res):\n",
        "         return True\n",
        "\n",
        "  return False\n",
        "\n",
        "\n",
        "def ConstraintinListOfConstraints(a,listOfConsts):\n",
        "\n",
        "  for l in listOfConsts:\n",
        "    if(a.isEqualTo(l)):\n",
        "      return True\n",
        "\n",
        "  return False\n",
        "\n",
        "import time\n",
        "def areIncompatible(vars_data,c1,c2):\n",
        "\n",
        "\n",
        "  scope1 = c1.scope_ids if isinstance(c1,Constraint) else c1.scope\n",
        "\n",
        "  scope2 = c2.scope_ids if isinstance(c2,Constraint) else c2.scope\n",
        "\n",
        "  scope = set(scope1).union(set(scope2))\n",
        "\n",
        "  vd = {}\n",
        "\n",
        "  for id in vars_data.keys():\n",
        "    if id in scope:\n",
        "      vd[id] = vars_data[id]\n",
        "\n",
        "  n = Network(\n",
        "      vd,\n",
        "      [c1,c2])\n",
        "\n",
        "  # print(\"\\n\\nThe network inside areIncompatible: \\n\")\n",
        "  # print(\"VarsData: \\n\")\n",
        "  # print(n.problem_data)\n",
        "  # print(\"Constraints: \\n\")\n",
        "\n",
        "  # for c in n.constraints:\n",
        "  #   print(c.rel,c.scope_ids)\n",
        "\n",
        "  # st = time.time()\n",
        "  if(n.solve() == []):\n",
        "    # print(\"To prove incompatibility, it took: \", time.time()-st)\n",
        "    return True\n",
        "\n",
        "  # print(\"To prove incompatibility, it took: \", time.time()-st)\n",
        "  return False\n",
        "\n",
        "\n",
        "# def joinConstraints(res,c1,c2):\n",
        "#   if(c1.isEqualTo(c2)):\n",
        "#     return c1\n",
        "#   else:\n",
        "#     res.append(Conjunction([c1,c2]))\n",
        "#     return Conjunction([c1,c2])\n",
        "\n",
        "\n",
        "def joinConstraints(res,c1,c2):\n",
        "  # you can add incompatibility test here to do just one pass\n",
        "  if(c1.isEqualTo(c2)):\n",
        "    return c1\n",
        "  else:\n",
        "    if(isinstance(c2,Conjunction) and isinstance(c1,Constraint)):\n",
        "      if(not ConstraintinListOfConstraints(c1, c2.elementaryConstraints)):\n",
        "        res.append(Conjunction([c1,c2]))\n",
        "        return Conjunction([c1,c2])\n",
        "      else:\n",
        "        res.append(c2)\n",
        "        return c2\n",
        "    elif(isinstance(c1,Conjunction) and isinstance(c2,Constraint)):\n",
        "      if(not ConstraintinListOfConstraints(c2, c1.elementaryConstraints)):\n",
        "        res.append(Conjunction([c1,c2]))\n",
        "        return Conjunction([c1,c2])\n",
        "      else:\n",
        "        res.append(c1)\n",
        "        return c1\n",
        "    elif(isinstance(c1,Conjunction) and isinstance(c2,Conjunction)):\n",
        "      if(ConstraintinListOfConstraints(c1, c2.constraints)):\n",
        "        res.append(c2)\n",
        "        return c2\n",
        "      elif(ConstraintinListOfConstraints(c2, c1.constraints)):\n",
        "        res.append(c1)\n",
        "        return c1\n",
        "      else:\n",
        "        res.append(Conjunction([c1,c2]))\n",
        "        return Conjunction([c1,c2])\n",
        "    else:\n",
        "      res.append(Conjunction([c1,c2]))\n",
        "      return Conjunction([c1,c2])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def joinNetworks(N1,N2):\n",
        "\n",
        "\n",
        "  if len(N2.constraints) == 0:\n",
        "    return N1\n",
        "\n",
        "\n",
        "\n",
        "  #vars_data = [(i,d,t) for i,d,t in zip(N1.vars_ids,N1.vars_domains,N1.vars_types)]\n",
        "\n",
        "  vars_data = N1.problem_data\n",
        "\n",
        "  already_ids = [k for k in vars_data.keys()]\n",
        "\n",
        "  for id in N2.problem_data.keys():\n",
        "    if(id not in already_ids):\n",
        "      vars_data[id] = N2.problem_data[id]\n",
        "\n",
        "  res = []\n",
        "\n",
        "  df1 = pd.DataFrame({\"consts\": N1.constraints})\n",
        "  df2 = pd.DataFrame({\"consts\": N2.constraints})\n",
        "\n",
        "\n",
        "\n",
        "  dfr = df1.join(df2, lsuffix='_df1', rsuffix='_df2', how=\"cross\")\n",
        "\n",
        "  #print(\"len(delta):\\n\", len(dfr))\n",
        "  #tqdm.pandas()\n",
        "  res = []\n",
        "\n",
        "\n",
        "\n",
        "  #dfr[\"Conjunction\"] = dfr.progress_apply(lambda x: joinConstraints(res,x.consts_df1, x.consts_df2) if not existAlready(res,x.consts_df1, x.consts_df2) else \"eee\", axis=1)\n",
        "  dfr[\"Conjunction\"] = dfr.apply(lambda x: joinConstraints(res,x.consts_df1, x.consts_df2) if not existAlready(res,x.consts_df1, x.consts_df2) and not areIncompatible(vars_data,x.consts_df1, x.consts_df2) else \"eee\", axis=1)\n",
        "\n",
        "\n",
        "  dfr = dfr[dfr[\"Conjunction\"] != \"eee\"]\n",
        "\n",
        "  #dfr[\"AreIncompatible\"] = dfr.progress_apply(lambda x: areIncompatible(vars_data,x.consts_df1, x.consts_df2), axis=1)\n",
        "  # dfr[\"AreIncompatible\"] = dfr.apply(lambda x: areIncompatible(vars_data,x.consts_df1, x.consts_df2), axis=1)\n",
        "\n",
        "\n",
        "  # dfr = dfr[dfr[\"AreIncompatible\"] == False]\n",
        "\n",
        "  res = []\n",
        "  #dfr[\"Conjunction_unique\"] = dfr[\"Conjunction\"].progress_apply(lambda x: x if not existAlready2(res,x) else \"eee\")\n",
        "  dfr[\"Conjunction_unique\"] = dfr[\"Conjunction\"].apply(lambda x: x if not existAlready2(res,x) else \"eee\")\n",
        "\n",
        "  dfr = dfr[dfr[\"Conjunction_unique\"] != \"eee\"]\n",
        "\n",
        "  return Network(vars_data,dfr[\"Conjunction_unique\"].values)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Evkl_tidu8w"
      },
      "outputs": [],
      "source": [
        "# def joinNetworks(n1,n2):\n",
        "#   if(len(n1.constraints)==0):\n",
        "#     return n2\n",
        "\n",
        "#   if(len(n2.constraints)==0):\n",
        "#     return n1\n",
        "\n",
        "\n",
        "#   vars_data = n1.problem_data\n",
        "\n",
        "#   already_ids = [k for k in vars_data.keys()]\n",
        "\n",
        "#   for id in n2.problem_data.keys():\n",
        "#     if(id not in already_ids):\n",
        "#       vars_data[id] = n2.problem_data[id]\n",
        "\n",
        "#   resConjs = []\n",
        "#   resElementary = []\n",
        "\n",
        "#   for c1 in n1.constraints:\n",
        "#     for c2 in n2.constraints:\n",
        "#       if(not c1.isEqualTo(c2) and Network(vars_data, [c1,c2]).solve()!=[]):\n",
        "#         conj = Conjunction([c1,c2])\n",
        "#         exist = False\n",
        "#         for co in resConjs:\n",
        "#           if(co.isEqualTo(conj)):\n",
        "#             #exist already\n",
        "#             exist = True\n",
        "#             break\n",
        "\n",
        "#         if(not exist):\n",
        "#           resConjs.append(conj)\n",
        "\n",
        "#       else:\n",
        "#         existE = False\n",
        "\n",
        "#         for e in resElementary:\n",
        "#           if(e.isEqualTo(c1)):\n",
        "#             existE = True\n",
        "#             break\n",
        "\n",
        "#         if(not existE):\n",
        "#           implied = False\n",
        "#           for co in resConjs:\n",
        "#             notc = deepcopy(c1)\n",
        "#             notc.isNegation = True\n",
        "#             if( Network(vars_data, [co, notc]).solve==[]):\n",
        "#               implied=True\n",
        "#               break\n",
        "\n",
        "#           if not implied:\n",
        "#             resElementary.append(c1)\n",
        "\n",
        "\n",
        "\n",
        "#   return Network(vars_data, resConjs + resElementary)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-wdYl89uedz7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lngFHC6GqPy7"
      },
      "source": [
        "## FindEPrime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p3FYiESIqSgK"
      },
      "outputs": [],
      "source": [
        "# from tqdm import tqdm\n",
        "# def isListConstsSubSetOtherListConsts(a,b):\n",
        "#   if(a == []):\n",
        "#     return True\n",
        "\n",
        "#   if(a == [] and b== []):\n",
        "#     return True\n",
        "\n",
        "#   if(a != [] and b == []):\n",
        "#     return False\n",
        "\n",
        "#   if(len(a)>len(b)):\n",
        "#     return False\n",
        "\n",
        "\n",
        "#   for aa in tqdm(a, ascii=\"True\", desc=\"SubList\"):\n",
        "#     cpt = 0\n",
        "#     for bb in b:\n",
        "#       if(aa.isEqualTo(bb)):\n",
        "#         cpt+=1\n",
        "\n",
        "#     if(cpt==0):\n",
        "#       return False\n",
        "\n",
        "#   return True\n",
        "\n",
        "\n",
        "# from random import random\n",
        "\n",
        "\n",
        "\n",
        "# # This was borrowed from ortools official documentation, it catches all the solutions\n",
        "# class SolutionsCallBack(cp.solvers.ortools.OrtSolutionCounter):\n",
        "\n",
        "\n",
        "#     def __init__(self, solver ,variables, delta):\n",
        "#         cp.solvers.ortools.OrtSolutionCounter.__init__(self)\n",
        "\n",
        "\n",
        "#         self.__solution_count = 0\n",
        "#         self.__solutions = []\n",
        "\n",
        "#         self.delta = delta\n",
        "#         self.ds = len(delta.constraints)\n",
        "#         self._cpm_vars = solver.user_vars\n",
        "#         self._varmap = solver._varmap\n",
        "\n",
        "#         self.solver_vars = solver.solver_vars(list(self._cpm_vars))\n",
        "\n",
        "#         self.__variables = variables\n",
        "\n",
        "#         self.variables2 = {}\n",
        "\n",
        "#         for k in self.__variables.keys():\n",
        "#           for v in self.solver_vars:\n",
        "#             if self.__variables[k].name == v.Name():\n",
        "#               self.variables2[k] = v\n",
        "\n",
        "\n",
        "#         self.solver = solver\n",
        "\n",
        "\n",
        "#         self.example = None\n",
        "\n",
        "\n",
        "\n",
        "#     def on_solution_callback(self):\n",
        "#         super().on_solution_callback()\n",
        "#         self.__solution_count+=1\n",
        "\n",
        "#         e = []\n",
        "#         for v in self.variables2.keys():\n",
        "#           e.append(\n",
        "#               (v, self.variables2[v].Name() , self.Value(self.variables2[v]))\n",
        "#           )\n",
        "\n",
        "#         print(\"\\n Inside Find Eprime Solution nb: \", self.solution_count())\n",
        "\n",
        "#         d = self.delta.networkOfConstraintsThatRejectE(e).constraints\n",
        "\n",
        "#         if( d!=[] and len(d)!= self.ds ):\n",
        "#           self.example = e\n",
        "#           self.StopSearch()\n",
        "\n",
        "#         # print(\"solution nb \",self.solution_count(),\" : \",e)\n",
        "\n",
        "#     def solution_count(self):\n",
        "#         return self.__solution_count\n",
        "\n",
        "#     def allSolutions(self):\n",
        "#       return self.__solutions\n",
        "\n",
        "# # def deltaIsImpliedByLY(delta,ll):\n",
        "\n",
        "\n",
        "# #   cpt=0\n",
        "# #   while(cpt<len(delta.constraints)):\n",
        "# #     notc = deepcopy(delta.constraints[cpt])\n",
        "# #     notc.isNegation = True\n",
        "\n",
        "# #     lll = deepcopy(ll)\n",
        "\n",
        "# #     lll = lll.addConstraint(notc)\n",
        "\n",
        "# #     e = lll.solve()\n",
        "\n",
        "# #     if e == []:\n",
        "# #       cpt+=1\n",
        "# #     else:\n",
        "# #       return False\n",
        "\n",
        "# #   return True\n",
        "\n",
        "\n",
        "# def findEPrime(L,Y,delta):\n",
        "\n",
        "#   ds = len(delta.constraints)\n",
        "\n",
        "#   if(ds==0 or ds==1):\n",
        "#     return [], None, None\n",
        "\n",
        "\n",
        "#   ll = L.ConstraintsIncludedInY(Y)\n",
        "\n",
        "#   # if deltaIsImpliedByLY(delta,ll):\n",
        "#   #   return []\n",
        "\n",
        "\n",
        "#   model = cp.Model()\n",
        "\n",
        "#   #vars = { id:  cp.intvar(dom[0],dom[1], shape=1, name=t+str(id))  for id,dom,t in zip(ll.vars_ids, ll.vars_domains, ll.vars_types) }\n",
        "#   vars = { id:  cp.intvar(ll.problem_data[id][\"domain\"][0],ll.problem_data[id][\"domain\"][1], shape=1, name=ll.problem_data[id][\"name\"])  for id in ll.problem_data.keys()}\n",
        "#   for c in ll.constraints:\n",
        "#     c.prepareCpModelConstraint(vars,model)\n",
        "\n",
        "#   for v in vars.keys():\n",
        "#       model += vars[v] >= vars[v].lb\n",
        "\n",
        "#   s = cp.SolverLookup.get(\"ortools\", model) # faster on a solver interface directly\n",
        "#   cb = SolutionsCallBack(s, vars, delta)\n",
        "\n",
        "#   s.solve(time_limit=20,enumerate_all_solutions= True, solution_callback=cb)\n",
        "\n",
        "#   if(cb.example != None):\n",
        "#     return cb.example, ll, delta\n",
        "#   else:\n",
        "#     return findEPrimeReification(L,Y,delta)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RSLPz23JXkSF"
      },
      "outputs": [],
      "source": [
        "# def solve2(ll,scope):\n",
        "\n",
        "#   #b = ll.ConstraintsIncludedInY(scope)\n",
        "#   r = ll.solve()\n",
        "#   if( r == []):\n",
        "#     return []\n",
        "#   else:\n",
        "#     #return projection(r,scope)\n",
        "#     return r\n",
        "\n",
        "# redundants2 = []\n",
        "# def findEPrime(L,Y,d):\n",
        "\n",
        "#   delta = deepcopy(d)\n",
        "#   ll = deepcopy(L.ConstraintsIncludedInY(Y))\n",
        "\n",
        "\n",
        "#   while(len(delta.constraints)!=0):\n",
        "#     notc = deepcopy(delta.constraints[0])\n",
        "\n",
        "#     notc.isNegation = True\n",
        "\n",
        "#     ll = ll.addConstraint(notc)\n",
        "#     res = solve2(ll, notc.scope_ids ) if isinstance(notc, Constraint) else solve2(ll, notc.scope )\n",
        "\n",
        "#     if(res == []):\n",
        "#       redundants2.append(delta.constraints[0])\n",
        "#       ll = ll.addConstraint(delta.constraints[0])\n",
        "#       delta = delta.removeListOfConstraints([delta.constraints[0]])\n",
        "#     else:\n",
        "#       if(len(d.networkOfConstraintsThatRejectE(res).constraints)== 0 or len(d.networkOfConstraintsThatRejectE(res).constraints)==len(d.constraints)):\n",
        "#         return []\n",
        "#       else:\n",
        "#         return res\n",
        "\n",
        "\n",
        "#   ll = ll.removeListOfConstraints(redundants2)\n",
        "#   return []\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0MdNc90rzHGL"
      },
      "outputs": [],
      "source": [
        "from copy import deepcopy\n",
        "findEPrimeCall = 0\n",
        "\n",
        "#def findEPrimeReification(L,Y,d):\n",
        "def findEPrime(L,Y,d):\n",
        "  ds = len(d.constraints)\n",
        "\n",
        "  if(ds==0 or ds==1):\n",
        "    return [], None, None\n",
        "\n",
        "\n",
        "  C = deepcopy(L.ConstraintsIncludedInY(Y))\n",
        "\n",
        "  vars = { id:  cp.intvar(C.problem_data[id][\"domain\"][0],C.problem_data[id][\"domain\"][1], shape=1, name=C.problem_data[id][\"name\"])  for id in C.problem_data.keys() }\n",
        "\n",
        "  model4 = cp.Model()\n",
        "\n",
        "\n",
        "  bools = [cp.boolvar(name=\"bool\"+str(i+1)) for i in range(len(d.constraints))]\n",
        "\n",
        "\n",
        "\n",
        "  elementary = [c for c in d.constraints if isinstance(c,Constraint)]\n",
        "  Conjs = [c for c in d.constraints if isinstance(c,Conjunction)]\n",
        "\n",
        "  cpt = 0\n",
        "  for c in elementary:\n",
        "    c.prepareCpModelConstraintReification(vars, model4, bools[cpt])\n",
        "    cpt+=1\n",
        "\n",
        "  for c in Conjs:\n",
        "    c.prepareCpModelConstraintReification(vars, model4, bools[cpt])\n",
        "    cpt+=1\n",
        "\n",
        "  #objvar = cp.intvar(1, len(bools)-1, shape=1, name=\"obj\")\n",
        "\n",
        "  #model += objvar >= objvar.lb\n",
        "  model4 += cp.all([(sum(bools) >= 1),(sum(bools) < len(d.constraints))])\n",
        "\n",
        "\n",
        "\n",
        "  for c in C.constraints:\n",
        "    c.prepareCpModelConstraint(vars,model4)\n",
        "\n",
        "  for v in vars.keys():\n",
        "      model4 += vars[v] >= vars[v].lb\n",
        "\n",
        "\n",
        "  #model += sum(bools) != 0\n",
        "\n",
        "  if(model4.solve(solver=\"exact\")):\n",
        "      e = []\n",
        "      for v in vars.keys():\n",
        "        e.append(\n",
        "            (v, vars[v].name, vars[v].value())\n",
        "          )\n",
        "      return projection(e, Y), C, d\n",
        "  else:\n",
        "    return [], C, d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cdzeQKC8D39r"
      },
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "\n",
        "# n = 4\n",
        "# ub = n * n + 1\n",
        "\n",
        "# vars_ids =   [i+1 for i in range(n)]\n",
        "# vars_names = [\"m\"+str(i+1) for i in range(n)]\n",
        "# vars_types = [\"marker\" for i in range(n)]\n",
        "# vars_domains = [(0,0)]\n",
        "# vars_domains = vars_domains + [(1,ub) for i in range(n-1)]\n",
        "# types = [\"marker\"]\n",
        "\n",
        "\n",
        "# problem_data = {i+1: {\"domain\": (1,ub), \"name\": \"m\"+str(i+1),\"type\": \"marker\"} for i in range(n)}\n",
        "# problem_data[1][\"domain\"] = (0,0)\n",
        "# print(problem_data)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# def eqDist(a,b,c,d):\n",
        "#   return operator.eq( operator.abs(operator.sub(a,b)) ,  operator.abs(operator.sub(c,d)))\n",
        "\n",
        "# def neDist(a,b,c,d):\n",
        "#   return operator.ne( operator.abs(operator.sub(a,b)) ,  operator.abs(operator.sub(c,d)))\n",
        "\n",
        "\n",
        "# def eqDist3(repeat,b,c):\n",
        "#   return operator.eq( operator.abs(operator.sub(repeat,b)) ,  operator.abs(operator.sub(c,repeat)))\n",
        "\n",
        "# def neDist3(repeat,b,c):\n",
        "#   return operator.ne( operator.abs(operator.sub(repeat,b)) ,  operator.abs(operator.sub(c,repeat)))\n",
        "\n",
        "\n",
        "# language = [\n",
        "#     (operator.ge, 2, False,None),\n",
        "#     (operator.le, 2, False,None),\n",
        "#     (operator.gt, 2, False,None),\n",
        "#     (operator.lt, 2, False,None),\n",
        "#     (operator.ne, 2, True,None),\n",
        "#     (operator.eq, 2, True,None),\n",
        "#     (eqDist, 4, True,None),\n",
        "#     (neDist, 4, True,None),\n",
        "#     (eqDist3, 3, False,None),\n",
        "#     (neDist3, 3, False,None)\n",
        "# ]\n",
        "\n",
        "# B = Basis(problem_data , language)\n",
        "# B.build()\n",
        "# # B = B.removeDuplicates()\n",
        "\n",
        "# targetconstraints = []\n",
        "\n",
        "\n",
        "# # there is an increasing constraint between markers\n",
        "# for i in range(n-1):\n",
        "#   targetconstraints.append(Constraint([vars_ids[i],vars_ids[i+1]], operator.lt, 2,False))\n",
        "\n",
        "\n",
        "# distances = list(combinations(range(1,n+1), 2))\n",
        "\n",
        "\n",
        "# for i in range(len(distances)-1):\n",
        "#   for j in range(i+1,len(distances)):\n",
        "#     p = list(distances[i])+list(distances[j])\n",
        "#     pp = np.array(p)\n",
        "#     if(len(np.unique(pp)) == len(pp)):\n",
        "#       targetconstraints.append(Constraint(p, neDist , 4,True))\n",
        "#     else:\n",
        "#       if(p[0]==p[2]):\n",
        "#         targetconstraints.append(Constraint([p[0],p[1],p[3]], neDist3 , 3,False))\n",
        "#       elif(p[0]==p[3]):\n",
        "#         targetconstraints.append(Constraint([p[0],p[1],p[2]], neDist3 , 3,False))\n",
        "#       elif(p[1]==p[2]):\n",
        "#         targetconstraints.append(Constraint([p[1],p[0],p[3]], neDist3 , 3,False))\n",
        "#       elif(p[1]==p[3]):\n",
        "#         targetconstraints.append(Constraint([p[1],p[0],p[2]], neDist3 , 3,False))\n",
        "\n",
        "\n",
        "\n",
        "# Target = Network( problem_data, targetconstraints)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "alT2O1K2EXmg"
      },
      "outputs": [],
      "source": [
        "# findEPrime(Target, [1,2,3], B)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EkJ0KiSLEth0"
      },
      "outputs": [],
      "source": [
        "#n1 = Network(problem_data,B.ConstraintsIncludedInY([1,2,3]))\n",
        "# n2 = Network(problem_data,B.ConstraintsIncludedInY([1,2,3]).constraints)\n",
        "\n",
        "#nn = joinNetworks(n1,n2)\n",
        "\n",
        "# for c in nn.constraints:\n",
        "#   print(c.rel, c.scope_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5jHMupEOFWnX"
      },
      "outputs": [],
      "source": [
        "# findEPrime(Target, [1,2,3], n2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xlXHmWhMegtZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtB1H314FsFv"
      },
      "source": [
        "## FindC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q1iAD7fXVeJK"
      },
      "outputs": [],
      "source": [
        "def ConstraintinListOfConstraints(a,listOfConsts):\n",
        "\n",
        "  for l in listOfConsts:\n",
        "    if(a.isEqualTo(l)):\n",
        "      return True\n",
        "\n",
        "  return False\n",
        "\n",
        "\n",
        "def existAlready2(res,c):\n",
        "\n",
        "    if(ConstraintinListOfConstraints(c, res)):\n",
        "        return True\n",
        "    res.append(c)\n",
        "    return False\n",
        "\n",
        "def removeDuplicates(listOfConstraints):\n",
        "  res = []\n",
        "  nonDup = []\n",
        "  for c in listOfConstraints:\n",
        "    if(existAlready2(res,c)):\n",
        "      continue\n",
        "    else:\n",
        "      nonDup.append(c)\n",
        "\n",
        "  # print(\"\\nBefore remove inside function: \", len(listOfConstraints))\n",
        "  # print(\"\\nAfter remove inside function: \", len(nonDup))\n",
        "  return nonDup\n",
        "\n",
        "def FindC(e,Y,L,B, TimeLimit = None, SolutionLimit = None):\n",
        "  global cpt\n",
        "  global newsize\n",
        "  global oldsizeexamples\n",
        "  global newsizeexamples\n",
        "  global nbExamplesc\n",
        "  global nbExamplesa\n",
        "  global Exampletimes\n",
        "  global byQuAcq\n",
        "  global newsizenlpqueries1\n",
        "  global newsizenlpqueries2\n",
        "  global newsizenlpqueries3\n",
        "  global nbConfirmationQueriesC\n",
        "  global nbConfirmationQueriesA\n",
        "  global oldsizenlpqueries2\n",
        "  global byNLP\n",
        "  global Bbk\n",
        "\n",
        "\n",
        "  delta = []\n",
        "\n",
        "  delta = B.ConstraintsIsExactlyY(Y)\n",
        "\n",
        "  print(\"Inside FindC-----------------------------------: \\n\\n\\n\")\n",
        "  print(\"e: \", e)\n",
        "  print(\"\\nY: \\n\",Y,\"\\n   B.ConstraintsIsExactlyY(Y): \",[(c.rel,c.scope_ids) for c in B.ConstraintsIsExactlyY(Y).constraints])\n",
        "  tmpp = delta.networkOfConstraintsThatRejectE(e)\n",
        "  print(\"\\n  delta.networkOfConstraintsThatRejectE(e): \",[(c.rel,c.scope_ids) for c in delta.networkOfConstraintsThatRejectE(e).constraints])\n",
        "  # tmpp.constraints = removeDuplicates(tmpp.constraints)\n",
        "\n",
        "  delta = joinNetworks( delta , tmpp)\n",
        "  print(\"\\n  delta: \",[(c.rel,c.scope_ids) for c in delta.constraints])\n",
        "\n",
        "  global findEPrimeCall\n",
        "  while(True):\n",
        "\n",
        "    # print(\"\\nSTARTSTARTSTARTSTARTSTARTSTARTSTART findEPrimeCall: \",findEPrimeCall,\"---------------------\\n\\n\\n\")\n",
        "    # print(\"\\n\\nDelta before findEPrime: \\n\")\n",
        "    # for c in delta.constraints:\n",
        "    #   print(c.rel, c.scope_ids)\n",
        "\n",
        "\n",
        "    # print(\"\\n\\nL[Y] before findEPrime: \\n\")\n",
        "    # for c in L.ConstraintsIncludedInY(Y).constraints:\n",
        "    #   print(c.rel, c.scope_ids)\n",
        "\n",
        "\n",
        "    stg = time.time()\n",
        "    ep, ll, d = findEPrime(L,Y,delta)\n",
        "    #register example sizes and times to generate them\n",
        "    Exampletimes.append(time.time()-stg)\n",
        "\n",
        "    #calculate number of examples generated until convergence and until finding an equivalent network\n",
        "    nbExamplesc+=1\n",
        "    if(newsizeexamples > oldsizeexamples):\n",
        "      oldsizeexamples = newsizeexamples\n",
        "      if(nbExamplesc > nbExamplesa):\n",
        "        nbExamplesa = nbExamplesc\n",
        "\n",
        "\n",
        "    # print(\"\\n Example: \", ep)\n",
        "\n",
        "    # print(\"\\n\\nDelta returned from findEPrime: \\n\")\n",
        "    # for c in delta.constraints:\n",
        "    #   print(c.rel, c.scope_ids)\n",
        "\n",
        "\n",
        "    # print(\"\\n\\nL[Y] returned from findEPrime: \\n\")\n",
        "    # for c in L.ConstraintsIncludedInY(Y).constraints:\n",
        "    #   print(c.rel, c.scope_ids)\n",
        "\n",
        "    # print(\"\\nENDENDENDENDENDENDENDENDENDENDEND findEPrimeCall: \",findEPrimeCall,\"---------------------\\n\\n\\n\")\n",
        "    findEPrimeCall+=1\n",
        "    cadded = None\n",
        "    if ep == []:\n",
        "      #print(\"\\n A new constraint was added to the being learned network !!!:\\n\")\n",
        "\n",
        "      print(\"I found a list of possible constraints, now I will ask you to choose one that is realy in the set of constraints you have in your mind\\n\\n\")\n",
        "\n",
        "      elementary = [c for c in delta.constraints if isinstance(c,Constraint)]\n",
        "      Conjs = [c for c in delta.constraints if isinstance(c,Conjunction)]\n",
        "\n",
        "      # print(\"The candidates!!!!!!!!!!!!!!!!!: \\n\")\n",
        "      # for c in delta.constraints:\n",
        "      #   if isinstance(c,Constraint):\n",
        "      #     print(c.rel,c.scope_ids,c.isCommutative,c.parameters,c.arity)\n",
        "      #   else:\n",
        "      #     print(c.rel, c.scope_ids)\n",
        "\n",
        "      # print(\"The violated constraint!!!!!!!!!!!!!!: \\n\")\n",
        "      # print(cons.rel,cons.scope_ids,cons.isCommutative,cons.parameters,cons.arity)\n",
        "\n",
        "\n",
        "      if(len(elementary) != 0):\n",
        "        for c in elementary:\n",
        "\n",
        "          nbConfirmationQueriesC += 1\n",
        "\n",
        "          if(newsizenlpqueries2 > oldsizenlpqueries2):\n",
        "            oldsizenlpqueries2 = newsizenlpqueries2\n",
        "            if(nbConfirmationQueriesC > nbConfirmationQueriesA):\n",
        "              nbConfirmationQueriesA = nbConfirmationQueriesC\n",
        "\n",
        "          print(\"Is this a part of what you are looking for ?:\\n\", c.rel,c.scope_ids,c.isCommutative,c.parameters,c.arity)\n",
        "          if ConstraintinListOfConstraints(c, Target.constraints):\n",
        "          #if c.rel == cons.rel and c.scope_ids == cons.scope_ids and c.parameters == cons.parameters and c.isCommutative == cons.isCommutative and c.arity == cons.arity:\n",
        "            cadded = c\n",
        "            break\n",
        "          else:\n",
        "            continue\n",
        "      else:\n",
        "        for c in Conjs:\n",
        "          nbConfirmationQueriesC += 1\n",
        "\n",
        "          if(newsizenlpqueries2 > oldsizenlpqueries2):\n",
        "            oldsizenlpqueries2 = newsizenlpqueries2\n",
        "            if(nbConfirmationQueriesC > nbConfirmationQueriesA):\n",
        "              nbConfirmationQueriesA = nbConfirmationQueriesC\n",
        "\n",
        "          print(\"Is this a part of what you are looking for ?:\\n\", c.rel,c.scope_ids)\n",
        "          if ConstraintinListOfConstraints(c, Target.constraints):\n",
        "          #if c.rel == cons.rel and c.scope_ids == cons.scope_ids and c.parameters == cons.parameters and c.isCommutative == cons.isCommutative and c.arity == cons.arity:\n",
        "            cadded = c\n",
        "            break\n",
        "          else:\n",
        "            continue\n",
        "\n",
        "      #c = delta.constraints[0]\n",
        "\n",
        "      if cadded != None:\n",
        "        print(\"the quacq added constraint: \\n\")\n",
        "        print(cadded.rel,cadded.scope_ids)\n",
        "        L = L.addConstraint(cadded)\n",
        "        byQuAcq+=1\n",
        "        newsize+=1\n",
        "        newsizeexamples+=1\n",
        "        newsizenlpqueries1+=1\n",
        "        newsizenlpqueries2+=1\n",
        "        newsizenlpqueries3+=1\n",
        "\n",
        "        # print(\"\\n Number of constraints so far: \\n\", len(L.constraints))\n",
        "        B = B.removeListOfConstraints(B.ConstraintsIsExactlyY(Y).constraints)\n",
        "        # print(\"\\n FindC : Remaining constraints in the Basis: \\n\", len(B.constraints))\n",
        "        # print(input())\n",
        "        break\n",
        "      else:\n",
        "        print('Here!!!!!!!!! RegularQuAcq2')\n",
        "        L,scope_ids = RegularQuAcq2(Bbk,L)\n",
        "        if scope_ids != None:\n",
        "          B = B.removeListOfConstraints(B.ConstraintsIsExactlyY(scope_ids).constraints)\n",
        "        else:\n",
        "          break\n",
        "\n",
        "    else:\n",
        "      res, cons2 = ask(ep,Target)\n",
        "      if(res):\n",
        "        print(\"inside findc, ep was classified positive\")\n",
        "        delta = delta.removeConstraintsThatRejectE(ep)\n",
        "        B = B.removeConstraintsThatRejectE(ep)\n",
        "        # print(\"\\n FindC : B reduced in size, new size: \",len(B.constraints))\n",
        "      else:\n",
        "        res, scope_ids = conversation(ep, cons2)\n",
        "        if res == \"FindC\":\n",
        "          S = FindScope(ep,[],Y,B)\n",
        "          if set(S).issubset(set(Y)) and len(S)!=len(Y) : #and S!=[]:\n",
        "            FindC(ep,S,L,B)\n",
        "          else:\n",
        "            tmp1 = delta\n",
        "            tmp11 = tmp1.networkOfConstraintsThatRejectE(ep)\n",
        "            delta = joinNetworks(tmp1, tmp11)\n",
        "            print(\"new delta: \", [(c.rel,c.scope_ids) for c in delta.constraints])\n",
        "        else:\n",
        "          if set(scope_ids).issubset(set(Y)) and len(scope_ids)!=len(Y) : #and S!=[]:\n",
        "            L = L.addConstraint(res)\n",
        "            print(\"The nlp added constraint: \\n\")\n",
        "            print(res.rel, res.scope_ids)\n",
        "            print(\"!!!!!!!!!!!!!!!!!!!!!!!!!\\n\\n\")\n",
        "            byNLP+=1\n",
        "            newsize+=1\n",
        "            newsizeexamples+=1\n",
        "            newsizenlpqueries1+=1\n",
        "            newsizenlpqueries2+=1\n",
        "            newsizenlpqueries3+=1\n",
        "            B = B.removeListOfConstraints(B.ConstraintsIsExactlyY(scope_ids).constraints)\n",
        "            print(\"L after nlp: \", len(L.constraints))\n",
        "            break\n",
        "          else:\n",
        "            tmp1 = delta\n",
        "            tmp11 = tmp1.networkOfConstraintsThatRejectE(ep)\n",
        "            delta = joinNetworks(tmp1, tmp11)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88zyiqLhaKfZ"
      },
      "outputs": [],
      "source": [
        "# def FindC(e,Y,L,B, TimeLimit = None, SolutionLimit = None):\n",
        "#   global cpt\n",
        "\n",
        "#   delta = []\n",
        "\n",
        "#   delta = B.ConstraintsIsExactlyY(Y)\n",
        "\n",
        "#   tmpp = delta.networkOfConstraintsThatRejectE(e)\n",
        "\n",
        "#   #tmpp.constraints = removeDuplicates(tmpp.constraints)\n",
        "\n",
        "#   delta = joinNetworks( delta , tmpp)\n",
        "\n",
        "#   print(\"\\n\\n\\n-------len(delta) before loop: \", len(delta.constraints))\n",
        "\n",
        "#   while(True):\n",
        "\n",
        "#     ep = findEPrime(L,Y,delta)\n",
        "\n",
        "#     if ep == []:\n",
        "#       print(\"\\n A new constraint was added to the being learned network !!!:\\n\")\n",
        "#       elementary = [c for c in delta.constraints if isinstance(c,Constraint)]\n",
        "#       Conjs = [c for c in delta.constraints if isinstance(c,Conjunction)]\n",
        "\n",
        "#       if(len(elementary) != 0):\n",
        "#         c = elementary[0]\n",
        "#       else:\n",
        "#         min = 100000\n",
        "#         c = None\n",
        "#         for cc in Conjs:\n",
        "#           if(len(cc.elementaryConstraints)<min):\n",
        "#             min = len(cc.elementaryConstraints)\n",
        "#             c = cc\n",
        "#       #c = delta.constraints[0]\n",
        "\n",
        "\n",
        "#       L = L.addConstraint(c)\n",
        "#       print(\"\\n Number of constraints so far: \\n\", len(L.constraints))\n",
        "#       B = B.removeListOfConstraints(B.ConstraintsIsExactlyY(Y).constraints)\n",
        "#       print(\"\\n FindC : Remaining constraints in the Basis: \\n\", len(B.constraints))\n",
        "#       # print(input())\n",
        "#       break\n",
        "#     else:\n",
        "#       if(ask(ep,Target)):\n",
        "#         delta = delta.removeConstraintsThatRejectE(ep)\n",
        "\n",
        "#         B = B.removeConstraintsThatRejectE(ep)\n",
        "\n",
        "#         print(\"\\n FindC : B reduced in size, new size: \",len(B.constraints))\n",
        "#       else:\n",
        "\n",
        "#         S = FindScope(ep,[],Y,B)\n",
        "#         if set(S).issubset(set(Y)) and len(S)!=len(Y) : #and S!=[]:\n",
        "#           FindC(ep,S,L,B,TimeLimit, SolutionLimit)\n",
        "#         else:\n",
        "#           tmp1 = delta\n",
        "#           tmp11 = tmp1.networkOfConstraintsThatRejectE(ep)\n",
        "#           delta = joinNetworks(tmp1, tmp11)\n",
        "\n",
        "#           print(\"\\nAfter join findcloop: \")\n",
        "#           print(len(delta.constraints))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t189NSesf1aI"
      },
      "source": [
        "# Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBDz0zpOf40l",
        "outputId": "60b13856-61df-42ca-a228-ec20ea80e664"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1oJPm8450UhQLCKLK8hw7JGAFPFAzaHn0\n",
            "To: /content/classif23classes.ckpt\n",
            "100% 1.31G/1.31G [00:07<00:00, 172MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown 1oJPm8450UhQLCKLK8hw7JGAFPFAzaHn0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nYQXOnn-f-3Z"
      },
      "outputs": [],
      "source": [
        "#REFERENCE: https://towardsdatascience.com/fine-tuning-bert-for-text-classification-54e7df642894"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 720,
          "referenced_widgets": [
            "5da4096a24ae4691a3d20bcca9278272",
            "c8b2e4e980f04257b9876af1d5f4d365",
            "1bf04e4767c24979aa93d8da6b5cfb6a",
            "b8fcefa300c0407aa69878e6405bbe13",
            "2eabec30ec5346dfb53861b1984ed398",
            "f4fc4579be3347aeb32c155d58e4bf36",
            "bc56505050b74125a0fb11b042b30b06",
            "ab5f5a61e7ff420e825b3f012d0e0259",
            "af408fcfb7744de08db2f7babcd4d154",
            "ed968e3a84874874afb11cb7c1941857",
            "8325c4009258417fb79517f9dab44ae9",
            "65fee762246148c393bce4667f6c9397",
            "964b43c83d1d493087ec6ce35a854689",
            "1495e1f5cfb5412d98ea261be651bcc1",
            "561b76c2616743f192632ab825c16056",
            "491cc9441174445d9a00566e9628245c",
            "63c12f8a94d8446fb569dd238be1cc6d",
            "cf2f21f3dc11484c947774ca097dc895",
            "7c265db26c18465587a074d2a1bb1b91",
            "b95a569e4e404ebc8bdbe77aa5d581f9",
            "7c5bde83780e438a9d1e96919b16d9ba",
            "12ce4959d8b64323820af60036dcc310",
            "edadf3227c074e01af7471fe04c2063f",
            "1becd7b8b8984db68a9d7230ccec62a1",
            "e693dfe2f25b4e0a8916ab0b1d656c63",
            "6474146c3499474c86093d1eb6e4a642",
            "61c18dd60c0a4931ac5275fa9d511075",
            "4b233aca09d2451398339fdf68e56b7a",
            "b87676ae6c11497a97737e6982342506",
            "7ffb368a12c44ea48026df7f1f22e74e",
            "6007dce4162949438efa923fc059e12b",
            "ba92eefb1d5f4ab2a17144c168e50110",
            "26d16d1fb7714e869a7574aa822ab522",
            "a957ca83c4fd48c5a5da2384fbc6afd6",
            "bf5b740c76094c27a40e5b5ffa4e5923",
            "5fd1066b1f2e490dae7fe7956a70b945",
            "5294c7640a7147548782deeea8ff2a3c",
            "c6626b7e561348be868629d9ee5fd286",
            "343861c4eb7a486fb665fa63aaae697d",
            "95b7aeaa7c9748278f3dfe2200233d8a",
            "ef1109521d064012ac8d0df67448ee34",
            "3575b6f569b9479b8cee46ea065962b1",
            "9b2f183d9d9849ab8370c790c6f2061c",
            "7cb0800845ee42afab867ba000d8194b"
          ]
        },
        "id": "5I7VKhZdgs77",
        "outputId": "7f86a403-8586-4139-cbc3-a0ec6f861009"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.30.2\n",
            "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.2) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers==4.30.2)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.2) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.2) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.2) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.2) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.2) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.30.2)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m110.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers==4.30.2)\n",
            "  Downloading safetensors-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m79.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.2) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.2) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.2) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.30.2) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.30.2) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.30.2) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.30.2) (2023.7.22)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.16.4 safetensors-0.3.2 tokenizers-0.13.3 transformers-4.30.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5da4096a24ae4691a3d20bcca9278272"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "65fee762246148c393bce4667f6c9397"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "edadf3227c074e01af7471fe04c2063f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a957ca83c4fd48c5a5da2384fbc6afd6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers==4.30.2\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from tabulate import tabulate\n",
        "from tqdm import trange\n",
        "import random\n",
        "\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\n",
        "    'bert-base-uncased',\n",
        "    do_lower_case = True\n",
        "    )\n",
        "\n",
        "\n",
        "def preprocessing(input_text, tokenizer):\n",
        "  '''\n",
        "  Returns <class transformers.tokenization_utils_base.BatchEncoding> with the following fields:\n",
        "    - input_ids: list of token ids\n",
        "    - token_type_ids: list of token type ids\n",
        "    - attention_mask: list of indices (0,1) specifying which tokens should considered by the model (return_attention_mask = True).\n",
        "  '''\n",
        "  return tokenizer.encode_plus(\n",
        "                        input_text,\n",
        "                        add_special_tokens = True,\n",
        "                        max_length = 128,\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,\n",
        "                        return_tensors = 'pt'\n",
        "                   )\n",
        "\n",
        "def loadClassifModel(modelPath = \"/content/classif23classes.ckpt\"):\n",
        "  # Load the BertForSequenceClassification model\n",
        "  model = BertForSequenceClassification.from_pretrained(\n",
        "      'bert-base-uncased',\n",
        "      num_labels = 24,\n",
        "      output_attentions = False,\n",
        "      output_hidden_states = False,\n",
        "  )\n",
        "\n",
        "  # Recommended learning rates (Adam): 5e-5, 3e-5, 2e-5. See: https://arxiv.org/pdf/1810.04805.pdf\n",
        "  optimizer = torch.optim.AdamW(model.parameters(),\n",
        "                                lr = 5e-5,\n",
        "                                eps = 1e-08\n",
        "                                )\n",
        "\n",
        "  # Run on GPU\n",
        "  model.cuda()\n",
        "\n",
        "  checkpoint = torch.load(modelPath)\n",
        "  model.load_state_dict(checkpoint['model_state_dict'])\n",
        "  optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "  epoch = checkpoint['epoch']\n",
        "  loss = checkpoint['loss']\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def classify(sentence, classifModel):\n",
        "  global device\n",
        "  # We need Token IDs and Attention Mask for inference on the new sentence\n",
        "  test_ids = []\n",
        "  test_attention_mask = []\n",
        "\n",
        "  # Apply the tokenizer\n",
        "  encoding = preprocessing(sentence, tokenizer)\n",
        "\n",
        "  # Extract IDs and Attention Mask\n",
        "  test_ids.append(encoding['input_ids'])\n",
        "  test_attention_mask.append(encoding['attention_mask'])\n",
        "  test_ids = torch.cat(test_ids, dim = 0)\n",
        "  test_attention_mask = torch.cat(test_attention_mask, dim = 0)\n",
        "\n",
        "  # Forward pass, calculate logit predictions\n",
        "  with torch.no_grad():\n",
        "    output = classifModel(test_ids.to(device), token_type_ids = None, attention_mask = test_attention_mask.to(device))\n",
        "\n",
        "  #prediction = 'ALL_EQUAL' if np.argmax(output.logits.cpu().numpy()).flatten().item() == 1 else 'ALL_DIFFERENT'\n",
        "\n",
        "\n",
        "\n",
        "  if np.argmax(output.logits.cpu().numpy()).flatten().item() == 0:\n",
        "    prediction = \"BINARY_gt\"\n",
        "  elif np.argmax(output.logits.cpu().numpy()).flatten().item() == 1:\n",
        "    prediction = \"BINARY_lt\"\n",
        "  elif np.argmax(output.logits.cpu().numpy()).flatten().item() == 2:\n",
        "    prediction = \"BINARY_ge\"\n",
        "  elif np.argmax(output.logits.cpu().numpy()).flatten().item() == 3:\n",
        "    prediction = \"BINARY_le\"\n",
        "  elif np.argmax(output.logits.cpu().numpy()).flatten().item() == 4:\n",
        "    prediction = \"BINARY_eq\"\n",
        "  elif np.argmax(output.logits.cpu().numpy()).flatten().item() == 5:\n",
        "    prediction = \"BINARY_ne\"\n",
        "  elif np.argmax(output.logits.cpu().numpy()).flatten().item() == 6:\n",
        "    prediction = \"BINARYDIST_gt\"\n",
        "  elif np.argmax(output.logits.cpu().numpy()).flatten().item() == 7:\n",
        "    prediction = \"BINARYDIST_lt\"\n",
        "  elif np.argmax(output.logits.cpu().numpy()).flatten().item() == 8:\n",
        "    prediction = \"BINARYDIST_ge\"\n",
        "  elif np.argmax(output.logits.cpu().numpy()).flatten().item() == 9:\n",
        "    prediction = \"BINARYDIST_le\"\n",
        "  elif np.argmax(output.logits.cpu().numpy()).flatten().item() == 10:\n",
        "    prediction = \"BINARYDIST_eq\"\n",
        "  elif np.argmax(output.logits.cpu().numpy()).flatten().item() == 11:\n",
        "    prediction = \"BINARYDIST_ne\"\n",
        "  elif np.argmax(output.logits.cpu().numpy()).flatten().item() == 12:\n",
        "    prediction = \"UNARYDIST_gt\"\n",
        "  elif np.argmax(output.logits.cpu().numpy()).flatten().item() == 13:\n",
        "    prediction = \"UNARYDIST_lt\"\n",
        "  elif np.argmax(output.logits.cpu().numpy()).flatten().item() == 14:\n",
        "    prediction = \"UNARYDIST_ge\"\n",
        "  elif np.argmax(output.logits.cpu().numpy()).flatten().item() == 15:\n",
        "    prediction = \"UNARYDIST_le\"\n",
        "  elif np.argmax(output.logits.cpu().numpy()).flatten().item() == 16:\n",
        "    prediction = \"UNARYDIST_eq\"\n",
        "  elif np.argmax(output.logits.cpu().numpy()).flatten().item() == 17:\n",
        "    prediction = \"UNARYDIST_ne\"\n",
        "  elif np.argmax(output.logits.cpu().numpy()).flatten().item() == 18:\n",
        "    prediction = \"UNARY_le\"\n",
        "  elif np.argmax(output.logits.cpu().numpy()).flatten().item() == 19:\n",
        "    prediction = \"UNARY_lt\"\n",
        "  elif np.argmax(output.logits.cpu().numpy()).flatten().item() == 20:\n",
        "    prediction = \"UNARY_ge\"\n",
        "  elif np.argmax(output.logits.cpu().numpy()).flatten().item() == 21:\n",
        "    prediction = \"UNARY_le\"\n",
        "  elif np.argmax(output.logits.cpu().numpy()).flatten().item() == 22:\n",
        "    prediction = \"UNARY_eq\"\n",
        "  elif np.argmax(output.logits.cpu().numpy()).flatten().item() == 23:\n",
        "    prediction = \"UNARY_ne\"\n",
        "  else:\n",
        "    prediction = \"UNKOWN\"\n",
        "\n",
        "\n",
        "  return prediction, output.logits.cpu().numpy()\n",
        "\n",
        "classifModel = loadClassifModel()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qV4HsfH_iFrW"
      },
      "outputs": [],
      "source": [
        "# c,l = classify(\"distance between V and C should be greater than gap betwen P and H 8\", classifModel)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l2H7xpo-y6N-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esa4sZNbjnQ3"
      },
      "source": [
        "# Named Entity Recognition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HB7qgEG-kcs-",
        "outputId": "c3400b45-a45c-4fd8-9a05-fca90ef1a309"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ner_playground'...\n",
            "remote: Enumerating objects: 97, done.\u001b[K\n",
            "remote: Counting objects: 100% (97/97), done.\u001b[K\n",
            "remote: Compressing objects: 100% (50/50), done.\u001b[K\n",
            "remote: Total 97 (delta 41), reused 80 (delta 28), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (97/97), 29.60 KiB | 4.93 MiB/s, done.\n",
            "Resolving deltas: 100% (41/41), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/CVxTz/ner_playground.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rZwqhULWkfH4"
      },
      "outputs": [],
      "source": [
        "!mkdir /content/ner_playground/models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OqFteBOAPN2h",
        "outputId": "7c494dc1-04b7-4b12-9660-03459e8c7b26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Ign2-RxfYw-dw1tZmJkoTSWtZnJ6CxLQ\n",
            "To: /content/ner-bert-binaryData-justparams.ckpt\n",
            "100% 438M/438M [00:15<00:00, 27.5MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=19OajoLpAP8PMUYGEU8qzKnEJE-9b1_A1\n",
            "To: /content/ner-bert-distances-justparams.ckpt\n",
            "100% 438M/438M [00:18<00:00, 23.7MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1pDYix4GPjyZZDP29Iq-bncJJzcEOQb3U\n",
            "To: /content/ner-bert-distancesUnary-justparams.ckpt\n",
            "100% 438M/438M [00:16<00:00, 27.0MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=16ewkqljJbK5stGMr8pMxKb2IfRjp1lWa\n",
            "To: /content/ner-bert-unaryConstraints-justparams.ckpt\n",
            "100% 438M/438M [00:17<00:00, 25.5MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown 1Ign2-RxfYw-dw1tZmJkoTSWtZnJ6CxLQ\n",
        "!gdown 19OajoLpAP8PMUYGEU8qzKnEJE-9b1_A1\n",
        "!gdown 1pDYix4GPjyZZDP29Iq-bncJJzcEOQb3U\n",
        "!gdown 16ewkqljJbK5stGMr8pMxKb2IfRjp1lWa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mO0Xen7ikcq0"
      },
      "outputs": [],
      "source": [
        "#!cp /content/drive/MyDrive/BertForNERforCSP/ner-bert-constraints-v2.ckpt /content/ner_playground/models\n",
        "#!cp /content/drive/MyDrive/BertForNERforCSP/ner-bert-alldiffner0.ckpt /content/ner_playground/models\n",
        "!cp /content/ner-bert-binaryData-justparams.ckpt /content/ner_playground/models\n",
        "!cp /content/ner-bert-distances-justparams.ckpt /content/ner_playground/models\n",
        "!cp /content/ner-bert-distancesUnary-justparams.ckpt /content/ner_playground/models\n",
        "!cp /content/ner-bert-unaryConstraints-justparams.ckpt /content/ner_playground/models\n",
        "#!cp /content/drive/MyDrive/BertForNERforCSP/ner-bert-allequalner0.ckpt /content/ner_playground/models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_J_cI7qXkmKk",
        "outputId": "d0bd2805-9b5b-4e39-e965-c649ff5692ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==1.22.3 (from -r /content/ner_playground/requirements.txt (line 1))\n",
            "  Downloading numpy-1.22.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m56.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pandas==1.4.2 (from -r /content/ner_playground/requirements.txt (line 2))\n",
            "  Downloading pandas-1.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.7/11.7 MB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytorch_lightning==1.6.3 (from -r /content/ner_playground/requirements.txt (line 3))\n",
            "  Downloading pytorch_lightning-1.6.3-py3-none-any.whl (584 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m584.0/584.0 kB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting setuptools==61.2.0 (from -r /content/ner_playground/requirements.txt (line 4))\n",
            "  Downloading setuptools-61.2.0-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m75.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers==0.12.1 (from -r /content/ner_playground/requirements.txt (line 5))\n",
            "  Downloading tokenizers-0.12.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m114.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tqdm==4.64.0 (from -r /content/ner_playground/requirements.txt (line 6))\n",
            "  Downloading tqdm-4.64.0-py2.py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers==4.19.2 (from -r /content/ner_playground/requirements.txt (line 7))\n",
            "  Downloading transformers-4.19.2-py3-none-any.whl (4.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m111.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nervaluate==0.1.8 (from -r /content/ner_playground/requirements.txt (line 8))\n",
            "  Downloading nervaluate-0.1.8-py3-none-any.whl (24 kB)\n",
            "Collecting scikit-learn==1.1.2 (from -r /content/ner_playground/requirements.txt (line 9))\n",
            "  Downloading scikit_learn-1.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.5/30.5 MB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas==1.4.2->-r /content/ner_playground/requirements.txt (line 2)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas==1.4.2->-r /content/ner_playground/requirements.txt (line 2)) (2023.3)\n",
            "Requirement already satisfied: torch>=1.8.* in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning==1.6.3->-r /content/ner_playground/requirements.txt (line 3)) (2.0.1+cu118)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning==1.6.3->-r /content/ner_playground/requirements.txt (line 3)) (6.0.1)\n",
            "Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning==1.6.3->-r /content/ner_playground/requirements.txt (line 3)) (2023.6.0)\n",
            "Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning==1.6.3->-r /content/ner_playground/requirements.txt (line 3)) (2.12.3)\n",
            "Collecting torchmetrics>=0.4.1 (from pytorch_lightning==1.6.3->-r /content/ner_playground/requirements.txt (line 3))\n",
            "  Downloading torchmetrics-1.0.3-py3-none-any.whl (731 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.6/731.6 kB\u001b[0m \u001b[31m59.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyDeprecate<0.4.0,>=0.3.1 (from pytorch_lightning==1.6.3->-r /content/ner_playground/requirements.txt (line 3))\n",
            "  Downloading pyDeprecate-0.3.2-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning==1.6.3->-r /content/ner_playground/requirements.txt (line 3)) (23.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning==1.6.3->-r /content/ner_playground/requirements.txt (line 3)) (4.7.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.19.2->-r /content/ner_playground/requirements.txt (line 7)) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.19.2->-r /content/ner_playground/requirements.txt (line 7)) (0.16.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.19.2->-r /content/ner_playground/requirements.txt (line 7)) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.19.2->-r /content/ner_playground/requirements.txt (line 7)) (2.31.0)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.1.2->-r /content/ner_playground/requirements.txt (line 9)) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.1.2->-r /content/ner_playground/requirements.txt (line 9)) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.1.2->-r /content/ner_playground/requirements.txt (line 9)) (3.2.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.6.3->-r /content/ner_playground/requirements.txt (line 3)) (3.8.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas==1.4.2->-r /content/ner_playground/requirements.txt (line 2)) (1.16.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==1.6.3->-r /content/ner_playground/requirements.txt (line 3)) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==1.6.3->-r /content/ner_playground/requirements.txt (line 3)) (1.56.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==1.6.3->-r /content/ner_playground/requirements.txt (line 3)) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==1.6.3->-r /content/ner_playground/requirements.txt (line 3)) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==1.6.3->-r /content/ner_playground/requirements.txt (line 3)) (3.4.4)\n",
            "Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==1.6.3->-r /content/ner_playground/requirements.txt (line 3)) (4.24.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==1.6.3->-r /content/ner_playground/requirements.txt (line 3)) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==1.6.3->-r /content/ner_playground/requirements.txt (line 3)) (2.3.6)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==1.6.3->-r /content/ner_playground/requirements.txt (line 3)) (0.41.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.19.2->-r /content/ner_playground/requirements.txt (line 7)) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.19.2->-r /content/ner_playground/requirements.txt (line 7)) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.19.2->-r /content/ner_playground/requirements.txt (line 7)) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.19.2->-r /content/ner_playground/requirements.txt (line 7)) (2023.7.22)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.*->pytorch_lightning==1.6.3->-r /content/ner_playground/requirements.txt (line 3)) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.*->pytorch_lightning==1.6.3->-r /content/ner_playground/requirements.txt (line 3)) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.*->pytorch_lightning==1.6.3->-r /content/ner_playground/requirements.txt (line 3)) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.*->pytorch_lightning==1.6.3->-r /content/ner_playground/requirements.txt (line 3)) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.*->pytorch_lightning==1.6.3->-r /content/ner_playground/requirements.txt (line 3)) (3.27.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.*->pytorch_lightning==1.6.3->-r /content/ner_playground/requirements.txt (line 3)) (16.0.6)\n",
            "Collecting lightning-utilities>=0.7.0 (from torchmetrics>=0.4.1->pytorch_lightning==1.6.3->-r /content/ner_playground/requirements.txt (line 3))\n",
            "  Downloading lightning_utilities-0.9.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.6.3->-r /content/ner_playground/requirements.txt (line 3)) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.6.3->-r /content/ner_playground/requirements.txt (line 3)) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.6.3->-r /content/ner_playground/requirements.txt (line 3)) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.6.3->-r /content/ner_playground/requirements.txt (line 3)) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.6.3->-r /content/ner_playground/requirements.txt (line 3)) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.6.3->-r /content/ner_playground/requirements.txt (line 3)) (1.3.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning==1.6.3->-r /content/ner_playground/requirements.txt (line 3)) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning==1.6.3->-r /content/ner_playground/requirements.txt (line 3)) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning==1.6.3->-r /content/ner_playground/requirements.txt (line 3)) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.2.0->pytorch_lightning==1.6.3->-r /content/ner_playground/requirements.txt (line 3)) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.2.0->pytorch_lightning==1.6.3->-r /content/ner_playground/requirements.txt (line 3)) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.*->pytorch_lightning==1.6.3->-r /content/ner_playground/requirements.txt (line 3)) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning==1.6.3->-r /content/ner_playground/requirements.txt (line 3)) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.2.0->pytorch_lightning==1.6.3->-r /content/ner_playground/requirements.txt (line 3)) (3.2.2)\n",
            "Installing collected packages: tokenizers, nervaluate, tqdm, setuptools, pyDeprecate, numpy, lightning-utilities, pandas, transformers, scikit-learn, torchmetrics, pytorch_lightning\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.13.3\n",
            "    Uninstalling tokenizers-0.13.3:\n",
            "      Successfully uninstalled tokenizers-0.13.3\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.66.0\n",
            "    Uninstalling tqdm-4.66.0:\n",
            "      Successfully uninstalled tqdm-4.66.0\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 67.7.2\n",
            "    Uninstalling setuptools-67.7.2:\n",
            "      Successfully uninstalled setuptools-67.7.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.23.5\n",
            "    Uninstalling numpy-1.23.5:\n",
            "      Successfully uninstalled numpy-1.23.5\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.5.3\n",
            "    Uninstalling pandas-1.5.3:\n",
            "      Successfully uninstalled pandas-1.5.3\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.30.2\n",
            "    Uninstalling transformers-4.30.2:\n",
            "      Successfully uninstalled transformers-4.30.2\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.2.2\n",
            "    Uninstalling scikit-learn-1.2.2:\n",
            "      Successfully uninstalled scikit-learn-1.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
            "cvxpy 1.3.2 requires setuptools>65.5.1, but you have setuptools 61.2.0 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==1.5.3, but you have pandas 1.4.2 which is incompatible.\n",
            "plotnine 0.12.2 requires numpy>=1.23.0, but you have numpy 1.22.3 which is incompatible.\n",
            "plotnine 0.12.2 requires pandas>=1.5.0, but you have pandas 1.4.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed lightning-utilities-0.9.0 nervaluate-0.1.8 numpy-1.22.3 pandas-1.4.2 pyDeprecate-0.3.2 pytorch_lightning-1.6.3 scikit-learn-1.1.2 setuptools-61.2.0 tokenizers-0.12.1 torchmetrics-1.0.3 tqdm-4.64.0 transformers-4.19.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "_distutils_hack",
                  "numpy",
                  "pandas",
                  "pkg_resources",
                  "setuptools",
                  "sklearn",
                  "tokenizers",
                  "tqdm",
                  "transformers"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install -r /content/ner_playground/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TyXB8c-BkoMQ",
        "outputId": "2e1a5551-aa7f-414c-80b8-c98206d8a637"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: 100% 570/570 [00:00<00:00, 2.67MB/s]\n",
            "Downloading: 100% 420M/420M [00:08<00:00, 49.2MB/s]\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Downloading: 100% 226k/226k [00:00<00:00, 32.1MB/s]\n",
            "Downloading: 100% 28.0/28.0 [00:00<00:00, 132kB/s]\n",
            "embeddings.word_embeddings.weight\n",
            "embeddings.position_embeddings.weight\n",
            "embeddings.token_type_embeddings.weight\n",
            "embeddings.LayerNorm.weight\n",
            "embeddings.LayerNorm.bias\n",
            "encoder.layer.0.attention.self.query.weight\n",
            "encoder.layer.0.attention.self.query.bias\n",
            "encoder.layer.0.attention.self.key.weight\n",
            "encoder.layer.0.attention.self.key.bias\n",
            "encoder.layer.0.attention.self.value.weight\n",
            "encoder.layer.0.attention.self.value.bias\n",
            "encoder.layer.0.attention.output.dense.weight\n",
            "encoder.layer.0.attention.output.dense.bias\n",
            "encoder.layer.0.attention.output.LayerNorm.weight\n",
            "encoder.layer.0.attention.output.LayerNorm.bias\n",
            "encoder.layer.0.intermediate.dense.weight\n",
            "encoder.layer.0.intermediate.dense.bias\n",
            "encoder.layer.0.output.dense.weight\n",
            "encoder.layer.0.output.dense.bias\n",
            "encoder.layer.0.output.LayerNorm.weight\n",
            "encoder.layer.0.output.LayerNorm.bias\n",
            "encoder.layer.1.attention.self.query.weight\n",
            "encoder.layer.1.attention.self.query.bias\n",
            "encoder.layer.1.attention.self.key.weight\n",
            "encoder.layer.1.attention.self.key.bias\n",
            "encoder.layer.1.attention.self.value.weight\n",
            "encoder.layer.1.attention.self.value.bias\n",
            "encoder.layer.1.attention.output.dense.weight\n",
            "encoder.layer.1.attention.output.dense.bias\n",
            "encoder.layer.1.attention.output.LayerNorm.weight\n",
            "encoder.layer.1.attention.output.LayerNorm.bias\n",
            "encoder.layer.1.intermediate.dense.weight\n",
            "encoder.layer.1.intermediate.dense.bias\n",
            "encoder.layer.1.output.dense.weight\n",
            "encoder.layer.1.output.dense.bias\n",
            "encoder.layer.1.output.LayerNorm.weight\n",
            "encoder.layer.1.output.LayerNorm.bias\n",
            "encoder.layer.2.attention.self.query.weight\n",
            "encoder.layer.2.attention.self.query.bias\n",
            "encoder.layer.2.attention.self.key.weight\n",
            "encoder.layer.2.attention.self.key.bias\n",
            "encoder.layer.2.attention.self.value.weight\n",
            "encoder.layer.2.attention.self.value.bias\n",
            "encoder.layer.2.attention.output.dense.weight\n",
            "encoder.layer.2.attention.output.dense.bias\n",
            "encoder.layer.2.attention.output.LayerNorm.weight\n",
            "encoder.layer.2.attention.output.LayerNorm.bias\n",
            "encoder.layer.2.intermediate.dense.weight\n",
            "encoder.layer.2.intermediate.dense.bias\n",
            "encoder.layer.2.output.dense.weight\n",
            "encoder.layer.2.output.dense.bias\n",
            "encoder.layer.2.output.LayerNorm.weight\n",
            "encoder.layer.2.output.LayerNorm.bias\n",
            "encoder.layer.3.attention.self.query.weight\n",
            "encoder.layer.3.attention.self.query.bias\n",
            "encoder.layer.3.attention.self.key.weight\n",
            "encoder.layer.3.attention.self.key.bias\n",
            "encoder.layer.3.attention.self.value.weight\n",
            "encoder.layer.3.attention.self.value.bias\n",
            "encoder.layer.3.attention.output.dense.weight\n",
            "encoder.layer.3.attention.output.dense.bias\n",
            "encoder.layer.3.attention.output.LayerNorm.weight\n",
            "encoder.layer.3.attention.output.LayerNorm.bias\n",
            "encoder.layer.3.intermediate.dense.weight\n",
            "encoder.layer.3.intermediate.dense.bias\n",
            "encoder.layer.3.output.dense.weight\n",
            "encoder.layer.3.output.dense.bias\n",
            "encoder.layer.3.output.LayerNorm.weight\n",
            "encoder.layer.3.output.LayerNorm.bias\n",
            "encoder.layer.4.attention.self.query.weight\n",
            "encoder.layer.4.attention.self.query.bias\n",
            "encoder.layer.4.attention.self.key.weight\n",
            "encoder.layer.4.attention.self.key.bias\n",
            "encoder.layer.4.attention.self.value.weight\n",
            "encoder.layer.4.attention.self.value.bias\n",
            "encoder.layer.4.attention.output.dense.weight\n",
            "encoder.layer.4.attention.output.dense.bias\n",
            "encoder.layer.4.attention.output.LayerNorm.weight\n",
            "encoder.layer.4.attention.output.LayerNorm.bias\n",
            "encoder.layer.4.intermediate.dense.weight\n",
            "encoder.layer.4.intermediate.dense.bias\n",
            "encoder.layer.4.output.dense.weight\n",
            "encoder.layer.4.output.dense.bias\n",
            "encoder.layer.4.output.LayerNorm.weight\n",
            "encoder.layer.4.output.LayerNorm.bias\n",
            "encoder.layer.5.attention.self.query.weight\n",
            "encoder.layer.5.attention.self.query.bias\n",
            "encoder.layer.5.attention.self.key.weight\n",
            "encoder.layer.5.attention.self.key.bias\n",
            "encoder.layer.5.attention.self.value.weight\n",
            "encoder.layer.5.attention.self.value.bias\n",
            "encoder.layer.5.attention.output.dense.weight\n",
            "encoder.layer.5.attention.output.dense.bias\n",
            "encoder.layer.5.attention.output.LayerNorm.weight\n",
            "encoder.layer.5.attention.output.LayerNorm.bias\n",
            "encoder.layer.5.intermediate.dense.weight\n",
            "encoder.layer.5.intermediate.dense.bias\n",
            "encoder.layer.5.output.dense.weight\n",
            "encoder.layer.5.output.dense.bias\n",
            "encoder.layer.5.output.LayerNorm.weight\n",
            "encoder.layer.5.output.LayerNorm.bias\n",
            "encoder.layer.6.attention.self.query.weight\n",
            "encoder.layer.6.attention.self.query.bias\n",
            "encoder.layer.6.attention.self.key.weight\n",
            "encoder.layer.6.attention.self.key.bias\n",
            "encoder.layer.6.attention.self.value.weight\n",
            "encoder.layer.6.attention.self.value.bias\n",
            "encoder.layer.6.attention.output.dense.weight\n",
            "encoder.layer.6.attention.output.dense.bias\n",
            "encoder.layer.6.attention.output.LayerNorm.weight\n",
            "encoder.layer.6.attention.output.LayerNorm.bias\n",
            "encoder.layer.6.intermediate.dense.weight\n",
            "encoder.layer.6.intermediate.dense.bias\n",
            "encoder.layer.6.output.dense.weight\n",
            "encoder.layer.6.output.dense.bias\n",
            "encoder.layer.6.output.LayerNorm.weight\n",
            "encoder.layer.6.output.LayerNorm.bias\n",
            "encoder.layer.7.attention.self.query.weight\n",
            "encoder.layer.7.attention.self.query.bias\n",
            "encoder.layer.7.attention.self.key.weight\n",
            "encoder.layer.7.attention.self.key.bias\n",
            "encoder.layer.7.attention.self.value.weight\n",
            "encoder.layer.7.attention.self.value.bias\n",
            "encoder.layer.7.attention.output.dense.weight\n",
            "encoder.layer.7.attention.output.dense.bias\n",
            "encoder.layer.7.attention.output.LayerNorm.weight\n",
            "encoder.layer.7.attention.output.LayerNorm.bias\n",
            "encoder.layer.7.intermediate.dense.weight\n",
            "encoder.layer.7.intermediate.dense.bias\n",
            "encoder.layer.7.output.dense.weight\n",
            "encoder.layer.7.output.dense.bias\n",
            "encoder.layer.7.output.LayerNorm.weight\n",
            "encoder.layer.7.output.LayerNorm.bias\n",
            "encoder.layer.8.attention.self.query.weight\n",
            "encoder.layer.8.attention.self.query.bias\n",
            "encoder.layer.8.attention.self.key.weight\n",
            "encoder.layer.8.attention.self.key.bias\n",
            "encoder.layer.8.attention.self.value.weight\n",
            "encoder.layer.8.attention.self.value.bias\n",
            "encoder.layer.8.attention.output.dense.weight\n",
            "encoder.layer.8.attention.output.dense.bias\n",
            "encoder.layer.8.attention.output.LayerNorm.weight\n",
            "encoder.layer.8.attention.output.LayerNorm.bias\n",
            "encoder.layer.8.intermediate.dense.weight\n",
            "encoder.layer.8.intermediate.dense.bias\n",
            "encoder.layer.8.output.dense.weight\n",
            "encoder.layer.8.output.dense.bias\n",
            "encoder.layer.8.output.LayerNorm.weight\n",
            "encoder.layer.8.output.LayerNorm.bias\n",
            "encoder.layer.9.attention.self.query.weight\n",
            "encoder.layer.9.attention.self.query.bias\n",
            "encoder.layer.9.attention.self.key.weight\n",
            "encoder.layer.9.attention.self.key.bias\n",
            "encoder.layer.9.attention.self.value.weight\n",
            "encoder.layer.9.attention.self.value.bias\n",
            "encoder.layer.9.attention.output.dense.weight\n",
            "encoder.layer.9.attention.output.dense.bias\n",
            "encoder.layer.9.attention.output.LayerNorm.weight\n",
            "encoder.layer.9.attention.output.LayerNorm.bias\n",
            "encoder.layer.9.intermediate.dense.weight\n",
            "encoder.layer.9.intermediate.dense.bias\n",
            "encoder.layer.9.output.dense.weight\n",
            "encoder.layer.9.output.dense.bias\n",
            "encoder.layer.9.output.LayerNorm.weight\n",
            "encoder.layer.9.output.LayerNorm.bias\n",
            "encoder.layer.10.attention.self.query.weight\n",
            "encoder.layer.10.attention.self.query.bias\n",
            "encoder.layer.10.attention.self.key.weight\n",
            "encoder.layer.10.attention.self.key.bias\n",
            "encoder.layer.10.attention.self.value.weight\n",
            "encoder.layer.10.attention.self.value.bias\n",
            "encoder.layer.10.attention.output.dense.weight\n",
            "encoder.layer.10.attention.output.dense.bias\n",
            "encoder.layer.10.attention.output.LayerNorm.weight\n",
            "encoder.layer.10.attention.output.LayerNorm.bias\n",
            "encoder.layer.10.intermediate.dense.weight\n",
            "encoder.layer.10.intermediate.dense.bias\n",
            "encoder.layer.10.output.dense.weight\n",
            "encoder.layer.10.output.dense.bias\n",
            "encoder.layer.10.output.LayerNorm.weight\n",
            "encoder.layer.10.output.LayerNorm.bias\n",
            "encoder.layer.11.attention.self.query.weight\n",
            "encoder.layer.11.attention.self.query.bias\n",
            "encoder.layer.11.attention.self.key.weight\n",
            "encoder.layer.11.attention.self.key.bias\n",
            "encoder.layer.11.attention.self.value.weight\n",
            "encoder.layer.11.attention.self.value.bias\n",
            "encoder.layer.11.attention.output.dense.weight\n",
            "encoder.layer.11.attention.output.dense.bias\n",
            "encoder.layer.11.attention.output.LayerNorm.weight\n",
            "encoder.layer.11.attention.output.LayerNorm.bias\n",
            "encoder.layer.11.intermediate.dense.weight\n",
            "encoder.layer.11.intermediate.dense.bias\n",
            "encoder.layer.11.output.dense.weight\n",
            "encoder.layer.11.output.dense.bias\n",
            "encoder.layer.11.output.LayerNorm.weight\n",
            "encoder.layer.11.output.LayerNorm.bias\n",
            "pooler.dense.weight\n",
            "pooler.dense.bias\n"
          ]
        }
      ],
      "source": [
        "!python /content/ner_playground/scripts/load_save_bert.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SnK2ZHSskqYC"
      },
      "outputs": [],
      "source": [
        "#!pip install torch==1.12.0 torchvision==0.13.0 torchaudio==0.12.0 torchtext==0.13.0 torchdata==0.4.0 nervaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nkzQkgSyl2yT"
      },
      "outputs": [],
      "source": [
        "#!pip install pytorch_lightning==1.8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xiIaVK0Wk3ZX"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from typing import Dict, List, Optional"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zoAm5jWDlY0w"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from transformers import BertConfig, BertModel\n",
        "\n",
        "\n",
        "CONFIG_PATH = \"/content/ner_playground/bert_model/bert_config.json\"\n",
        "CONFIG = BertConfig.from_json_file(CONFIG_PATH)\n",
        "\n",
        "import torch\n",
        "from transformers import get_cosine_schedule_with_warmup\n",
        "\n",
        "\n",
        "def masked_accuracy(y_true: torch.Tensor, y_pred: torch.Tensor, mask):\n",
        "    y_true = torch.masked_select(y_true, mask)\n",
        "    y_pred = torch.masked_select(y_pred, mask)\n",
        "\n",
        "    acc = (y_true == y_pred).double().mean()\n",
        "\n",
        "    return acc\n",
        "\n",
        "\n",
        "class BaseModel(pl.LightningModule):\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        return self._step(batch, batch_idx, name=\"train\")\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        return self._step(batch, batch_idx, name=\"valid\")\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        return self._step(batch, batch_idx, name=\"test\")\n",
        "\n",
        "    def _step(self, batch, batch_idx, name=\"train\"):\n",
        "        x, y = batch\n",
        "\n",
        "        y_hat = self(x)\n",
        "\n",
        "        y_hat = y_hat.reshape(-1, y_hat.size(2))\n",
        "        y = y.view(-1)\n",
        "\n",
        "        loss = F.cross_entropy(y_hat, y, reduction=\"mean\")\n",
        "\n",
        "        _, predicted = torch.max(y_hat, 1)\n",
        "\n",
        "        mask = x != self.pad_idx\n",
        "        mask = mask.view(-1)\n",
        "\n",
        "        acc = masked_accuracy(y, predicted, mask)\n",
        "\n",
        "        self.log(f\"{name}_loss\", loss)\n",
        "        self.log(f\"{name}_acc\", acc)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        opt = torch.optim.AdamW(self.parameters(), lr=self.lr)\n",
        "        lr_schedulers = {\n",
        "            \"scheduler\": get_cosine_schedule_with_warmup(\n",
        "                optimizer=opt, num_warmup_steps=1000, num_training_steps=7700\n",
        "            ),\n",
        "            \"name\": \"learning_rate\",\n",
        "            \"interval\": \"step\",\n",
        "            \"frequency\": 1,\n",
        "        }\n",
        "        return [opt], [lr_schedulers]\n",
        "\n",
        "\n",
        "class BertNerModel(BaseModel):\n",
        "    def __init__(\n",
        "        self,\n",
        "#        n_classes=len(LABEL_MAPPING),\n",
        "        n_classes,\n",
        "        pad_idx,\n",
        "        lr=1e-4,\n",
        "        # pad_idx=PAD_IDX,\n",
        "        dropout=0.2,\n",
        "        bert_path=None,\n",
        "        keep_layers=(\"embeddings\", \"encoder\", \"pooler\"),\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "        self.lr = lr\n",
        "        self.pad_idx = pad_idx\n",
        "\n",
        "        self.n_classes = n_classes\n",
        "\n",
        "        CONFIG.hidden_dropout_prob = dropout\n",
        "\n",
        "        self.bert = BertModel(config=CONFIG)\n",
        "\n",
        "        for name, param in self.bert.named_parameters():\n",
        "            if not any(name.startswith(a) for a in keep_layers):\n",
        "                param.requires_grad = False\n",
        "\n",
        "        if bert_path:\n",
        "            state_dict = torch.load(bert_path)\n",
        "            self.bert.load_state_dict(state_dict)\n",
        "\n",
        "        self.do = nn.Dropout(p=dropout)\n",
        "\n",
        "        self.out_linear = nn.Linear(CONFIG.hidden_size, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        mask = (x != self.pad_idx).int()\n",
        "        x = self.bert(\n",
        "            x, attention_mask=mask, encoder_attention_mask=mask\n",
        "        ).last_hidden_state\n",
        "        # [batch, Seq_len, CONFIG.hidden_size]\n",
        "\n",
        "        x = self.do(x)\n",
        "\n",
        "        out = self.out_linear(x)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class PositionalEncoding(torch.nn.Module):\n",
        "    #  https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
        "\n",
        "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
        "        super().__init__()\n",
        "        self.dropout = torch.nn.Dropout(p=dropout)\n",
        "\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(\n",
        "            torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model)\n",
        "        )\n",
        "        pe = torch.zeros(1, max_len, d_model)\n",
        "        pe[0:, :, 0::2] = torch.sin(position * div_term)\n",
        "        pe[0:, :, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer(\"pe\", pe)\n",
        "\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
        "        \"\"\"\n",
        "\n",
        "        x = x + self.pe[:, : x.size(1)] / math.sqrt(self.d_model)\n",
        "\n",
        "        return self.dropout(x)\n",
        "\n",
        "\n",
        "class BaseNerModel(BaseModel):\n",
        "    def __init__(\n",
        "        self,\n",
        "        #n_classes=len(LABEL_MAPPING),\n",
        "        n_classes,\n",
        "        pad_idx,\n",
        "        n_vocab,\n",
        "        lr=1e-4,\n",
        "        #pad_idx=PAD_IDX,\n",
        "        dropout=0.2,\n",
        "        d_model=9466,\n",
        "        #n_vocab=N_VOCAB,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "        self.lr = lr\n",
        "        self.pad_idx = pad_idx\n",
        "        self.d_model = d_model\n",
        "        self.dropout = dropout\n",
        "        self.n_vocab = n_vocab\n",
        "\n",
        "        self.n_classes = n_classes\n",
        "\n",
        "        # Text\n",
        "        self.embeddings = torch.nn.Embedding(self.n_vocab, self.d_model)\n",
        "        self.pos_encoder = PositionalEncoding(\n",
        "            d_model=self.d_model, dropout=self.dropout\n",
        "        )\n",
        "        encoder_layer = torch.nn.TransformerEncoderLayer(\n",
        "            d_model=self.d_model, nhead=4, dropout=self.dropout, batch_first=True\n",
        "        )\n",
        "        self.encoder = torch.nn.TransformerEncoder(encoder_layer, num_layers=4)\n",
        "\n",
        "        self.do = nn.Dropout(p=dropout)\n",
        "\n",
        "        self.out_linear = nn.Linear(self.d_model, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        mask = (x == self.pad_idx).float()\n",
        "\n",
        "        mask = mask.masked_fill(mask == 1, float(\"-inf\")).masked_fill(\n",
        "            mask == 0, float(0.0)\n",
        "        )\n",
        "\n",
        "        x = self.embeddings(x)\n",
        "        x = self.pos_encoder(x)\n",
        "        x = self.encoder(x, src_key_padding_mask=mask)\n",
        "\n",
        "        x = self.do(x)\n",
        "\n",
        "        out = self.out_linear(x)\n",
        "\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "epOr2mnnllDe"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizerFast\n",
        "from pathlib import Path\n",
        "from pprint import pprint\n",
        "import pandas as pd\n",
        "import torch\n",
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ti2rEXqCXR8k"
      },
      "source": [
        "## ner_binary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VyKXAHPsj67P"
      },
      "outputs": [],
      "source": [
        "class TokenBinary:\n",
        "  def __init__(\n",
        "      self,\n",
        "      token: str,\n",
        "      index: int,\n",
        "      start_index: int,\n",
        "      end_index: int,\n",
        "      raw_label: Optional[str] = None,\n",
        "      bio_label: Optional[str] = None,\n",
        "  ):\n",
        "      self.token = token\n",
        "      self.index = index\n",
        "      self.start_index = start_index\n",
        "      self.end_index = end_index\n",
        "      self.raw_label = raw_label\n",
        "      self.bio_label = bio_label\n",
        "\n",
        "  def __repr__(self):\n",
        "      return (\n",
        "          f\"T: {self.token} / \"\n",
        "          f\"I: {self.index} / \"\n",
        "          f\"S: {self.start_index} / \"\n",
        "          f\"E: {self.end_index} / \"\n",
        "          f\"RL: {self.raw_label} / \"\n",
        "          f\"CL: {self.clean_label} / \"\n",
        "          f\"BIO: {self.bio_label}\"\n",
        "      )\n",
        "\n",
        "  @property\n",
        "  def bio_idx(self):\n",
        "      return LABEL_MAPPINGBinary[self.bio_label]\n",
        "\n",
        "  @property\n",
        "  def clean_label(self):\n",
        "      return re.sub(r\"\\s#\\d+$\", \"\", self.raw_label)\n",
        "\n",
        "  def as_dict(self):\n",
        "      return {\n",
        "          \"token\": self.token,\n",
        "          \"index\": self.index,\n",
        "          \"start_index\": self.start_index,\n",
        "          \"end_index\": self.end_index,\n",
        "          \"raw_label\": self.raw_label,\n",
        "          \"bio_label\": self.bio_label,\n",
        "      }\n",
        "\n",
        "  @classmethod\n",
        "  def from_dict(cls, as_dict: Dict):\n",
        "      return cls(**as_dict)\n",
        "\n",
        "\n",
        "def tokenizeBinary(text: str):\n",
        "  encoded = TOKENIZERBinary.encode_plus(text, return_offsets_mapping=True)\n",
        "  ids = encoded[\"input_ids\"]\n",
        "  offsets = encoded[\"offset_mapping\"]\n",
        "  tokens = TOKENIZERBinary.convert_ids_to_tokens(ids)\n",
        "\n",
        "  tokens = [\n",
        "      TokenBinary(token=token, index=index, start_index=offset[0], end_index=offset[1])\n",
        "      for token, index, offset in zip(tokens, ids, offsets)\n",
        "  ]\n",
        "\n",
        "  return tokens\n",
        "\n",
        "\n",
        "def most_frequentBinary(list_of_labels):\n",
        "  return max(set(list_of_labels), key=list_of_labels.count)\n",
        "\n",
        "\n",
        "def generate_labeled_tokensBinary(text: str, labels: List[Dict]):\n",
        "  tokens = tokenizeBinary(text=text)\n",
        "\n",
        "  char_label = [\"O\"] * len(text)\n",
        "\n",
        "  for i, span in enumerate(labels):\n",
        "\n",
        "      label = span[\"label\"]\n",
        "      start = span[\"start\"]\n",
        "      end = span[\"end\"]\n",
        "\n",
        "      char_label[start:end] = [f\"{label} #{i}\"] * (end - start)\n",
        "\n",
        "  for i, token in enumerate(tokens):\n",
        "      if token.start_index != token.end_index:\n",
        "          token.raw_label = most_frequentBinary(\n",
        "              char_label[token.start_index : token.end_index]\n",
        "          )\n",
        "      else:\n",
        "          token.raw_label = \"O\"\n",
        "\n",
        "  # BIO labels\n",
        "  for i, token in enumerate(tokens):\n",
        "      if token.raw_label != \"O\":\n",
        "          if i == 0:\n",
        "              token.bio_label = \"B-\" + token.clean_label\n",
        "\n",
        "          else:\n",
        "              if tokens[i - 1].raw_label == tokens[i].raw_label:\n",
        "                  token.bio_label = \"I-\" + token.clean_label\n",
        "              else:\n",
        "                  token.bio_label = \"B-\" + token.clean_label\n",
        "      else:\n",
        "          token.bio_label = token.clean_label\n",
        "\n",
        "  return tokens\n",
        "\n",
        "\n",
        "def group_tokens_by_entityBinary(tokens: List[TokenBinary]):\n",
        "  \"\"\"\n",
        "  List to List[List[Token]]\n",
        "\n",
        "  :param tokens:\n",
        "  :return:\n",
        "  \"\"\"\n",
        "  block_tokens = []\n",
        "  for i, token in enumerate(tokens):\n",
        "      if token.bio_label == \"O\" or token.start_index == token.end_index == 0:\n",
        "          continue\n",
        "      elif i == 0:\n",
        "          block_tokens.append([token])\n",
        "      elif (\n",
        "          tokens[i].bio_label.split(\"-\")[0] == \"B\"\n",
        "          or tokens[i - 1].bio_label.split(\"-\")[-1]\n",
        "          != tokens[i].bio_label.split(\"-\")[-1]\n",
        "      ):\n",
        "          block_tokens.append([token])\n",
        "      else:\n",
        "          block_tokens[-1].append(token)\n",
        "\n",
        "  return block_tokens\n",
        "\n",
        "\n",
        "def decode_labeled_tokensBinary(tokens: List[TokenBinary]):\n",
        "  \"\"\"\n",
        "  decode labeled tokens into word indexes\n",
        "\n",
        "  :param tokens:\n",
        "  :return:\n",
        "  \"\"\"\n",
        "  block_tokens = group_tokens_by_entityBinary(tokens=tokens)\n",
        "\n",
        "  labels = []\n",
        "  for block in block_tokens:\n",
        "      start = min(token.start_index for token in block)\n",
        "      end = max(token.end_index for token in block)\n",
        "      label = block[0].bio_label.split(\"-\")[-1]\n",
        "      labels.append({\"label\": label, \"start\": start, \"end\": end})\n",
        "\n",
        "  return labels\n",
        "\n",
        "\n",
        "TOKENIZER_PATHBinary = \"/content/ner_playground/bert_model\"\n",
        "TOKENIZERBinary = BertTokenizerFast.from_pretrained(str(TOKENIZER_PATHBinary))\n",
        "CLASSESBinary = [\n",
        "    \"DESC\",\n",
        "    \"CONST_ELMS_LEFT\",\n",
        "    \"CONST_ELMS_RIGHT\"\n",
        "]\n",
        "\n",
        "LABEL_MAPPINGBinary = {\n",
        "    \"O\": 0,\n",
        "}\n",
        "i = 1\n",
        "for c in CLASSESBinary:\n",
        "    LABEL_MAPPINGBinary[f\"B-{c}\"] = i\n",
        "    LABEL_MAPPINGBinary[f\"I-{c}\"] = i + 1\n",
        "    i += 2\n",
        "\n",
        "\n",
        "INV_LABEL_MAPPINGBinary = {v: k for k, v in LABEL_MAPPINGBinary.items()}\n",
        "\n",
        "CLSBinary = \"[CLS]\"\n",
        "PADBinary = \"[PAD]\"\n",
        "SEPBinary = \"[SEP]\"\n",
        "\n",
        "PAD_IDXBinary = TOKENIZERBinary.pad_token_id\n",
        "CLS_IDXBinary = TOKENIZERBinary.cls_token_id\n",
        "SEP_IDXBinary = TOKENIZERBinary.sep_token_id\n",
        "\n",
        "N_VOCABBinary = len(TOKENIZERBinary.get_vocab())\n",
        "\n",
        "MAX_LENBinary = 256\n",
        "\n",
        "\n",
        "modelnBinary = \"ner-bert-binaryData-justparams.ckpt\"\n",
        "BASE_PATHBinary = \"/content/ner_playground\"\n",
        "BERT_PATHBinary = BASE_PATHBinary+ \"/bert_model/pytorch_model.bin\"\n",
        "\n",
        "\n",
        "modelnerbinary = BertNerModel(\n",
        "      lr=5e-5,\n",
        "      n_classes= len(LABEL_MAPPINGBinary),\n",
        "      pad_idx=PAD_IDXBinary,\n",
        "  )\n",
        "\n",
        "modelnerbinary.eval()\n",
        "\n",
        "model_pathBinary = BASE_PATHBinary + \"/models/\"+modelnBinary\n",
        "modelnerbinary.load_state_dict(torch.load(model_pathBinary)[\"state_dict\"])\n",
        "\n",
        "\n",
        "def ner_binary(sentence, model):\n",
        "\n",
        "  gold_spansBinary = []\n",
        "  predicted_spansBinary = []\n",
        "\n",
        "  cptBinary = 0\n",
        "\n",
        "\n",
        "  prediction_tokensBinary = generate_labeled_tokensBinary(sentence, labels=[])[:MAX_LENBinary]\n",
        "\n",
        "  x = torch.tensor([token.index for token in prediction_tokensBinary], dtype=torch.long)\n",
        "  x = x.unsqueeze(0)\n",
        "\n",
        "  with torch.no_grad():\n",
        "\n",
        "    prediction_score = model(x).squeeze(0)\n",
        "\n",
        "    _, predicted = torch.max(prediction_score, 1)\n",
        "\n",
        "    for token, label_index in zip(prediction_tokensBinary, predicted.tolist()):\n",
        "      token.bio_label = INV_LABEL_MAPPINGBinary[label_index]\n",
        "\n",
        "    predicted_spansBinary.append(decode_labeled_tokensBinary(prediction_tokensBinary))\n",
        "\n",
        "\n",
        "  return predicted_spansBinary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YhRMGLrfqnPW"
      },
      "outputs": [],
      "source": [
        "# ner_binary(\"V should be greater than W\",modelnerbinary)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qrx4IMmurf95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRTZvDY_XWe_"
      },
      "source": [
        "## ner_unary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YY1d5ba-j645"
      },
      "outputs": [],
      "source": [
        "\n",
        "class TokenUnary:\n",
        "  def __init__(\n",
        "      self,\n",
        "      token: str,\n",
        "      index: int,\n",
        "      start_index: int,\n",
        "      end_index: int,\n",
        "      raw_label: Optional[str] = None,\n",
        "      bio_label: Optional[str] = None,\n",
        "  ):\n",
        "      self.token = token\n",
        "      self.index = index\n",
        "      self.start_index = start_index\n",
        "      self.end_index = end_index\n",
        "      self.raw_label = raw_label\n",
        "      self.bio_label = bio_label\n",
        "\n",
        "  def __repr__(self):\n",
        "      return (\n",
        "          f\"T: {self.token} / \"\n",
        "          f\"I: {self.index} / \"\n",
        "          f\"S: {self.start_index} / \"\n",
        "          f\"E: {self.end_index} / \"\n",
        "          f\"RL: {self.raw_label} / \"\n",
        "          f\"CL: {self.clean_label} / \"\n",
        "          f\"BIO: {self.bio_label}\"\n",
        "      )\n",
        "\n",
        "  @property\n",
        "  def bio_idx(self):\n",
        "      return LABEL_MAPPINGUnary[self.bio_label]\n",
        "\n",
        "  @property\n",
        "  def clean_label(self):\n",
        "      return re.sub(r\"\\s#\\d+$\", \"\", self.raw_label)\n",
        "\n",
        "  def as_dict(self):\n",
        "      return {\n",
        "          \"token\": self.token,\n",
        "          \"index\": self.index,\n",
        "          \"start_index\": self.start_index,\n",
        "          \"end_index\": self.end_index,\n",
        "          \"raw_label\": self.raw_label,\n",
        "          \"bio_label\": self.bio_label,\n",
        "      }\n",
        "\n",
        "  @classmethod\n",
        "  def from_dict(cls, as_dict: Dict):\n",
        "      return cls(**as_dict)\n",
        "\n",
        "\n",
        "def tokenizeUnary(text: str):\n",
        "  encoded = TOKENIZERUnary.encode_plus(text, return_offsets_mapping=True)\n",
        "  ids = encoded[\"input_ids\"]\n",
        "  offsets = encoded[\"offset_mapping\"]\n",
        "  tokens = TOKENIZERUnary.convert_ids_to_tokens(ids)\n",
        "\n",
        "  tokens = [\n",
        "      TokenUnary(token=token, index=index, start_index=offset[0], end_index=offset[1])\n",
        "      for token, index, offset in zip(tokens, ids, offsets)\n",
        "  ]\n",
        "\n",
        "  return tokens\n",
        "\n",
        "\n",
        "def most_frequentUnary(list_of_labels):\n",
        "  return max(set(list_of_labels), key=list_of_labels.count)\n",
        "\n",
        "\n",
        "def generate_labeled_tokensUnary(text: str, labels: List[Dict]):\n",
        "  tokens = tokenizeUnary(text=text)\n",
        "\n",
        "  char_label = [\"O\"] * len(text)\n",
        "\n",
        "  for i, span in enumerate(labels):\n",
        "\n",
        "      label = span[\"label\"]\n",
        "      start = span[\"start\"]\n",
        "      end = span[\"end\"]\n",
        "\n",
        "      char_label[start:end] = [f\"{label} #{i}\"] * (end - start)\n",
        "\n",
        "  for i, token in enumerate(tokens):\n",
        "      if token.start_index != token.end_index:\n",
        "          token.raw_label = most_frequentUnary(\n",
        "              char_label[token.start_index : token.end_index]\n",
        "          )\n",
        "      else:\n",
        "          token.raw_label = \"O\"\n",
        "\n",
        "  # BIO labels\n",
        "  for i, token in enumerate(tokens):\n",
        "      if token.raw_label != \"O\":\n",
        "          if i == 0:\n",
        "              token.bio_label = \"B-\" + token.clean_label\n",
        "\n",
        "          else:\n",
        "              if tokens[i - 1].raw_label == tokens[i].raw_label:\n",
        "                  token.bio_label = \"I-\" + token.clean_label\n",
        "              else:\n",
        "                  token.bio_label = \"B-\" + token.clean_label\n",
        "      else:\n",
        "          token.bio_label = token.clean_label\n",
        "\n",
        "  return tokens\n",
        "\n",
        "\n",
        "def group_tokens_by_entityUnary(tokens: List[TokenUnary]):\n",
        "  \"\"\"\n",
        "  List to List[List[Token]]\n",
        "\n",
        "  :param tokens:\n",
        "  :return:\n",
        "  \"\"\"\n",
        "  block_tokens = []\n",
        "  for i, token in enumerate(tokens):\n",
        "      if token.bio_label == \"O\" or token.start_index == token.end_index == 0:\n",
        "          continue\n",
        "      elif i == 0:\n",
        "          block_tokens.append([token])\n",
        "      elif (\n",
        "          tokens[i].bio_label.split(\"-\")[0] == \"B\"\n",
        "          or tokens[i - 1].bio_label.split(\"-\")[-1]\n",
        "          != tokens[i].bio_label.split(\"-\")[-1]\n",
        "      ):\n",
        "          block_tokens.append([token])\n",
        "      else:\n",
        "          block_tokens[-1].append(token)\n",
        "\n",
        "  return block_tokens\n",
        "\n",
        "\n",
        "def decode_labeled_tokensUnary(tokens: List[TokenUnary]):\n",
        "  \"\"\"\n",
        "  decode labeled tokens into word indexes\n",
        "\n",
        "  :param tokens:\n",
        "  :return:\n",
        "  \"\"\"\n",
        "  block_tokens = group_tokens_by_entityUnary(tokens=tokens)\n",
        "\n",
        "  labels = []\n",
        "  for block in block_tokens:\n",
        "      start = min(token.start_index for token in block)\n",
        "      end = max(token.end_index for token in block)\n",
        "      label = block[0].bio_label.split(\"-\")[-1]\n",
        "      labels.append({\"label\": label, \"start\": start, \"end\": end})\n",
        "\n",
        "  return labels\n",
        "\n",
        "\n",
        "TOKENIZER_PATHUnary = \"/content/ner_playground/bert_model\"\n",
        "TOKENIZERUnary = BertTokenizerFast.from_pretrained(str(TOKENIZER_PATHUnary))\n",
        "CLASSESUnary = [\n",
        "    \"DESC\",\n",
        "    \"VARIABLE\",\n",
        "    \"PARAMETER\"\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "LABEL_MAPPINGUnary = {\n",
        "    \"O\": 0,\n",
        "}\n",
        "i = 1\n",
        "for c in CLASSESUnary:\n",
        "    LABEL_MAPPINGUnary[f\"B-{c}\"] = i\n",
        "    LABEL_MAPPINGUnary[f\"I-{c}\"] = i + 1\n",
        "    i += 2\n",
        "\n",
        "\n",
        "INV_LABEL_MAPPINGUnary = {v: k for k, v in LABEL_MAPPINGUnary.items()}\n",
        "\n",
        "CLSUnary = \"[CLS]\"\n",
        "PADUnary = \"[PAD]\"\n",
        "SEPUnary = \"[SEP]\"\n",
        "\n",
        "PAD_IDXUnary = TOKENIZERUnary.pad_token_id\n",
        "CLS_IDXUnary = TOKENIZERUnary.cls_token_id\n",
        "SEP_IDXUnary = TOKENIZERUnary.sep_token_id\n",
        "\n",
        "N_VOCABUnary = len(TOKENIZERUnary.get_vocab())\n",
        "\n",
        "MAX_LENUnary = 256\n",
        "\n",
        "\n",
        "modelnUnary = \"ner-bert-unaryConstraints-justparams.ckpt\"\n",
        "BASE_PATHUnary = \"/content/ner_playground\"\n",
        "BERT_PATHUnary = BASE_PATHUnary+ \"/bert_model/pytorch_model.bin\"\n",
        "\n",
        "\n",
        "modelnerunary = BertNerModel(\n",
        "      lr=5e-5,\n",
        "      n_classes= len(LABEL_MAPPINGUnary),\n",
        "      pad_idx=PAD_IDXUnary,\n",
        "  )\n",
        "\n",
        "modelnerunary.eval()\n",
        "\n",
        "model_pathUnary = BASE_PATHUnary + \"/models/\"+modelnUnary\n",
        "modelnerunary.load_state_dict(torch.load(model_pathUnary)[\"state_dict\"])\n",
        "\n",
        "\n",
        "\n",
        "def ner_unary(sentence,model):\n",
        "  gold_spansUnary = []\n",
        "  predicted_spansUnary = []\n",
        "\n",
        "  cpt = 0\n",
        "\n",
        "\n",
        "  prediction_tokensUnary = generate_labeled_tokensUnary(sentence, labels=[])[:MAX_LENUnary]\n",
        "\n",
        "  x = torch.tensor([token.index for token in prediction_tokensUnary], dtype=torch.long)\n",
        "  x = x.unsqueeze(0)\n",
        "\n",
        "  with torch.no_grad():\n",
        "\n",
        "    prediction_score = model(x).squeeze(0)\n",
        "\n",
        "    _, predicted = torch.max(prediction_score, 1)\n",
        "\n",
        "    for token, label_index in zip(prediction_tokensUnary, predicted.tolist()):\n",
        "      token.bio_label = INV_LABEL_MAPPINGUnary[label_index]\n",
        "\n",
        "    predicted_spansUnary.append(decode_labeled_tokensUnary(prediction_tokensUnary))\n",
        "\n",
        "\n",
        "\n",
        "  return predicted_spansUnary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3W-ymgMOrRZN"
      },
      "outputs": [],
      "source": [
        "# ner_unary(\"X should be greater than 9\",modelnerunary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pp4pJYa1XZ9e"
      },
      "source": [
        "## ner_DistanceBinary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hdy5doNxj63C"
      },
      "outputs": [],
      "source": [
        "class TokenDistanceBinary:\n",
        "  def __init__(\n",
        "      self,\n",
        "      token: str,\n",
        "      index: int,\n",
        "      start_index: int,\n",
        "      end_index: int,\n",
        "      raw_label: Optional[str] = None,\n",
        "      bio_label: Optional[str] = None,\n",
        "  ):\n",
        "      self.token = token\n",
        "      self.index = index\n",
        "      self.start_index = start_index\n",
        "      self.end_index = end_index\n",
        "      self.raw_label = raw_label\n",
        "      self.bio_label = bio_label\n",
        "\n",
        "  def __repr__(self):\n",
        "      return (\n",
        "          f\"T: {self.token} / \"\n",
        "          f\"I: {self.index} / \"\n",
        "          f\"S: {self.start_index} / \"\n",
        "          f\"E: {self.end_index} / \"\n",
        "          f\"RL: {self.raw_label} / \"\n",
        "          f\"CL: {self.clean_label} / \"\n",
        "          f\"BIO: {self.bio_label}\"\n",
        "      )\n",
        "\n",
        "  @property\n",
        "  def bio_idx(self):\n",
        "      return LABEL_MAPPINGDistanceBinary[self.bio_label]\n",
        "\n",
        "  @property\n",
        "  def clean_label(self):\n",
        "      return re.sub(r\"\\s#\\d+$\", \"\", self.raw_label)\n",
        "\n",
        "  def as_dict(self):\n",
        "      return {\n",
        "          \"token\": self.token,\n",
        "          \"index\": self.index,\n",
        "          \"start_index\": self.start_index,\n",
        "          \"end_index\": self.end_index,\n",
        "          \"raw_label\": self.raw_label,\n",
        "          \"bio_label\": self.bio_label,\n",
        "      }\n",
        "\n",
        "  @classmethod\n",
        "  def from_dict(cls, as_dict: Dict):\n",
        "      return cls(**as_dict)\n",
        "\n",
        "\n",
        "def tokenizeDistanceBinary(text: str):\n",
        "  encoded = TOKENIZERDistanceBinary.encode_plus(text, return_offsets_mapping=True)\n",
        "  ids = encoded[\"input_ids\"]\n",
        "  offsets = encoded[\"offset_mapping\"]\n",
        "  tokens = TOKENIZERDistanceBinary.convert_ids_to_tokens(ids)\n",
        "\n",
        "  tokens = [\n",
        "      TokenDistanceBinary(token=token, index=index, start_index=offset[0], end_index=offset[1])\n",
        "      for token, index, offset in zip(tokens, ids, offsets)\n",
        "  ]\n",
        "\n",
        "  return tokens\n",
        "\n",
        "\n",
        "def most_frequentDistanceBinary(list_of_labels):\n",
        "  return max(set(list_of_labels), key=list_of_labels.count)\n",
        "\n",
        "\n",
        "def generate_labeled_tokensDistanceBinary(text: str, labels: List[Dict]):\n",
        "  tokens = tokenizeDistanceBinary(text=text)\n",
        "\n",
        "  char_label = [\"O\"] * len(text)\n",
        "\n",
        "  for i, span in enumerate(labels):\n",
        "\n",
        "      label = span[\"label\"]\n",
        "      start = span[\"start\"]\n",
        "      end = span[\"end\"]\n",
        "\n",
        "      char_label[start:end] = [f\"{label} #{i}\"] * (end - start)\n",
        "\n",
        "  for i, token in enumerate(tokens):\n",
        "      if token.start_index != token.end_index:\n",
        "          token.raw_label = most_frequentDistanceBinary(\n",
        "              char_label[token.start_index : token.end_index]\n",
        "          )\n",
        "      else:\n",
        "          token.raw_label = \"O\"\n",
        "\n",
        "  # BIO labels\n",
        "  for i, token in enumerate(tokens):\n",
        "      if token.raw_label != \"O\":\n",
        "          if i == 0:\n",
        "              token.bio_label = \"B-\" + token.clean_label\n",
        "\n",
        "          else:\n",
        "              if tokens[i - 1].raw_label == tokens[i].raw_label:\n",
        "                  token.bio_label = \"I-\" + token.clean_label\n",
        "              else:\n",
        "                  token.bio_label = \"B-\" + token.clean_label\n",
        "      else:\n",
        "          token.bio_label = token.clean_label\n",
        "\n",
        "  return tokens\n",
        "\n",
        "\n",
        "def group_tokens_by_entityDistanceBinary(tokens: List[TokenDistanceBinary]):\n",
        "  \"\"\"\n",
        "  List to List[List[Token]]\n",
        "\n",
        "  :param tokens:\n",
        "  :return:\n",
        "  \"\"\"\n",
        "  block_tokens = []\n",
        "  for i, token in enumerate(tokens):\n",
        "      if token.bio_label == \"O\" or token.start_index == token.end_index == 0:\n",
        "          continue\n",
        "      elif i == 0:\n",
        "          block_tokens.append([token])\n",
        "      elif (\n",
        "          tokens[i].bio_label.split(\"-\")[0] == \"B\"\n",
        "          or tokens[i - 1].bio_label.split(\"-\")[-1]\n",
        "          != tokens[i].bio_label.split(\"-\")[-1]\n",
        "      ):\n",
        "          block_tokens.append([token])\n",
        "      else:\n",
        "          block_tokens[-1].append(token)\n",
        "\n",
        "  return block_tokens\n",
        "\n",
        "\n",
        "def decode_labeled_tokensDistanceBinary(tokens: List[TokenDistanceBinary]):\n",
        "  \"\"\"\n",
        "  decode labeled tokens into word indexes\n",
        "\n",
        "  :param tokens:\n",
        "  :return:\n",
        "  \"\"\"\n",
        "  block_tokens = group_tokens_by_entityDistanceBinary(tokens=tokens)\n",
        "\n",
        "  labels = []\n",
        "  for block in block_tokens:\n",
        "      start = min(token.start_index for token in block)\n",
        "      end = max(token.end_index for token in block)\n",
        "      label = block[0].bio_label.split(\"-\")[-1]\n",
        "      labels.append({\"label\": label, \"start\": start, \"end\": end})\n",
        "\n",
        "  return labels\n",
        "\n",
        "TOKENIZER_PATHDistanceBinary = \"/content/ner_playground/bert_model\"\n",
        "TOKENIZERDistanceBinary = BertTokenizerFast.from_pretrained(str(TOKENIZER_PATHDistanceBinary))\n",
        "CLASSESDistanceBinary = [\n",
        "    \"DESC\",\n",
        "    \"Dist_Right_Elm_1\",\n",
        "    \"Dist_Right_Elm_2\",\n",
        "    \"Dist_Left_Elm_1\",\n",
        "    \"Dist_Left_Elm_2\"\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "LABEL_MAPPINGDistanceBinary = {\n",
        "    \"O\": 0,\n",
        "}\n",
        "i = 1\n",
        "for c in CLASSESDistanceBinary:\n",
        "    LABEL_MAPPINGDistanceBinary[f\"B-{c}\"] = i\n",
        "    LABEL_MAPPINGDistanceBinary[f\"I-{c}\"] = i + 1\n",
        "    i += 2\n",
        "\n",
        "\n",
        "INV_LABEL_MAPPINGDistanceBinary = {v: k for k, v in LABEL_MAPPINGDistanceBinary.items()}\n",
        "\n",
        "CLSDistanceBinary = \"[CLS]\"\n",
        "PADDistanceBinary = \"[PAD]\"\n",
        "SEPDistanceBinary = \"[SEP]\"\n",
        "\n",
        "PAD_IDXDistanceBinary = TOKENIZERDistanceBinary.pad_token_id\n",
        "CLS_IDXDistanceBinary = TOKENIZERDistanceBinary.cls_token_id\n",
        "SEP_IDXDistanceBinary = TOKENIZERDistanceBinary.sep_token_id\n",
        "\n",
        "N_VOCABDistanceBinary = len(TOKENIZERDistanceBinary.get_vocab())\n",
        "\n",
        "\n",
        "MAX_LENDistanceBinary = 256\n",
        "\n",
        "\n",
        "modelnDistanceBinary = \"ner-bert-distances-justparams.ckpt\"\n",
        "BASE_PATHDistanceBinary = \"/content/ner_playground\"\n",
        "BERT_PATHDistanceBinary = BASE_PATHDistanceBinary+ \"/bert_model/pytorch_model.bin\"\n",
        "\n",
        "\n",
        "modelDistanceBinary = BertNerModel(\n",
        "      lr=5e-5,\n",
        "      n_classes= len(LABEL_MAPPINGDistanceBinary),\n",
        "      pad_idx=PAD_IDXDistanceBinary,\n",
        "  )\n",
        "\n",
        "modelDistanceBinary.eval()\n",
        "\n",
        "model_pathDistanceBinary = BASE_PATHDistanceBinary + \"/models/\"+modelnDistanceBinary\n",
        "modelDistanceBinary.load_state_dict(torch.load(model_pathDistanceBinary)[\"state_dict\"])\n",
        "\n",
        "\n",
        "def ner_DistanceBinary(sentence,model):\n",
        "\n",
        "  gold_spans = []\n",
        "  predicted_spans = []\n",
        "\n",
        "  cpt = 0\n",
        "\n",
        "\n",
        "  prediction_tokens = generate_labeled_tokensDistanceBinary(sentence, labels=[])[:MAX_LENDistanceBinary]\n",
        "\n",
        "  x = torch.tensor([token.index for token in prediction_tokens], dtype=torch.long)\n",
        "  x = x.unsqueeze(0)\n",
        "\n",
        "  with torch.no_grad():\n",
        "\n",
        "    prediction_score = model(x).squeeze(0)\n",
        "\n",
        "    _, predicted = torch.max(prediction_score, 1)\n",
        "\n",
        "    for token, label_index in zip(prediction_tokens, predicted.tolist()):\n",
        "      token.bio_label = INV_LABEL_MAPPINGDistanceBinary[label_index]\n",
        "\n",
        "    predicted_spans.append(decode_labeled_tokensDistanceBinary(prediction_tokens))\n",
        "\n",
        "\n",
        "  return predicted_spans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A5k_ctChrzgB"
      },
      "outputs": [],
      "source": [
        "# ner_DistanceBinary(\"d1 and D2 should be greater than D3 and D4\",modelDistanceBinary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zgigds8EXe8g"
      },
      "source": [
        "## ner_DistanceUnary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y7NI--WSj60E"
      },
      "outputs": [],
      "source": [
        "class TokenDistanceUnary:\n",
        "  def __init__(\n",
        "      self,\n",
        "      token: str,\n",
        "      index: int,\n",
        "      start_index: int,\n",
        "      end_index: int,\n",
        "      raw_label: Optional[str] = None,\n",
        "      bio_label: Optional[str] = None,\n",
        "  ):\n",
        "      self.token = token\n",
        "      self.index = index\n",
        "      self.start_index = start_index\n",
        "      self.end_index = end_index\n",
        "      self.raw_label = raw_label\n",
        "      self.bio_label = bio_label\n",
        "\n",
        "  def __repr__(self):\n",
        "      return (\n",
        "          f\"T: {self.token} / \"\n",
        "          f\"I: {self.index} / \"\n",
        "          f\"S: {self.start_index} / \"\n",
        "          f\"E: {self.end_index} / \"\n",
        "          f\"RL: {self.raw_label} / \"\n",
        "          f\"CL: {self.clean_label} / \"\n",
        "          f\"BIO: {self.bio_label}\"\n",
        "      )\n",
        "\n",
        "  @property\n",
        "  def bio_idx(self):\n",
        "      return LABEL_MAPPINGDistanceUnary[self.bio_label]\n",
        "\n",
        "  @property\n",
        "  def clean_label(self):\n",
        "      return re.sub(r\"\\s#\\d+$\", \"\", self.raw_label)\n",
        "\n",
        "  def as_dict(self):\n",
        "      return {\n",
        "          \"token\": self.token,\n",
        "          \"index\": self.index,\n",
        "          \"start_index\": self.start_index,\n",
        "          \"end_index\": self.end_index,\n",
        "          \"raw_label\": self.raw_label,\n",
        "          \"bio_label\": self.bio_label,\n",
        "      }\n",
        "\n",
        "  @classmethod\n",
        "  def from_dict(cls, as_dict: Dict):\n",
        "      return cls(**as_dict)\n",
        "\n",
        "\n",
        "def tokenizeDistanceUnary(text: str):\n",
        "  encoded = TOKENIZERDistanceUnary.encode_plus(text, return_offsets_mapping=True)\n",
        "  ids = encoded[\"input_ids\"]\n",
        "  offsets = encoded[\"offset_mapping\"]\n",
        "  tokens = TOKENIZERDistanceUnary.convert_ids_to_tokens(ids)\n",
        "\n",
        "  tokens = [\n",
        "      TokenDistanceUnary(token=token, index=index, start_index=offset[0], end_index=offset[1])\n",
        "      for token, index, offset in zip(tokens, ids, offsets)\n",
        "  ]\n",
        "\n",
        "  return tokens\n",
        "\n",
        "\n",
        "def most_frequentDistanceUnary(list_of_labels):\n",
        "  return max(set(list_of_labels), key=list_of_labels.count)\n",
        "\n",
        "\n",
        "def generate_labeled_tokensDistanceUnary(text: str, labels: List[Dict]):\n",
        "  tokens = tokenizeDistanceUnary(text=text)\n",
        "\n",
        "  char_label = [\"O\"] * len(text)\n",
        "\n",
        "  for i, span in enumerate(labels):\n",
        "\n",
        "      label = span[\"label\"]\n",
        "      start = span[\"start\"]\n",
        "      end = span[\"end\"]\n",
        "\n",
        "      char_label[start:end] = [f\"{label} #{i}\"] * (end - start)\n",
        "\n",
        "  for i, token in enumerate(tokens):\n",
        "      if token.start_index != token.end_index:\n",
        "          token.raw_label = most_frequentDistanceUnary(\n",
        "              char_label[token.start_index : token.end_index]\n",
        "          )\n",
        "      else:\n",
        "          token.raw_label = \"O\"\n",
        "\n",
        "  # BIO labels\n",
        "  for i, token in enumerate(tokens):\n",
        "      if token.raw_label != \"O\":\n",
        "          if i == 0:\n",
        "              token.bio_label = \"B-\" + token.clean_label\n",
        "\n",
        "          else:\n",
        "              if tokens[i - 1].raw_label == tokens[i].raw_label:\n",
        "                  token.bio_label = \"I-\" + token.clean_label\n",
        "              else:\n",
        "                  token.bio_label = \"B-\" + token.clean_label\n",
        "      else:\n",
        "          token.bio_label = token.clean_label\n",
        "\n",
        "  return tokens\n",
        "\n",
        "\n",
        "def group_tokens_by_entityDistanceUnary(tokens: List[TokenDistanceUnary]):\n",
        "  \"\"\"\n",
        "  List to List[List[Token]]\n",
        "\n",
        "  :param tokens:\n",
        "  :return:\n",
        "  \"\"\"\n",
        "  block_tokens = []\n",
        "  for i, token in enumerate(tokens):\n",
        "      if token.bio_label == \"O\" or token.start_index == token.end_index == 0:\n",
        "          continue\n",
        "      elif i == 0:\n",
        "          block_tokens.append([token])\n",
        "      elif (\n",
        "          tokens[i].bio_label.split(\"-\")[0] == \"B\"\n",
        "          or tokens[i - 1].bio_label.split(\"-\")[-1]\n",
        "          != tokens[i].bio_label.split(\"-\")[-1]\n",
        "      ):\n",
        "          block_tokens.append([token])\n",
        "      else:\n",
        "          block_tokens[-1].append(token)\n",
        "\n",
        "  return block_tokens\n",
        "\n",
        "\n",
        "def decode_labeled_tokensDistanceUnary(tokens: List[TokenDistanceUnary]):\n",
        "  \"\"\"\n",
        "  decode labeled tokens into word indexes\n",
        "\n",
        "  :param tokens:\n",
        "  :return:\n",
        "  \"\"\"\n",
        "  block_tokens = group_tokens_by_entityDistanceUnary(tokens=tokens)\n",
        "\n",
        "  labels = []\n",
        "  for block in block_tokens:\n",
        "      start = min(token.start_index for token in block)\n",
        "      end = max(token.end_index for token in block)\n",
        "      label = block[0].bio_label.split(\"-\")[-1]\n",
        "      labels.append({\"label\": label, \"start\": start, \"end\": end})\n",
        "\n",
        "  return labels\n",
        "\n",
        "TOKENIZER_PATHDistanceUnary = \"/content/ner_playground/bert_model\"\n",
        "TOKENIZERDistanceUnary = BertTokenizerFast.from_pretrained(str(TOKENIZER_PATHDistanceUnary))\n",
        "CLASSESDistanceUnary = [\n",
        "    \"DESC\",\n",
        "    \"Dist_Left_Elm_1\",\n",
        "    \"Dist_Left_Elm_2\",\n",
        "    \"Parameter\"\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "LABEL_MAPPINGDistanceUnary = {\n",
        "    \"O\": 0,\n",
        "}\n",
        "i = 1\n",
        "for c in CLASSESDistanceUnary:\n",
        "    LABEL_MAPPINGDistanceUnary[f\"B-{c}\"] = i\n",
        "    LABEL_MAPPINGDistanceUnary[f\"I-{c}\"] = i + 1\n",
        "    i += 2\n",
        "\n",
        "\n",
        "INV_LABEL_MAPPINGDistanceUnary = {v: k for k, v in LABEL_MAPPINGDistanceUnary.items()}\n",
        "\n",
        "CLS = \"[CLS]\"\n",
        "PAD = \"[PAD]\"\n",
        "SEP = \"[SEP]\"\n",
        "\n",
        "PAD_IDX = TOKENIZERDistanceUnary.pad_token_id\n",
        "CLS_IDX = TOKENIZERDistanceUnary.cls_token_id\n",
        "SEP_IDX = TOKENIZERDistanceUnary.sep_token_id\n",
        "\n",
        "N_VOCABDistanceUnary = len(TOKENIZERDistanceUnary.get_vocab())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "MAX_LENDistanceUnary = 256\n",
        "\n",
        "\n",
        "modelnDistanceUnary = \"ner-bert-distancesUnary-justparams.ckpt\"\n",
        "BASE_PATHDistanceUnary = \"/content/ner_playground\"\n",
        "BERT_PATHDistanceUnary = BASE_PATHDistanceUnary+ \"/bert_model/pytorch_model.bin\"\n",
        "\n",
        "\n",
        "modelDistanceUnary = BertNerModel(\n",
        "      lr=5e-5,\n",
        "      n_classes= len(LABEL_MAPPINGDistanceUnary),\n",
        "      pad_idx=PAD_IDX,\n",
        "  )\n",
        "\n",
        "modelDistanceUnary.eval()\n",
        "\n",
        "model_pathDistanceUnary = BASE_PATHDistanceUnary + \"/models/\"+modelnDistanceUnary\n",
        "modelDistanceUnary.load_state_dict(torch.load(model_pathDistanceUnary)[\"state_dict\"])\n",
        "\n",
        "\n",
        "\n",
        "def ner_DistanceUnary(sentence,model):\n",
        "\n",
        "  gold_spans = []\n",
        "  predicted_spans = []\n",
        "\n",
        "  cpt = 0\n",
        "\n",
        "\n",
        "  prediction_tokens = generate_labeled_tokensDistanceUnary(sentence, labels=[])[:MAX_LENDistanceUnary]\n",
        "\n",
        "  x = torch.tensor([token.index for token in prediction_tokens], dtype=torch.long)\n",
        "  x = x.unsqueeze(0)\n",
        "\n",
        "  with torch.no_grad():\n",
        "\n",
        "    prediction_score = model(x).squeeze(0)\n",
        "\n",
        "    _, predicted = torch.max(prediction_score, 1)\n",
        "\n",
        "    for token, label_index in zip(prediction_tokens, predicted.tolist()):\n",
        "      token.bio_label = INV_LABEL_MAPPINGDistanceUnary[label_index]\n",
        "\n",
        "    predicted_spans.append(decode_labeled_tokensDistanceUnary(prediction_tokens))\n",
        "\n",
        "\n",
        "  return predicted_spans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HHKbzOOEj6xX"
      },
      "outputs": [],
      "source": [
        "# ner_DistanceUnary(\"X1 and X2 must be less than 1\",modelDistanceUnary)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9SDRAPN43qwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lnGAbwcsc6C"
      },
      "source": [
        "# Parsers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AgNockPNj6vX"
      },
      "outputs": [],
      "source": [
        "labels = {\n",
        "    \"0\" : \"DESC\",\n",
        "    \"1\":\"CONST_ELMS_LEFT\",\n",
        "    \"2\":\"CONST_ELMS_LEFT_TYPE\",\n",
        "    \"3\":\"CONST_ELMS_RIGHT\",\n",
        "    \"4\":\"CONST_ELMS_RIGHT_TYPE\",\n",
        "    \"5\":\"gt\",\n",
        "    \"6\":\"ge\",\n",
        "    \"7\":\"lt\",\n",
        "    \"8\":\"le\",\n",
        "    \"9\":\"eq\",\n",
        "    \"10\":\"ne\",\n",
        "    \"11\":\"alldiff\",\n",
        "    \"12\":\"allequ\"\n",
        "}\n",
        "\n",
        "# isCommutative for distances means | a - b | is the same as | b - a | because we have absolute value\n",
        "operators_mappings = {\n",
        "    \"gt\" : {\n",
        "        \"operator\": operator.gt,\n",
        "        \"arity\": 2,\n",
        "        \"isCommutative\":False,\n",
        "        \"parameters\" : None\n",
        "    },\n",
        "    \"ge\" : {\n",
        "        \"operator\": operator.ge,\n",
        "        \"arity\": 2,\n",
        "        \"isCommutative\":False,\n",
        "        \"parameters\" : None\n",
        "    },\n",
        "    \"lt\" : {\n",
        "        \"operator\": operator.lt,\n",
        "        \"arity\": 2,\n",
        "        \"isCommutative\":False,\n",
        "        \"parameters\" : None\n",
        "    },\n",
        "    \"le\" : {\n",
        "        \"operator\": operator.le,\n",
        "        \"arity\": 2,\n",
        "        \"isCommutative\":False,\n",
        "        \"parameters\" : None\n",
        "    },\n",
        "    \"ne\" : {\n",
        "        \"operator\": operator.ne,\n",
        "        \"arity\": 2,\n",
        "        \"isCommutative\":True,\n",
        "        \"parameters\" : None\n",
        "    },\n",
        "    \"eq\" : {\n",
        "        \"operator\": operator.eq,\n",
        "        \"arity\": 2,\n",
        "        \"isCommutative\":True,\n",
        "        \"parameters\" : None\n",
        "    },\n",
        "    \"gtConst\":\n",
        "     {\n",
        "        \"operator\": gtConst,\n",
        "        \"arity\": 1,\n",
        "        \"isCommutative\":False,\n",
        "        \"parameters\" : []\n",
        "    },\n",
        "    \"geConst\":\n",
        "     {\n",
        "        \"operator\": geConst,\n",
        "        \"arity\": 1,\n",
        "        \"isCommutative\":False,\n",
        "        \"parameters\" : []\n",
        "    },\n",
        "    \"ltConst\":\n",
        "     {\n",
        "        \"operator\": ltConst,\n",
        "        \"arity\": 1,\n",
        "        \"isCommutative\":False,\n",
        "        \"parameters\" : []\n",
        "    },\n",
        "    \"leConst\":\n",
        "     {\n",
        "        \"operator\": leConst,\n",
        "        \"arity\": 1,\n",
        "        \"isCommutative\":False,\n",
        "        \"parameters\" : []\n",
        "    },\n",
        "    \"neConst\":\n",
        "     {\n",
        "        \"operator\": neConst,\n",
        "        \"arity\": 1,\n",
        "        \"isCommutative\":True,\n",
        "        \"parameters\" : []\n",
        "    },\n",
        "    \"eqConst\":\n",
        "     {\n",
        "        \"operator\": eqConst,\n",
        "        \"arity\": 1,\n",
        "        \"isCommutative\":True,\n",
        "        \"parameters\" : []\n",
        "    },\n",
        "    \"DistgtValue\":{\n",
        "        \"operator\": DistgtValue,\n",
        "        \"arity\": 2,\n",
        "        \"isCommutative\":True,\n",
        "        \"parameters\" : []\n",
        "    },\n",
        "    \"DistgeValue\":{\n",
        "        \"operator\": DistgeValue,\n",
        "        \"arity\": 2,\n",
        "        \"isCommutative\":True,\n",
        "        \"parameters\" : []\n",
        "    },\n",
        "    \"DistltValue\":{\n",
        "        \"operator\": DistltValue,\n",
        "        \"arity\": 2,\n",
        "        \"isCommutative\":True,\n",
        "        \"parameters\" : []\n",
        "    },\n",
        "    \"DistleValue\":{\n",
        "        \"operator\": DistleValue,\n",
        "        \"arity\": 2,\n",
        "        \"isCommutative\":True,\n",
        "        \"parameters\" : []\n",
        "    },\n",
        "    \"DisteqValue\":{\n",
        "        \"operator\": DisteqValue,\n",
        "        \"arity\": 2,\n",
        "        \"isCommutative\":True,\n",
        "        \"parameters\" : []\n",
        "    },\n",
        "    \"DistneValue\":{\n",
        "        \"operator\": DistneValue,\n",
        "        \"arity\": 2,\n",
        "        \"isCommutative\":True,\n",
        "        \"parameters\" : []\n",
        "    },\n",
        "    \"DisteqDist\":{\n",
        "        \"operator\": eqDist,\n",
        "        \"arity\": 4,\n",
        "        \"isCommutative\":True,\n",
        "        \"parameters\" : None\n",
        "    },\n",
        "    \"DisteqDist3\":{\n",
        "        \"operator\": eqDist3,\n",
        "        \"arity\": 3,\n",
        "        \"isCommutative\":False,\n",
        "        \"parameters\" : None\n",
        "    },\n",
        "    \"DistneDist\":{\n",
        "        \"operator\": neDist,\n",
        "        \"arity\": 4,\n",
        "        \"isCommutative\":True,\n",
        "        \"parameters\" : None\n",
        "    },\n",
        "    \"DistneDist3\":{\n",
        "        \"operator\": neDist3,\n",
        "        \"arity\": 3,\n",
        "        \"isCommutative\":False,\n",
        "        \"parameters\" : None\n",
        "    },\n",
        "    \"DistgtDist\":{\n",
        "        \"operator\": gtDist,\n",
        "        \"arity\": 4,\n",
        "        \"isCommutative\":False,\n",
        "        \"parameters\" : None\n",
        "    },\n",
        "    \"DistgeDist3\":{\n",
        "        \"operator\": geDist3,\n",
        "        \"arity\": 3,\n",
        "        \"isCommutative\":False,\n",
        "        \"parameters\" : None\n",
        "    },\n",
        "    \"DistltDist\":{\n",
        "        \"operator\": ltDist,\n",
        "        \"arity\": 4,\n",
        "        \"isCommutative\":False,\n",
        "        \"parameters\" : None\n",
        "    },\n",
        "    \"DistleDist3\":{\n",
        "        \"operator\": leDist3,\n",
        "        \"arity\": 3,\n",
        "        \"isCommutative\":False,\n",
        "        \"parameters\" : None\n",
        "    }\n",
        "\n",
        "\n",
        "    # \"distanceUnary\":\n",
        "    # \"distanceBinary\":\n",
        "    # \"distanceTurnary\":\n",
        "}\n",
        "\n",
        "\n",
        "constTypes = list(operators_mappings.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFWAJzz7sfyu"
      },
      "source": [
        "## ParserUnary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M31X1taGj6s-"
      },
      "outputs": [],
      "source": [
        "def parseUnary(text,predicted_spans):\n",
        "\n",
        "  constraints = []\n",
        "\n",
        "  i = 0\n",
        "  # get indexes of constraint types (e.g. gt, lt ...)\n",
        "  # these indexes will be used to begin the search\n",
        "  # for the other attributes const_elms_left and const_elms_right\n",
        "\n",
        "  constraint = {} # begin constructing the constraint\n",
        "  while(i<len(predicted_spans[0])):\n",
        "    if(predicted_spans[0][i][\"label\"] == \"VARIABLE\"):\n",
        "      constraint[predicted_spans[0][i]['label']] = text[predicted_spans[0][i][\"start\"] : predicted_spans[0][i][\"end\"]]\n",
        "    elif(predicted_spans[0][i][\"label\"] == \"PARAMETER\"):\n",
        "      constraint[predicted_spans[0][i]['label']] = text[predicted_spans[0][i][\"start\"] : predicted_spans[0][i][\"end\"]]\n",
        "    i+=1\n",
        "\n",
        "  return constraint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iO8RF7Zp2JS0"
      },
      "outputs": [],
      "source": [
        "# t = \"W should be greater than 4\"\n",
        "# parseUnary(t,ner_unary(t))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WuATaUPst6Bv"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def formulateUnary(text, c, const_type):\n",
        "\n",
        "\n",
        "  if( \"VARIABLE\" in c.keys() ):\n",
        "    variable = c[\"VARIABLE\"]\n",
        "  else:\n",
        "    return False, ([] , None, None), None\n",
        "\n",
        "\n",
        "  if( \"PARAMETER\" in c.keys() ):\n",
        "    try:\n",
        "      parameter = [int(c[\"PARAMETER\"])]\n",
        "    except:\n",
        "      return False, ([] , None, None), None\n",
        "  else:\n",
        "    return False, ([] , None, None), None\n",
        "\n",
        "  scope_ids = []\n",
        "  for k in problem_data.keys():\n",
        "      if problem_data[k][\"name\"] == variable:\n",
        "        scope_ids.append(k)\n",
        "\n",
        "  if len(scope_ids) == operators_mappings[const_type][\"arity\"]:\n",
        "    print(scope_ids ,\n",
        "              operators_mappings[const_type][\"operator\"],\n",
        "              operators_mappings[const_type][\"arity\"],\n",
        "              operators_mappings[const_type][\"isCommutative\"],\n",
        "              parameter\n",
        "            )\n",
        "    return True, Constraint( scope_ids ,\n",
        "              operators_mappings[const_type][\"operator\"],\n",
        "              operators_mappings[const_type][\"arity\"],\n",
        "              operators_mappings[const_type][\"isCommutative\"],\n",
        "              parameters = parameter\n",
        "            ), scope_ids\n",
        "  else:\n",
        "    print(\"We couldn't find the scope\")\n",
        "    print(\"Here is the constraint: \", operators_mappings[const_type][\"operator\"])\n",
        "    return False, ([] , variable, None), scope_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eoYO5iIRyXn6"
      },
      "outputs": [],
      "source": [
        "\n",
        "# vars_ids =   [   1 ,   2   ,  3    ,  4     ,   5  ,  6  ,    7   ,   8 ,    9  ,   10  ,   11   ,  12 ,  13  ,  14          ,   15  ,  16     , 17       ,18        ,19          ,20        ,21  , 22, 23,24 ,25]\n",
        "# vars_names = [\"W\",\"green\",\"ivory\",\"yellow\",\"blue\",\"dog\",\"snails\",\"fox\",\"horse\",\"zebra\",\"coffee\",\"tea\",\"milk\",\"orange juice\",\"water\",\"English\",\"Sparniad\",\"Ukranian\",\"Norweigian\",\"Japanese\",\"ol\",\"k\",\"c\",\"L\",\"P\"]\n",
        "# vars_types = [\"color\",\"color\",\"color\",\"color\",\"color\",\"pet\",\"pet\",\"pet\",\"pet\",\"pet\",\"drink\",\"drink\",\"drink\",\"drink\",\"drink\",\"country\",\"country\",\"country\",\"country\",\"country\",\"brand\",\"brand\",\"brand\",\"brand\",\"brand\"]\n",
        "# vars_domains = [(1,5),(1,5),(1,5),(1,5),(1,5),(1,5),(1,5),(1,5),(1,5),(1,5),(1,5),(1,5),(1,5),(1,5),(1,5),(1,5),(1,5),(1,5),(1,5),(1,5),(1,5),(1,5),(1,5),(1,5),(1,5)]\n",
        "# types = [\"color\",\"pet\",\"drink\", \"country\",\"brand\"]\n",
        "\n",
        "\n",
        "# problem_data = {\n",
        "#   id: {\n",
        "#       \"domain\": vars_domains[id-1],\n",
        "#        \"type\": vars_types[id-1],\n",
        "#        \"name\": vars_names[id-1]\n",
        "#   } for id in vars_ids\n",
        "# }\n",
        "\n",
        "# t = \"W should be greater than 4\"\n",
        "# formulateUnary(t,parseUnary(t,ner_unary(t)),\"gtConst\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etxz10Ag6jw2"
      },
      "source": [
        "## ParserBinary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8qbwknJ86nr5"
      },
      "outputs": [],
      "source": [
        "def parseBinary(text,predicted_spans):\n",
        "\n",
        "\n",
        "  i = 0\n",
        "\n",
        "  constraint = {} # begin constructing the constraint\n",
        "\n",
        "  # get indexes of constraint types (e.g. gt, lt ...)\n",
        "  # these indexes will be used to begin the search\n",
        "  # for the other attributes const_elms_left and const_elms_right\n",
        "  while(i<len(predicted_spans[0])):\n",
        "    if(predicted_spans[0][i]['label'] == \"CONST_ELMS_LEFT\"):\n",
        "      constraint[predicted_spans[0][i]['label']] = text[predicted_spans[0][i][\"start\"] : predicted_spans[0][i][\"end\"]]\n",
        "    elif(predicted_spans[0][i]['label'] == \"CONST_ELMS_RIGHT\"):\n",
        "      constraint[predicted_spans[0][i]['label']] = text[predicted_spans[0][i][\"start\"] : predicted_spans[0][i][\"end\"]]\n",
        "    i+=1\n",
        "\n",
        "  return constraint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGE-rv89nxkQ"
      },
      "outputs": [],
      "source": [
        "# t = \"W should be greater than V\"\n",
        "# parseBinary(t,ner_binary(t))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0AZNg3un7KtE"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def formulateBinary(text, c, const_type):\n",
        "\n",
        "  if( \"CONST_ELMS_LEFT\" in c.keys() ):\n",
        "    elm_left = c[\"CONST_ELMS_LEFT\"]\n",
        "  else:\n",
        "    elm_left = None\n",
        "\n",
        "  if( \"CONST_ELMS_RIGHT\" in c.keys() ):\n",
        "    elm_right = c[\"CONST_ELMS_RIGHT\"]\n",
        "  else:\n",
        "    elm_right = None\n",
        "\n",
        "\n",
        "  #print(\"CONST_TYPEEEE\", const_type)\n",
        "  if(elm_left == [] and elm_right == []):\n",
        "    scope = []\n",
        "\n",
        "  elif(elm_left==[]):\n",
        "    scope = [elm_right]\n",
        "\n",
        "  elif(elm_right==[]):\n",
        "    scope = [elm_left]\n",
        "\n",
        "  else:\n",
        "    scope= [elm_left, elm_right]\n",
        "\n",
        "\n",
        "\n",
        "  scope_ids = []\n",
        "\n",
        "  for s in scope:\n",
        "    for k in problem_data.keys():\n",
        "      if problem_data[k][\"name\"] == s:\n",
        "        scope_ids.append(k)\n",
        "\n",
        "  if len(scope_ids) == operators_mappings[const_type][\"arity\"]:\n",
        "    print(scope_ids ,\n",
        "              operators_mappings[const_type][\"operator\"],\n",
        "              operators_mappings[const_type][\"arity\"],\n",
        "              operators_mappings[const_type][\"isCommutative\"])\n",
        "    return True, Constraint( scope_ids ,\n",
        "              operators_mappings[const_type][\"operator\"],\n",
        "              operators_mappings[const_type][\"arity\"],\n",
        "              operators_mappings[const_type][\"isCommutative\"]\n",
        "            ), scope_ids\n",
        "  else:\n",
        "    print(\"We couldn't find the scope\")\n",
        "    print(\"Here is the constraint: \", operators_mappings[const_type][\"operator\"])\n",
        "    return False, ([] , scope, None), scope"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p2AeuMfj7t3h"
      },
      "outputs": [],
      "source": [
        "\n",
        "# vars_ids =   [   1 ,   2   ,  3    ,  4     ,   5  ,  6  ,    7   ,   8 ,    9  ,   10  ,   11   ,  12 ,  13  ,  14          ,   15  ,  16     , 17       ,18        ,19          ,20        ,21  , 22, 23,24 ,25]\n",
        "# vars_names = [\"A\",\"B\",\"ivory\",\"yellow\",\"blue\",\"dog\",\"snails\",\"fox\",\"horse\",\"zebra\",\"coffee\",\"tea\",\"milk\",\"orange juice\",\"water\",\"English\",\"Sparniad\",\"Ukranian\",\"Norweigian\",\"Japanese\",\"ol\",\"k\",\"c\",\"L\",\"P\"]\n",
        "# vars_types = [\"color\",\"color\",\"color\",\"color\",\"color\",\"pet\",\"pet\",\"pet\",\"pet\",\"pet\",\"drink\",\"drink\",\"drink\",\"drink\",\"drink\",\"country\",\"country\",\"country\",\"country\",\"country\",\"brand\",\"brand\",\"brand\",\"brand\",\"brand\"]\n",
        "# vars_domains = [(1,5),(1,5),(1,5),(1,5),(1,5),(1,5),(1,5),(1,5),(1,5),(1,5),(1,5),(1,5),(1,5),(1,5),(1,5),(1,5),(1,5),(1,5),(1,5),(1,5),(1,5),(1,5),(1,5),(1,5),(1,5)]\n",
        "# types = [\"color\",\"pet\",\"drink\", \"country\",\"brand\"]\n",
        "\n",
        "\n",
        "# problem_data = {\n",
        "#   id: {\n",
        "#       \"domain\": vars_domains[id-1],\n",
        "#        \"type\": vars_types[id-1],\n",
        "#        \"name\": vars_names[id-1]\n",
        "#   } for id in vars_ids\n",
        "# }\n",
        "\n",
        "\n",
        "# t= \"the value of A should be less than the value of B\"\n",
        "# ner_binary(t), parseBinary(t, ner_binary(t)) ,formulateBinary(t,parseBinary(t, ner_binary(t)), \"gt\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KBFyBPL9LWN"
      },
      "source": [
        "## ParserDistanceUnary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nr5MPrsXo9Jr"
      },
      "outputs": [],
      "source": [
        "# t = \" distance between A and B should be less than 2\"\n",
        "# ner_DistanceUnary(t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZijYHnH90zD"
      },
      "outputs": [],
      "source": [
        "def parseDistanceUnary(text,predicted_spans):\n",
        "\n",
        "  i=0\n",
        "  constraint = {} # begin constructing the constraint\n",
        "  while(i<len(predicted_spans[0])):\n",
        "    if(predicted_spans[0][i][\"label\"] == \"Dist_Left_Elm_1\"):\n",
        "      constraint[predicted_spans[0][i]['label']] = text[predicted_spans[0][i][\"start\"] : predicted_spans[0][i][\"end\"]]\n",
        "    elif(predicted_spans[0][i][\"label\"] == \"Dist_Left_Elm_2\"):\n",
        "      constraint[predicted_spans[0][i]['label']] = text[predicted_spans[0][i][\"start\"] : predicted_spans[0][i][\"end\"]]\n",
        "    elif(predicted_spans[0][i][\"label\"] == \"Parameter\"):\n",
        "      constraint[predicted_spans[0][i]['label']] = text[predicted_spans[0][i][\"start\"] : predicted_spans[0][i][\"end\"]]\n",
        "    i+=1\n",
        "\n",
        "\n",
        "\n",
        "  return constraint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AABFDeMlpXrt"
      },
      "outputs": [],
      "source": [
        "# t = \" distance between A and B should be less than 2\"\n",
        "# parseDistanceUnary(t,ner_DistanceUnary(t))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVmw2SwR9YiQ"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def formulateDistanceUnary(text, c, const_type):\n",
        "\n",
        "# \"DistneValue\",\n",
        "# \"Dist_Left_Elm_1\",\n",
        "# \"Dist_Left_Elm_2\",\n",
        "# \"Parameter\"\n",
        "\n",
        "\n",
        "  # Dist_Left_Elm_1\n",
        "  if( \"Dist_Left_Elm_1\" in c.keys() ):\n",
        "    dist_left_elm1 = c[\"Dist_Left_Elm_1\"]\n",
        "  else:\n",
        "    dist_left_elm1 = []\n",
        "\n",
        "  # Dist_Left_Elm_2\n",
        "  if( \"Dist_Left_Elm_2\" in c.keys() ):\n",
        "    dist_left_elm2 = c[\"Dist_Left_Elm_2\"]\n",
        "  else:\n",
        "    dist_left_elm2 = []\n",
        "\n",
        "  # Parameter\n",
        "\n",
        "  if(\"Parameter\" in c.keys() ):\n",
        "    try:\n",
        "      parameter = [int(c[\"Parameter\"])]\n",
        "    except:\n",
        "      return False, ([] , None, None), None\n",
        "  else:\n",
        "    return False, ([] , None, None), None\n",
        "\n",
        "\n",
        "\n",
        "  if(dist_left_elm1 == [] or dist_left_elm2 == []):\n",
        "    scope = []\n",
        "  else:\n",
        "    scope = [dist_left_elm1,dist_left_elm2]\n",
        "\n",
        "\n",
        "  scope_ids = []\n",
        "\n",
        "  for s in scope:\n",
        "    for k in problem_data.keys():\n",
        "      if problem_data[k][\"name\"] == s:\n",
        "        scope_ids.append(k)\n",
        "\n",
        "  if len(scope_ids) == operators_mappings[const_type][\"arity\"] and parameter!=[]:\n",
        "    print(   scope_ids,\n",
        "              operators_mappings[const_type][\"operator\"],\n",
        "              operators_mappings[const_type][\"arity\"],\n",
        "              operators_mappings[const_type][\"isCommutative\"],\n",
        "              parameter\n",
        "          )\n",
        "    return True, Constraint( scope_ids ,\n",
        "                        operators_mappings[const_type][\"operator\"],\n",
        "                        operators_mappings[const_type][\"arity\"],\n",
        "                        operators_mappings[const_type][\"isCommutative\"],\n",
        "                        parameter\n",
        "            ), scope_ids\n",
        "  else:\n",
        "    print(\"We couldn't find the scope or the parameter\")\n",
        "    print(\"Here is the constraint: \", operators_mappings[const_type][\"operator\"])\n",
        "    return False, ([] , scope, None), scope"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7wj_m-Uv728P"
      },
      "outputs": [],
      "source": [
        "# formulateDistanceUnary(\"A and B should be less than 4\",parseDistanceUnary(\"A and B should be less than 4\",ner_DistanceUnary(\"A and B should be less than 4\")), \"DistltValue\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sj5wKsfd6l5L"
      },
      "source": [
        "## ParserDistanceBinary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKJQgNG2qA3t"
      },
      "outputs": [],
      "source": [
        "# t = \"distance between A 2 and F should be equal to distance between Zebra and bsvd\"\n",
        "# ner_DistanceBinary(t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mXen9nNB6ocO"
      },
      "outputs": [],
      "source": [
        "def parseDistanceBinary(text,predicted_spans):\n",
        "  i=0\n",
        "  constraint = {} # begin constructing the constraint\n",
        "  while(i<len(predicted_spans[0])):\n",
        "    if(predicted_spans[0][i][\"label\"] == \"Dist_Left_Elm_1\"):\n",
        "      constraint[predicted_spans[0][i]['label']] = text[predicted_spans[0][i][\"start\"] : predicted_spans[0][i][\"end\"]]\n",
        "    elif(predicted_spans[0][i][\"label\"] == \"Dist_Left_Elm_2\"):\n",
        "      constraint[predicted_spans[0][i]['label']] = text[predicted_spans[0][i][\"start\"] : predicted_spans[0][i][\"end\"]]\n",
        "    elif(predicted_spans[0][i][\"label\"] == \"Dist_Right_Elm_1\"):\n",
        "      constraint[predicted_spans[0][i]['label']] = text[predicted_spans[0][i][\"start\"] : predicted_spans[0][i][\"end\"]]\n",
        "    elif(predicted_spans[0][i][\"label\"] == \"Dist_Right_Elm_2\"):\n",
        "      constraint[predicted_spans[0][i]['label']] = text[predicted_spans[0][i][\"start\"] : predicted_spans[0][i][\"end\"]]\n",
        "    i+=1\n",
        "\n",
        "  return constraint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GmCtSZvaqecU"
      },
      "outputs": [],
      "source": [
        "# t = \"distance between A 2 and F should be equal to distance between Zebra and bsvd\"\n",
        "# parseDistanceBinary(t,ner_DistanceBinary(t))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8We0fog83_6"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def formulateDistanceBinary(text, c, const_type):\n",
        "\n",
        "\n",
        "  # Dist_Left_Elm_1\n",
        "\n",
        "  if( \"Dist_Left_Elm_1\" in c.keys()):\n",
        "    dist_left_elm1 = c[\"Dist_Left_Elm_1\"]\n",
        "  else:\n",
        "    dist_left_elm1 = []\n",
        "\n",
        "\n",
        "  if( \"Dist_Left_Elm_2\" in c.keys()):\n",
        "    dist_left_elm2 = c[\"Dist_Left_Elm_2\"]\n",
        "  else:\n",
        "    dist_left_elm2 = []\n",
        "\n",
        "\n",
        "  if( \"Dist_Right_Elm_1\" in c.keys()):\n",
        "    dist_right_elm1 = c[\"Dist_Right_Elm_1\"]\n",
        "  else:\n",
        "    dist_right_elm1 = []\n",
        "\n",
        "\n",
        "  if( \"Dist_Right_Elm_2\" in c.keys()):\n",
        "    dist_right_elm2 = c[\"Dist_Right_Elm_2\"]\n",
        "  else:\n",
        "    dist_right_elm2 = []\n",
        "\n",
        "# TODO: check if an id is repeated\n",
        "  if(\n",
        "      dist_right_elm2 == []\n",
        "      or\n",
        "      dist_right_elm1 == []\n",
        "      or\n",
        "      dist_left_elm1 == []\n",
        "      or\n",
        "      dist_left_elm2 == []\n",
        "      ):\n",
        "    scope = []\n",
        "  else:\n",
        "    scope = [dist_left_elm1,dist_left_elm2,dist_right_elm1,dist_right_elm2]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  scope_ids = []\n",
        "  for s in scope:\n",
        "    for k in problem_data.keys():\n",
        "      if problem_data[k][\"name\"] == s:\n",
        "        scope_ids.append(k)\n",
        "\n",
        "\n",
        "  if len(scope_ids) == operators_mappings[const_type][\"arity\"]:\n",
        "    print(scope_ids ,\n",
        "              operators_mappings[const_type][\"operator\"],\n",
        "              operators_mappings[const_type][\"arity\"],\n",
        "              operators_mappings[const_type][\"isCommutative\"])\n",
        "    return True ,Constraint( scope_ids ,\n",
        "              operators_mappings[const_type][\"operator\"],\n",
        "              operators_mappings[const_type][\"arity\"],\n",
        "              operators_mappings[const_type][\"isCommutative\"]\n",
        "            ), scope_ids\n",
        "  else:\n",
        "    print(\"We couldn't find the scope\")\n",
        "    print(\"Here is the constraint: \", operators_mappings[const_type][\"operator\"])\n",
        "    return False, ([] , scope, None), scope"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L9l33iFz_HT3"
      },
      "outputs": [],
      "source": [
        "# vars_ids =   [   1 ,   2   ,  3    ,  4     ,   5  ,  6  ,    7   ,   8 ,    9  ,   10  ,   11   ,  12 ,  13  ,  14          ,   15  ,  16     , 17       ,18        ,19          ,20        ,21  , 22, 23,24 ,25]\n",
        "# vars_names = [\"X\",\"Y\",\"C\",\"B\",\"blue\",\"dog\",\"snails\",\"fox\",\"horse\",\"zebra\",\"coffee\",\"tea\",\"milk\",\"orange juice\",\"water\",\"English\",\"Sparniad\",\"Ukranian\",\"Norweigian\",\"Japanese\",\"ol\",\"k\",\"c\",\"L\",\"P\"]\n",
        "# vars_types = [\"color\",\"color\",\"color\",\"color\",\"color\",\"pet\",\"pet\",\"pet\",\"pet\",\"pet\",\"drink\",\"drink\",\"drink\",\"drink\",\"drink\",\"country\",\"country\",\"country\",\"country\",\"country\",\"brand\",\"brand\",\"brand\",\"brand\",\"brand\"]\n",
        "# vars_domains = [(1,5),(1,5),(1,5),(1,5),(1,5),(1,5),(1,5),(1,5),(1,5),(1,5),(1,5),(1,5),(1,5),(1,5),(1,5),(1,5),(1,5),(1,5),(1,5),(1,5),(1,5),(1,5),(1,5),(1,5),(1,5)]\n",
        "# types = [\"color\",\"pet\",\"drink\", \"country\",\"brand\"]\n",
        "\n",
        "\n",
        "# problem_data = {\n",
        "#   id: {\n",
        "#       \"domain\": vars_domains[id-1],\n",
        "#        \"type\": vars_types[id-1],\n",
        "#        \"name\": vars_names[id-1]\n",
        "#   } for id in vars_ids\n",
        "# }\n",
        "\n",
        "\n",
        "# t = \"the distance from X to Y should be different than the distance between C and B\"\n",
        "\n",
        "\n",
        "# formulateDistanceBinary(t,parseDistanceBinary(t, ner_DistanceBinary(t)),\"DistneDist\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T__Dz8vFtcOZ"
      },
      "source": [
        "# pyQuAcqNLP"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## QuAcq"
      ],
      "metadata": {
        "id": "dghtreaVcLju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ConstraintinListOfConstraints(a,listOfConsts):\n",
        "\n",
        "  for l in listOfConsts:\n",
        "    if(a.isEqualTo(l)):\n",
        "      return True\n",
        "\n",
        "  return False\n",
        "\n",
        "\n",
        "def existAlready2(res,c):\n",
        "\n",
        "    if(ConstraintinListOfConstraints(c, res)):\n",
        "        return True\n",
        "    res.append(c)\n",
        "    return False\n",
        "\n",
        "def removeDuplicates(listOfConstraints):\n",
        "  res = []\n",
        "  nonDup = []\n",
        "  for c in listOfConstraints:\n",
        "    if(existAlready2(res,c)):\n",
        "      continue\n",
        "    else:\n",
        "      nonDup.append(c)\n",
        "\n",
        "  # print(\"\\nBefore remove inside function: \", len(listOfConstraints))\n",
        "  # print(\"\\nAfter remove inside function: \", len(nonDup))\n",
        "  return nonDup\n",
        "\n",
        "def RegularFindC(e,Y,L,B):\n",
        "  global cpt\n",
        "  global newsize\n",
        "  global oldsizeexamples\n",
        "  global newsizeexamples\n",
        "  global nbExamplesc\n",
        "  global nbExamplesa\n",
        "  global Exampletimes\n",
        "  global byQuAcq\n",
        "  global newsizenlpqueries1\n",
        "  global newsizenlpqueries2\n",
        "  global newsizenlpqueries3\n",
        "  global nbConfirmationQueriesC\n",
        "  global nbConfirmationQueriesA\n",
        "  global oldsizenlpqueries2\n",
        "  global byNLP\n",
        "\n",
        "\n",
        "  delta = []\n",
        "\n",
        "  delta = B.ConstraintsIsExactlyY(Y)\n",
        "\n",
        "  tmpp = delta.networkOfConstraintsThatRejectE(e)\n",
        "\n",
        "  # tmpp.constraints = removeDuplicates(tmpp.constraints)\n",
        "\n",
        "  delta = joinNetworks( delta , tmpp)\n",
        "\n",
        "  global findEPrimeCall\n",
        "  while(True):\n",
        "\n",
        "    print(\"\\nSTARTSTARTSTARTSTARTSTARTSTARTSTART findEPrimeCall: \",findEPrimeCall,\"---------------------\\n\\n\\n\")\n",
        "    print(\"\\n\\nDelta before findEPrime: \\n\")\n",
        "    for c in delta.constraints:\n",
        "      print(c.rel, c.scope_ids)\n",
        "\n",
        "\n",
        "    print(\"\\n\\nL[Y] before findEPrime: \\n\")\n",
        "    for c in L.ConstraintsIncludedInY(Y).constraints:\n",
        "      print(c.rel, c.scope_ids)\n",
        "\n",
        "\n",
        "    stg = time.time()\n",
        "    ep, ll, d = findEPrime(L,Y,delta)\n",
        "    #register example sizes and times to generate them\n",
        "    Exampletimes.append(time.time()-stg)\n",
        "\n",
        "    #calculate number of examples generated until convergence and until finding an equivalent network\n",
        "    nbExamplesc+=1\n",
        "    if(newsizeexamples > oldsizeexamples):\n",
        "      oldsizeexamples = newsizeexamples\n",
        "      if(nbExamplesc > nbExamplesa):\n",
        "        nbExamplesa = nbExamplesc\n",
        "\n",
        "\n",
        "    print(\"\\n Example: \", ep)\n",
        "\n",
        "    print(\"\\n\\nDelta returned from findEPrime: \\n\")\n",
        "    for c in delta.constraints:\n",
        "      print(c.rel, c.scope_ids)\n",
        "\n",
        "\n",
        "    print(\"\\n\\nL[Y] returned from findEPrime: \\n\")\n",
        "    for c in L.ConstraintsIncludedInY(Y).constraints:\n",
        "      print(c.rel, c.scope_ids)\n",
        "\n",
        "    print(\"\\nENDENDENDENDENDENDENDENDENDENDEND findEPrimeCall: \",findEPrimeCall,\"---------------------\\n\\n\\n\")\n",
        "    findEPrimeCall+=1\n",
        "    if ep == []:\n",
        "      print(\"\\n A new constraint was added to the being learned network !!!:\\n\")\n",
        "      elementary = [c for c in delta.constraints if isinstance(c,Constraint)]\n",
        "      Conjs = [c for c in delta.constraints if isinstance(c,Conjunction)]\n",
        "\n",
        "      if(len(elementary) != 0):\n",
        "        c = elementary[0]\n",
        "      else:\n",
        "        min = 100000\n",
        "        c = None\n",
        "        for cc in Conjs:\n",
        "          if(len(cc.elementaryConstraints)<min):\n",
        "            min = len(cc.elementaryConstraints)\n",
        "            c = cc\n",
        "      #c = delta.constraints[0]\n",
        "\n",
        "\n",
        "      L = L.addConstraint(c)\n",
        "      byQuAcq+=1\n",
        "      newsize+=1\n",
        "      newsizeexamples+=1\n",
        "      newsizenlpqueries1+=1\n",
        "      newsizenlpqueries2+=1\n",
        "      newsizenlpqueries3+=1\n",
        "\n",
        "      print(\"\\n Number of constraints so far: \\n\", len(L.constraints))\n",
        "      B = B.removeListOfConstraints(B.ConstraintsIsExactlyY(Y).constraints)\n",
        "      print(\"\\n FindC : Remaining constraints in the Basis: \\n\", len(B.constraints))\n",
        "      # print(input())\n",
        "      #break\n",
        "      return Y\n",
        "    else:\n",
        "      if(ask(ep,Target)[0]):\n",
        "        delta = delta.removeConstraintsThatRejectE(ep)\n",
        "\n",
        "        B = B.removeConstraintsThatRejectE(ep)\n",
        "\n",
        "        print(\"\\n FindC : B reduced in size, new size: \",len(B.constraints))\n",
        "      else:\n",
        "\n",
        "        S = FindScope(ep,[],Y,B)\n",
        "        if set(S).issubset(set(Y)) and len(S)!=len(Y) : #and S!=[]:\n",
        "          RegularFindC(ep,S,L,B)\n",
        "        else:\n",
        "          tmp1 = delta\n",
        "          tmp11 = tmp1.networkOfConstraintsThatRejectE(ep)\n",
        "          delta = joinNetworks(tmp1, tmp11)\n",
        "\n",
        "          print(\"\\nAfter join findcloop: \")\n",
        "          print(len(delta.constraints))"
      ],
      "metadata": {
        "id": "uGXUJV4OcLDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# main\n",
        "\n",
        "def RegularQuAcq2(B,L):\n",
        "  global Target\n",
        "  global problem_data\n",
        "  global redundants\n",
        "  global nbExamplesc\n",
        "  global nbExamplesa\n",
        "  global oldsizeexamples\n",
        "  global newsizeexamples\n",
        "  global Exampletimes\n",
        "\n",
        "\n",
        "\n",
        "  print(\"In QuAcq2: len of constraints in B is: \", len(B.constraints))\n",
        "  cpt = 0\n",
        "  while(True):\n",
        "\n",
        "    stg = time.time()\n",
        "    e = generateExample(list(problem_data.keys()),L, B)\n",
        "    #register example times to generate them\n",
        "    Exampletimes.append(time.time()-stg)\n",
        "\n",
        "    #calculate number of examples generated until convergence and until finding an equivalent network\n",
        "    nbExamplesc+=1\n",
        "    if(newsizeexamples > oldsizeexamples):\n",
        "      oldsizeexamples = newsizeexamples\n",
        "      if(nbExamplesc > nbExamplesa):\n",
        "        nbExamplesa = nbExamplesc\n",
        "\n",
        "\n",
        "    if(e==[]):\n",
        "      return L,None # convergence\n",
        "\n",
        "    if(ask(e, Target)[0]):\n",
        "\n",
        "      B = B.removeConstraintsThatRejectE(e)\n",
        "      print(\"\\nQuacq B reduced in size, new size: \",len(B.constraints))\n",
        "    else:\n",
        "      scope_ids = RegularFindC(e, FindScope(e , [] , list(problem_data.keys()), B ), L, B)\n",
        "      return L,scope_ids\n"
      ],
      "metadata": {
        "id": "OLaRoJPPcVRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kMvhhVlpzAcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ggVrguCPBZ0"
      },
      "source": [
        "## getConstraint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s9H5L_oswFfs"
      },
      "outputs": [],
      "source": [
        "def getConstraint(text):\n",
        "  global Target\n",
        "  global problem_data\n",
        "  global classifModel\n",
        "  global ClassifyTimes\n",
        "  global NERTimes\n",
        "\n",
        "\n",
        "  tclass = time.time()\n",
        "  pred, logits = classify(text, classifModel)\n",
        "  ClassifyTimes.append(time.time()-tclass)\n",
        "\n",
        "  t = pred.split(\"_\")\n",
        "\n",
        "  if t[0] == \"UNARY\":\n",
        "    if t[1] == \"gt\":\n",
        "      const_type = \"gtConst\"\n",
        "    elif t[1] == \"ge\":\n",
        "      const_type = \"geConst\"\n",
        "    elif t[1] == \"lt\":\n",
        "      const_type = \"ltConst\"\n",
        "    elif t[1] == \"le\":\n",
        "      const_type = \"leConst\"\n",
        "    elif t[1] == \"eq\":\n",
        "      const_type = \"eqConst\"\n",
        "    elif t[1] == \"ne\":\n",
        "      const_type = \"neConst\"\n",
        "\n",
        "    tner = time.time()\n",
        "    resNER = ner_unary(text,modelnerunary)\n",
        "    NERTimes.append(time.time() - tner)\n",
        "\n",
        "    return formulateUnary(text,parseUnary(text,resNER),const_type)\n",
        "\n",
        "  elif t[0] == \"UNARYDIST\":\n",
        "\n",
        "\n",
        "    if t[1] == \"gt\":\n",
        "      const_type = \"DistgtValue\"\n",
        "    elif t[1] == \"ge\":\n",
        "      const_type = \"DistgeValue\"\n",
        "    elif t[1] == \"lt\":\n",
        "      const_type = \"DistltValue\"\n",
        "    elif t[1] == \"le\":\n",
        "      const_type = \"DistleValue\"\n",
        "    elif t[1] == \"eq\":\n",
        "      const_type = \"DisteqValue\"\n",
        "    elif t[1] == \"ne\":\n",
        "      const_type = \"DistneValue\"\n",
        "\n",
        "    tner = time.time()\n",
        "    resNER = ner_DistanceUnary(text,modelDistanceUnary)\n",
        "    NERTimes.append(time.time() - tner)\n",
        "\n",
        "    return formulateDistanceUnary(text,parseDistanceUnary(text,resNER),const_type)\n",
        "\n",
        "  elif t[0] == \"BINARYDIST\":\n",
        "\n",
        "    if t[1] == \"gt\":\n",
        "      const_type = \"DistgtDist\"\n",
        "    elif t[1] == \"ge\":\n",
        "      const_type = \"DistgeDist\"\n",
        "    elif t[1] == \"lt\":\n",
        "      const_type = \"DistltDist\"\n",
        "    elif t[1] == \"le\":\n",
        "      const_type = \"DistleDist\"\n",
        "    elif t[1] == \"eq\":\n",
        "      const_type = \"DisteqDist\"\n",
        "    elif t[1] == \"ne\":\n",
        "      const_type = \"DistneDist\"\n",
        "\n",
        "\n",
        "    tner = time.time()\n",
        "    resNER = ner_DistanceBinary(text,modelDistanceBinary)\n",
        "    NERTimes.append(time.time() - tner)\n",
        "\n",
        "\n",
        "    return formulateDistanceBinary(text,parseDistanceBinary(text,resNER),const_type)\n",
        "\n",
        "  elif t[0] == \"BINARY\":\n",
        "    const_type = t[1]\n",
        "\n",
        "    tner = time.time()\n",
        "    resNER = ner_binary(text,modelnerbinary)\n",
        "    NERTimes.append(time.time() - tner)\n",
        "\n",
        "    return formulateBinary(text,parseBinary(text,resNER),const_type)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlSOkkydO_Bu"
      },
      "source": [
        "## conversation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zVUisCqOvSjV"
      },
      "outputs": [],
      "source": [
        "def conversation(e, cons):\n",
        "\n",
        "  global oldsizenlpqueries1\n",
        "  global newsizenlpqueries1\n",
        "\n",
        "  global oldsizenlpqueries2\n",
        "  global newsizenlpqueries2\n",
        "\n",
        "  global oldsizenlpqueries3\n",
        "  global newsizenlpqueries3\n",
        "\n",
        "  global nlpConstraints\n",
        "  global Examplesizes\n",
        "\n",
        "  global nbWhatIsWrongQueriesA\n",
        "  global nbWhatIsWrongQueriesC\n",
        "\n",
        "  global nbConfirmationQueriesA\n",
        "  global nbConfirmationQueriesC\n",
        "\n",
        "  global nbReformulationQueriesA\n",
        "  global nbReformulationQueriesC\n",
        "\n",
        "  Examplesizes.append(len(e))\n",
        "\n",
        "\n",
        "  nbWhatIsWrongQueriesC += 1\n",
        "\n",
        "\n",
        "  if(newsizenlpqueries1 > oldsizenlpqueries1):\n",
        "    oldsizenlpqueries1 = newsizenlpqueries1\n",
        "    if(nbWhatIsWrongQueriesC > nbWhatIsWrongQueriesA):\n",
        "      nbWhatIsWrongQueriesA = nbWhatIsWrongQueriesC\n",
        "\n",
        "  print(\"\\n\\n Can you tell what is wrong with this example: \")\n",
        "  #print(ex)\n",
        "  # mat = []\n",
        "\n",
        "  # cpt = 1\n",
        "  # for i in range(3):\n",
        "  #   mat.append([])\n",
        "  #   for j in range(4):\n",
        "  #     mat[i].append(cpt)\n",
        "  #     cpt+=1\n",
        "\n",
        "\n",
        "  # for ee in e:\n",
        "  #   for i in range(3):\n",
        "  #     for j in range(4):\n",
        "  #       if(ee[0] == mat[i][j]):\n",
        "  #         mat[i][j] = ee\n",
        "\n",
        "  # for i in range(len(mat)):\n",
        "  #   print(mat[i])\n",
        "  #   print()\n",
        "  print(e)\n",
        "\n",
        "  print(\"\\n\\n\")\n",
        "\n",
        "  #problem = input()\n",
        "\n",
        "  for c in coords:\n",
        "    print(\"Here !!!!\")\n",
        "    print(c[\"indice\"],list(c[\"scope\"]), c[\"rel\"],\"\\n\")\n",
        "    print(cons.scope_ids, cons.rel, cons.parameters,\"\\n\")\n",
        "    print(\"----------\\n\")\n",
        "    print(list(c[\"scope\"]) == cons.scope_ids and c[\"rel\"].__name__ == cons.rel.__name__)\n",
        "    #print(input())\n",
        "    if list(c[\"scope\"]) == cons.scope_ids and c[\"rel\"].__name__ == cons.rel.__name__:\n",
        "      indice = c[\"indice\"]\n",
        "  print(nlpConstraints[indice])\n",
        "  problem = nlpConstraints[indice]\n",
        "  #nlpConstraints = nlpConstraints[:indice] + nlpConstraints[indice+1:]\n",
        "\n",
        "  if(problem == \"Nothing wrong\"): # train a model to detect similar expressions\n",
        "    return \"Converged\", []\n",
        "  elif(problem == \"Try to find it yourself\"): # train a model to detect similar expressions\n",
        "    return \"FindC\", []\n",
        "\n",
        "\n",
        "  res, c , scope_ids = getConstraint(problem)\n",
        "  #print(\"\\n\\nConstraints: \", constraints)\n",
        "  #print(\"\\n\\nL's Constraints: \", L.constraints)\n",
        "\n",
        "  while(True):\n",
        "    if res:\n",
        "      # add confirmation query\n",
        "      nbConfirmationQueriesC += 1\n",
        "\n",
        "\n",
        "      if(newsizenlpqueries2 > oldsizenlpqueries2):\n",
        "        oldsizenlpqueries2 = newsizenlpqueries2\n",
        "        if(nbConfirmationQueriesC > nbConfirmationQueriesA):\n",
        "          nbConfirmationQueriesA = nbConfirmationQueriesC\n",
        "\n",
        "      print(\"Is this what you meant ?:\\n\", c.rel,c.scope_ids,c.isCommutative,c.parameters,c.arity)\n",
        "      print(\"cons: \\n\", cons.rel,cons.scope_ids,cons.isCommutative,cons.parameters,cons.arity)\n",
        "      print(\"test: \\n\", c.rel == cons.rel and c.scope_ids == cons.scope_ids and c.parameters == cons.parameters and c.isCommutative == cons.isCommutative and c.arity == cons.arity)\n",
        "      print(\"one by one: \\n\")\n",
        "      print(\"c.rel == cons.rel: \",c.rel == cons.rel)\n",
        "      print(\"c.scope_ids == cons.scope_ids \",c.scope_ids == cons.scope_ids)\n",
        "      print(\"c.parameters == cons.parameters: \",c.parameters == cons.parameters)\n",
        "      print(\"c.isCommutative == cons.isCommutative: \",c.isCommutative == cons.isCommutative)\n",
        "      print(\"c.arity == cons.arity: \",c.arity == cons.arity)\n",
        "\n",
        "      if c.rel == cons.rel and c.scope_ids == cons.scope_ids and c.parameters == cons.parameters and c.isCommutative == cons.isCommutative and c.arity == cons.arity:\n",
        "        return c, scope_ids\n",
        "      else:\n",
        "        print(\"Sorry for this confusion, would you reformulate it ? or let me try to find it\")\n",
        "        nbReformulationQueriesC += 1\n",
        "        if(newsizenlpqueries3 > oldsizenlpqueries3):\n",
        "          oldsizenlpqueries3 = newsizenlpqueries3\n",
        "          if(nbReformulationQueriesC > nbReformulationQueriesA):\n",
        "            nbReformulationQueriesA = nbReformulationQueriesC\n",
        "\n",
        "        answer = \"just try to find it\" #input()\n",
        "        if answer == \"just try to find it\": # train a model to detect similar expressions\n",
        "          return \"FindC\", []\n",
        "        else:\n",
        "          res, c, scope_ids = getConstraint(answer)\n",
        "\n",
        "    else: # TODO: manage the case where we couldn't find some parameters and the case of inccorrect prediction; more questions may be asked to the user\n",
        "      # when we fire regular findC Also in partial queries (inside findC and findScope) use nlp !!\n",
        "      print(\"I couldn't understand that, would you reformulate it ? or let me try to find it\")\n",
        "      # here a metric of number of constraints found by regular quack\n",
        "      nbReformulationQueriesC += 1\n",
        "      if(newsizenlpqueries3 > oldsizenlpqueries3):\n",
        "        oldsizenlpqueries3 = newsizenlpqueries3\n",
        "        if(nbReformulationQueriesC > nbReformulationQueriesA):\n",
        "          nbReformulationQueriesA = nbReformulationQueriesC\n",
        "\n",
        "      answer = \"just try to find it\" #input()\n",
        "      if answer == \"just try to find it\": # train a model to detect similar expressions\n",
        "        return \"FindC\", []\n",
        "      else:\n",
        "        res, c, scope_ids = getConstraint(answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJmNdefvO7tm"
      },
      "source": [
        "## QuAcqBert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FMt8vqsftisA"
      },
      "outputs": [],
      "source": [
        "def QuAcq2(B):\n",
        "  global Target\n",
        "  global problem_data\n",
        "  global redundants\n",
        "  global nbExamplesc\n",
        "  global nbExamplesa\n",
        "  global oldsizeexamples\n",
        "  global newsizeexamples\n",
        "  global Exampletimes\n",
        "  global newsize\n",
        "  global byNLP\n",
        "  global newsizenlpqueries1\n",
        "  global newsizenlpqueries2\n",
        "  global newsizenlpqueries3\n",
        "  global Bbk\n",
        "\n",
        "  print(\"In QuAcq2: len of constraints in B is: \", len(B.constraints))\n",
        "  L = Network(problem_data,[])\n",
        "  cpt = 0\n",
        "  while(True):\n",
        "\n",
        "    stg = time.time()\n",
        "    e = generateExample(list(problem_data.keys()),L, B)\n",
        "    #register example times to generate them\n",
        "    Exampletimes.append(time.time()-stg)\n",
        "\n",
        "    #calculate number of examples generated until convergence and until finding an equivalent network\n",
        "    nbExamplesc+=1\n",
        "    if(newsizeexamples > oldsizeexamples):\n",
        "      oldsizeexamples = newsizeexamples\n",
        "      if(nbExamplesc > nbExamplesa):\n",
        "        nbExamplesa = nbExamplesc\n",
        "\n",
        "\n",
        "    if(e==[]):\n",
        "      return True, L # convergence\n",
        "\n",
        "\n",
        "    res, cons = ask(e, Target)\n",
        "\n",
        "    if(res):\n",
        "      # print(e,\" was classified positive positive\")\n",
        "      B = B.removeConstraintsThatRejectE(e)\n",
        "      print(\"\\nQuacq B reduced in size, new size: \",len(B.constraints))\n",
        "    else:\n",
        "      # print(e,\" was classified negative\")\n",
        "      res, scope_ids = conversation(e, cons)\n",
        "      if(res==\"FindC\"):\n",
        "        # think of calling RegularQuAcq2 here with original basis and L as background knowledge\n",
        "        #FindC(e, FindScope(e , [] , list(problem_data.keys()), B ), L, B)\n",
        "        scope_ids = RegularFindC(e, FindScope(e , [] , list(problem_data.keys()), Bbk ), L, Bbk)\n",
        "        B = B.removeListOfConstraints(B.ConstraintsIsExactlyY(scope_ids).constraints)\n",
        "      elif(res==\"Converged\"):\n",
        "        return True, L # convergence\n",
        "      else:\n",
        "        L = L.addConstraint(res)\n",
        "        print(\"The nlp added constraint: \\n\")\n",
        "        print(res.rel, res.scope_ids)\n",
        "        print(\"!!!!!!!!!!!!!!!!!!!!!!!!!\\n\\n\")\n",
        "        byNLP+=1\n",
        "        newsize+=1\n",
        "        newsizeexamples+=1\n",
        "        newsizenlpqueries1+=1\n",
        "        newsizenlpqueries2+=1\n",
        "        newsizenlpqueries3+=1\n",
        "        B = B.removeListOfConstraints(B.ConstraintsIsExactlyY(scope_ids).constraints)\n",
        "        print(\"L after nlp: \", len(L.constraints))\n",
        "\n",
        "  return L\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "egdslKVGCPF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ExperimentZebra"
      ],
      "metadata": {
        "id": "ofqGo0cs0m3M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## One run"
      ],
      "metadata": {
        "id": "A2sD-R-DO13S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import pickle\n",
        "\n",
        "# filepath = \"/content/constraintsNLPzebra4.txt\"\n",
        "# coordspath = \"/content/zebraCoods.txt\"\n",
        "\n",
        "# nlpConstraints = []\n",
        "# with open(filepath, \"r\") as f:\n",
        "#   for l in f.readlines():\n",
        "#     nlpConstraints.append(l)\n",
        "\n",
        "# coords = []\n",
        "# with open(coordspath, \"rb\") as f:\n",
        "#   coords = pickle.load(f)\n",
        "\n",
        "# print(coords)"
      ],
      "metadata": {
        "id": "3XhGBP53x02d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calls= 0\n",
        "\n",
        "# from random import random\n",
        "\n",
        "# vars_ids =   [   1 ,   2   ,  3    ,  4     ,   5  ,  6  ,    7   ,   8 ,    9  ,   10  ,   11   ,  12 ,  13  ,  14          ,   15  ,  16     , 17       ,18        ,19          ,20        ,21  , 22, 23,24 ,25]\n",
        "# vars_names = [\"red\",\"green\",\"ivory\",\"yellow\",\"blue\",\"dog\",\"snails\",\"fox\",\"horse\",\"zebra\",\"coffee\",\"tea\",\"milk\",\"orange juice\",\"water\",\"English\",\"Sparniad\",\"Ukranian\",\"Norweigian\",\"Japanese\",\"Old Gold\",\"Kools\",\"Chesterfields\",\"Lucky Strike\",\"Parliaments\"]\n",
        "# vars_types = [\"color\",\"color\",\"color\",\"color\",\"color\",\"pet\",\"pet\",\"pet\",\"pet\",\"pet\",\"drink\",\"drink\",\"drink\",\"drink\",\"drink\",\"country\",\"country\",\"country\",\"country\",\"country\",\"brand\",\"brand\",\"brand\",\"brand\",\"brand\"]\n",
        "# vars_domains = [(1,5),(1,5),(1,5),(1,5),(1,5),(1,5),(1,5),(1,5),(1,5),(1,5),(1,5),(1,5),(1,5),(1,5),(1,5),(1,5),(1,5),(1,5),(1,5),(1,5),(1,5),(1,5),(1,5),(1,5),(1,5)]\n",
        "# types = [\"color\",\"pet\",\"drink\", \"country\",\"brand\"]\n",
        "\n",
        "\n",
        "# problem_data = {\n",
        "#   id: {\n",
        "#       \"domain\": vars_domains[id-1],\n",
        "#        \"type\": vars_types[id-1],\n",
        "#        \"name\": vars_names[id-1]\n",
        "#   } for id in vars_ids\n",
        "# }\n",
        "\n",
        "\n",
        "\n",
        "# vals = [1,2,3,4,5]\n",
        "# ops = [operator.ge,operator.le,operator.lt,operator.gt,operator.ne,operator.eq]\n",
        "\n",
        "\n",
        "# language = [ (operator.ge,2, False,None),\n",
        "#              (operator.le,2, False,None),\n",
        "#              (operator.lt,2, False,None),\n",
        "#              (operator.gt,2, False,None),\n",
        "#              (operator.ne,2,True,None),\n",
        "#              (operator.eq,2,True,None),\n",
        "#              (eqConst,1, False,[1,2,3,4,5]),\n",
        "#              (geConst,1, False,[1,2,3,4,5]),\n",
        "#              (leConst,1, False,[1,2,3,4,5]),\n",
        "#              (ltConst,1, False,[1,2,3,4,5]),\n",
        "#              (gtConst,1, False,[1,2,3,4,5]),\n",
        "#              (neConst,1, False,[1,2,3,4,5]),\n",
        "#              (eqDist1,2,True,None),\n",
        "#              (neDist1,2,True,None)\n",
        "#              ]\n",
        "# constraints = [\n",
        "\n",
        "#     # alldiff colors\n",
        "#     Constraint([1,2], operator.ne, 2,True),\n",
        "#     Constraint([1,3], operator.ne, 2,True),\n",
        "#     Constraint([1,4], operator.ne, 2,True),\n",
        "#     Constraint([1,5], operator.ne, 2,True),\n",
        "#     Constraint([2,3], operator.ne, 2,True),\n",
        "#     Constraint([2,4], operator.ne, 2,True),\n",
        "#     Constraint([2,5], operator.ne, 2,True),\n",
        "#     Constraint([3,4], operator.ne, 2,True),\n",
        "#     Constraint([3,5], operator.ne, 2,True),\n",
        "#     Constraint([4,5], operator.ne, 2,True),\n",
        "\n",
        "#     # alldiff pets 6,7,8,9,10\n",
        "#     Constraint([6,7], operator.ne, 2,True),\n",
        "#     Constraint([6,8], operator.ne, 2,True),\n",
        "#     Constraint([6,9], operator.ne, 2,True),\n",
        "#     Constraint([6,10], operator.ne, 2,True),\n",
        "#     Constraint([7,8], operator.ne, 2,True),\n",
        "#     Constraint([7,9], operator.ne, 2,True),\n",
        "#     Constraint([7,10], operator.ne, 2,True),\n",
        "#     Constraint([8,9], operator.ne, 2,True),\n",
        "#     Constraint([8,10], operator.ne, 2,True),\n",
        "#     Constraint([9,10], operator.ne, 2,True),\n",
        "\n",
        "#     # alldiff drink 11,12,13,14,15\n",
        "#     Constraint([11,12], operator.ne, 2,True),\n",
        "#     Constraint([11,13], operator.ne, 2,True),\n",
        "#     Constraint([11,14], operator.ne, 2,True),\n",
        "#     Constraint([11,15], operator.ne, 2,True),\n",
        "#     Constraint([12,13], operator.ne, 2,True),\n",
        "#     Constraint([12,14], operator.ne, 2,True),\n",
        "#     Constraint([12,15], operator.ne, 2,True),\n",
        "#     Constraint([13,14], operator.ne, 2,True),\n",
        "#     Constraint([13,15], operator.ne, 2,True),\n",
        "#     Constraint([14,15], operator.ne, 2,True),\n",
        "\n",
        "#     # alldiff country 16,17,18,19,20\n",
        "#     Constraint([16,17], operator.ne, 2,True),\n",
        "#     Constraint([16,18], operator.ne, 2,True),\n",
        "#     Constraint([16,19], operator.ne, 2,True),\n",
        "#     Constraint([16,20], operator.ne, 2,True),\n",
        "#     Constraint([17,18], operator.ne, 2,True),\n",
        "#     Constraint([17,19], operator.ne, 2,True),\n",
        "#     Constraint([17,20], operator.ne, 2,True),\n",
        "#     Constraint([18,19], operator.ne, 2,True),\n",
        "#     Constraint([18,20], operator.ne, 2,True),\n",
        "#     Constraint([19,20], operator.ne, 2,True),\n",
        "\n",
        "#     #alldiff brand 21,22,23,24,25\n",
        "#     Constraint([21,22], operator.ne, 2,True),\n",
        "#     Constraint([21,23], operator.ne, 2,True),\n",
        "#     Constraint([21,24], operator.ne, 2,True),\n",
        "#     Constraint([21,25], operator.ne, 2,True),\n",
        "#     Constraint([22,23], operator.ne, 2,True),\n",
        "#     Constraint([22,24], operator.ne, 2,True),\n",
        "#     Constraint([22,25], operator.ne, 2,True),\n",
        "#     Constraint([23,24], operator.ne, 2,True),\n",
        "#     Constraint([23,25], operator.ne, 2,True),\n",
        "#     Constraint([24,25], operator.ne, 2,True),\n",
        "\n",
        "\n",
        "#     #############################\n",
        "#      Constraint([16,1], operator.eq, 2,True),\n",
        "#      Constraint([17,6], operator.eq, 2,True),\n",
        "#      Constraint([11,2], operator.eq, 2,True),\n",
        "#      Constraint([18,12], operator.eq, 2,True),\n",
        "#      Constraint([2,3], operator.gt, 2,False),\n",
        "#      Constraint([2,3], eqDist1, 2,True),\n",
        "#      Constraint([21,7], operator.eq, 2,True),\n",
        "#      Constraint([22,4], operator.eq, 2,True),\n",
        "#      Constraint([13], eqConst, 1,False ,parameters = [3]),\n",
        "#      Constraint([19], eqConst, 1,False, parameters = [1]),\n",
        "#      Constraint([23,8], eqDist1, 2,True),\n",
        "#      Constraint([22,9], eqDist1, 2,True),\n",
        "#      Constraint([24,14], operator.eq,2,True),\n",
        "#      Constraint([20,25], operator.eq,2, True),\n",
        "#      Constraint([19,5], eqDist1, 2,True),\n",
        "#  ]"
      ],
      "metadata": {
        "id": "YYNBHI98QaXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# B = Basis(problem_data, language)\n",
        "# B.build()\n",
        "# B = B.removeDuplicates()\n",
        "# lbasis = len(B.constraints)\n",
        "\n",
        "# Target = Network(problem_data , constraints)\n",
        "# Target = Target.removeDuplicates()\n",
        "# ltarget = len(Target.constraints)\n",
        "\n",
        "# Qc = 0\n",
        "# Qa = 0\n",
        "\n",
        "# nbExamplesa = 0\n",
        "# nbExamplesc = 0\n",
        "\n",
        "\n",
        "# Exampletimes = []\n",
        "# Examplesizes = []\n",
        "\n",
        "# ClassifyTimes = []\n",
        "# NERTimes = []\n",
        "\n",
        "\n",
        "# nbClassifQueries = 0\n",
        "# nbWhatIsWrongQueries = 0\n",
        "# nbConfirmationQueries = 0\n",
        "\n",
        "# oldsize = 0\n",
        "# newsize = 0\n",
        "\n",
        "# oldsizeexamples = 0\n",
        "# newsizeexamples = 0\n",
        "\n",
        "# waittimes = []\n",
        "\n",
        "\n",
        "# ttts = time.time()\n",
        "# maxwaittimeold = time.time()\n",
        "\n",
        "\n",
        "# byNLP = 0\n",
        "# byQuAcq = 0\n",
        "\n",
        "# res = QuAcq2(B)\n",
        "# ttte = time.time() - ttts\n",
        "\n",
        "# averageGenerationTime = sum(Exampletimes)/len(Exampletimes)\n",
        "# averageExampleSize = sum(Examplesizes) / len(Examplesizes)\n",
        "\n",
        "# cumulativeWaitingUntilLearning = sum(Exampletimes[:nbExamplesa])\n",
        "# totalCumulativeWaiting = sum(Exampletimes[:nbExamplesc])"
      ],
      "metadata": {
        "id": "HcoUzm__Q5Wa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# p={\n",
        "#         \"Asolution\":res[1].solve(),\n",
        "#         \"isAccepted\":Target.isAccepted(res[1].solve())[0],\n",
        "#         \"TargetEquivalentToLearned\": Target.isEquivalentTo(res[1]),\n",
        "#         \"sizeOfL\":len(res[1].constraints),\n",
        "#         \"sizeOfBasis\":lbasis,\n",
        "#         \"sizeOfTarget\":ltarget,\n",
        "#         \"QA\":Qa,\n",
        "#         \"QC\":Qc,\n",
        "#         \"nbExamplesa\":nbExamplesa,\n",
        "#         \"nbExamplesc\":nbExamplesc,\n",
        "#         \"tmax\":max(waittimes),\n",
        "#         \"t\":averageGenerationTime,\n",
        "#         \"averageExampleSize\":averageExampleSize,\n",
        "#         \"timeA\":cumulativeWaitingUntilLearning,\n",
        "#         \"timeC\":totalCumulativeWaiting,\n",
        "#         \"runningTime\":ttte,\n",
        "#         \"byQuAcq\": byQuAcq,\n",
        "#         \"byNLP\": byNLP,\n",
        "#         \"TotalClassifyTimes\" : sum(ClassifyTimes),\n",
        "#         \"AverageClassifyTimes\" : sum(ClassifyTimes)/len(ClassifyTimes),\n",
        "#         \"TotalNERTimes\" : sum(NERTimes),\n",
        "#         \"AverageNERTimes\" : sum(NERTimes)/len(NERTimes),\n",
        "#         \"nbClassifQueries\" : nbClassifQueries,\n",
        "#         \"nbWhatIsWrongQueries\":nbWhatIsWrongQueries,\n",
        "#         \"nbConfirmationQueries\": nbConfirmationQueries\n",
        "#     }\n",
        "# p"
      ],
      "metadata": {
        "id": "OUcSYlQ2Pwx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10 runs"
      ],
      "metadata": {
        "id": "Yoh31uPeZv3m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "filepath = \"/content/constraintsNLPzebra!N!.txt\"\n",
        "coordspath = \"/content/zebraCoods.txt\"\n",
        "\n",
        "nlpConstraints10files = []\n",
        "for i in range(10):\n",
        "  nlpConstraints10files.append([])\n",
        "  tmp = re.sub(\"!N!\",str(i+1),filepath)\n",
        "  with open(tmp, \"r\") as f:\n",
        "    for l in f.readlines():\n",
        "      nlpConstraints10files[i].append(l)\n",
        "\n",
        "coordsALL = []\n",
        "with open(coordspath, \"rb\") as f:\n",
        "  coordsALL = pickle.load(f)\n",
        "\n",
        "print(coordsALL)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VaZ0iKU5ZvrS",
        "outputId": "63945949-806e-4226-bcaa-452c5cfd699a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'scope': (1, 2), 'rel': <built-in function ne>, 'indice': 0}, {'scope': (1, 3), 'rel': <built-in function ne>, 'indice': 1}, {'scope': (1, 4), 'rel': <built-in function ne>, 'indice': 2}, {'scope': (1, 5), 'rel': <built-in function ne>, 'indice': 3}, {'scope': (2, 3), 'rel': <built-in function ne>, 'indice': 4}, {'scope': (2, 4), 'rel': <built-in function ne>, 'indice': 5}, {'scope': (2, 5), 'rel': <built-in function ne>, 'indice': 6}, {'scope': (3, 4), 'rel': <built-in function ne>, 'indice': 7}, {'scope': (3, 5), 'rel': <built-in function ne>, 'indice': 8}, {'scope': (4, 5), 'rel': <built-in function ne>, 'indice': 9}, {'scope': (6, 7), 'rel': <built-in function ne>, 'indice': 10}, {'scope': (6, 8), 'rel': <built-in function ne>, 'indice': 11}, {'scope': (6, 9), 'rel': <built-in function ne>, 'indice': 12}, {'scope': (6, 10), 'rel': <built-in function ne>, 'indice': 13}, {'scope': (7, 8), 'rel': <built-in function ne>, 'indice': 14}, {'scope': (7, 9), 'rel': <built-in function ne>, 'indice': 15}, {'scope': (7, 10), 'rel': <built-in function ne>, 'indice': 16}, {'scope': (8, 9), 'rel': <built-in function ne>, 'indice': 17}, {'scope': (8, 10), 'rel': <built-in function ne>, 'indice': 18}, {'scope': (9, 10), 'rel': <built-in function ne>, 'indice': 19}, {'scope': (11, 12), 'rel': <built-in function ne>, 'indice': 20}, {'scope': (11, 13), 'rel': <built-in function ne>, 'indice': 21}, {'scope': (11, 14), 'rel': <built-in function ne>, 'indice': 22}, {'scope': (11, 15), 'rel': <built-in function ne>, 'indice': 23}, {'scope': (12, 13), 'rel': <built-in function ne>, 'indice': 24}, {'scope': (12, 14), 'rel': <built-in function ne>, 'indice': 25}, {'scope': (12, 15), 'rel': <built-in function ne>, 'indice': 26}, {'scope': (13, 14), 'rel': <built-in function ne>, 'indice': 27}, {'scope': (13, 15), 'rel': <built-in function ne>, 'indice': 28}, {'scope': (14, 15), 'rel': <built-in function ne>, 'indice': 29}, {'scope': (16, 17), 'rel': <built-in function ne>, 'indice': 30}, {'scope': (16, 18), 'rel': <built-in function ne>, 'indice': 31}, {'scope': (16, 19), 'rel': <built-in function ne>, 'indice': 32}, {'scope': (16, 20), 'rel': <built-in function ne>, 'indice': 33}, {'scope': (17, 18), 'rel': <built-in function ne>, 'indice': 34}, {'scope': (17, 19), 'rel': <built-in function ne>, 'indice': 35}, {'scope': (17, 20), 'rel': <built-in function ne>, 'indice': 36}, {'scope': (18, 19), 'rel': <built-in function ne>, 'indice': 37}, {'scope': (18, 20), 'rel': <built-in function ne>, 'indice': 38}, {'scope': (19, 20), 'rel': <built-in function ne>, 'indice': 39}, {'scope': (21, 22), 'rel': <built-in function ne>, 'indice': 40}, {'scope': (21, 23), 'rel': <built-in function ne>, 'indice': 41}, {'scope': (21, 24), 'rel': <built-in function ne>, 'indice': 42}, {'scope': (21, 25), 'rel': <built-in function ne>, 'indice': 43}, {'scope': (22, 23), 'rel': <built-in function ne>, 'indice': 44}, {'scope': (22, 24), 'rel': <built-in function ne>, 'indice': 45}, {'scope': (22, 25), 'rel': <built-in function ne>, 'indice': 46}, {'scope': (23, 24), 'rel': <built-in function ne>, 'indice': 47}, {'scope': (23, 25), 'rel': <built-in function ne>, 'indice': 48}, {'scope': (24, 25), 'rel': <built-in function ne>, 'indice': 49}, {'scope': (16, 1), 'rel': <built-in function eq>, 'indice': 50}, {'scope': (17, 6), 'rel': <built-in function eq>, 'indice': 51}, {'scope': (11, 2), 'rel': <built-in function eq>, 'indice': 52}, {'scope': (18, 12), 'rel': <built-in function eq>, 'indice': 53}, {'scope': (21, 7), 'rel': <built-in function eq>, 'indice': 54}, {'scope': (22, 4), 'rel': <built-in function eq>, 'indice': 55}, {'scope': (24, 14), 'rel': <built-in function eq>, 'indice': 56}, {'scope': (20, 25), 'rel': <built-in function eq>, 'indice': 57}, {'scope': (2, 3), 'rel': <built-in function gt>, 'indice': 58}, {'scope': (2, 3), 'rel': <function eqDist1 at 0x7a060c349fc0>, 'indice': 59}, {'scope': (23, 8), 'rel': <function eqDist1 at 0x7a060c349fc0>, 'indice': 60}, {'scope': (22, 9), 'rel': <function eqDist1 at 0x7a060c349fc0>, 'indice': 61}, {'scope': (19, 5), 'rel': <function eqDist1 at 0x7a060c349fc0>, 'indice': 62}, {'scope': (13,), 'rel': <function eqConst at 0x7a060c34a830>, 'param': [3], 'indice': 63}, {'scope': (19,), 'rel': <function eqConst at 0x7a060c34a830>, 'param': [1], 'indice': 64}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "calls= 0\n",
        "\n",
        "from random import random\n",
        "\n",
        "vars_ids =   [   1 ,   2   ,  3    ,  4     ,   5  ,  6  ,    7   ,   8 ,    9  ,   10  ,   11   ,  12 ,  13  ,  14          ,   15  ,  16     , 17       ,18        ,19          ,20        ,21  , 22, 23,24 ,25]\n",
        "vars_names = [\"red\",\"green\",\"ivory\",\"yellow\",\"blue\",\"dog\",\"snails\",\"fox\",\"horse\",\"zebra\",\"coffee\",\"tea\",\"milk\",\"orange juice\",\"water\",\"English\",\"Sparniad\",\"Ukranian\",\"Norweigian\",\"Japanese\",\"Old Gold\",\"Kools\",\"Chesterfields\",\"Lucky Strike\",\"Parliaments\"]\n",
        "vars_types = [\"color\",\"color\",\"color\",\"color\",\"color\",\"pet\",\"pet\",\"pet\",\"pet\",\"pet\",\"drink\",\"drink\",\"drink\",\"drink\",\"drink\",\"country\",\"country\",\"country\",\"country\",\"country\",\"brand\",\"brand\",\"brand\",\"brand\",\"brand\"]\n",
        "vars_domains = [(1,5),(1,5),(1,5),(1,5),(1,5),(1,5),(1,5),(1,5),(1,5),(1,5),(1,5),(1,5),(1,5),(1,5),(1,5),(1,5),(1,5),(1,5),(1,5),(1,5),(1,5),(1,5),(1,5),(1,5),(1,5)]\n",
        "types = [\"color\",\"pet\",\"drink\", \"country\",\"brand\"]\n",
        "\n",
        "\n",
        "problem_data = {\n",
        "  id: {\n",
        "      \"domain\": vars_domains[id-1],\n",
        "       \"type\": vars_types[id-1],\n",
        "       \"name\": vars_names[id-1]\n",
        "  } for id in vars_ids\n",
        "}\n",
        "\n",
        "\n",
        "ressss = []\n",
        "\n",
        "vals = [1,2,3,4,5]\n",
        "ops = [operator.ge,operator.le,operator.lt,operator.gt,operator.ne,operator.eq]\n",
        "\n",
        "\n",
        "language = [ (operator.ge,2, False,None),\n",
        "            (operator.le,2, False,None),\n",
        "            (operator.lt,2, False,None),\n",
        "            (operator.gt,2, False,None),\n",
        "            (operator.ne,2,True,None),\n",
        "            (operator.eq,2,True,None),\n",
        "            (eqConst,1, False,[1,2,3,4,5]),\n",
        "            (geConst,1, False,[1,2,3,4,5]),\n",
        "            (leConst,1, False,[1,2,3,4,5]),\n",
        "            (ltConst,1, False,[1,2,3,4,5]),\n",
        "            (gtConst,1, False,[1,2,3,4,5]),\n",
        "            (neConst,1, False,[1,2,3,4,5]),\n",
        "            (eqDist1,2,True,None),\n",
        "            (neDist1,2,True,None)\n",
        "            ]\n",
        "constraints = [\n",
        "\n",
        "  # alldiff colors\n",
        "  Constraint([1,2], operator.ne, 2,True),\n",
        "  Constraint([1,3], operator.ne, 2,True),\n",
        "  Constraint([1,4], operator.ne, 2,True),\n",
        "  Constraint([1,5], operator.ne, 2,True),\n",
        "  Constraint([2,3], operator.ne, 2,True),\n",
        "  Constraint([2,4], operator.ne, 2,True),\n",
        "  Constraint([2,5], operator.ne, 2,True),\n",
        "  Constraint([3,4], operator.ne, 2,True),\n",
        "  Constraint([3,5], operator.ne, 2,True),\n",
        "  Constraint([4,5], operator.ne, 2,True),\n",
        "\n",
        "  # alldiff pets 6,7,8,9,10\n",
        "  Constraint([6,7], operator.ne, 2,True),\n",
        "  Constraint([6,8], operator.ne, 2,True),\n",
        "  Constraint([6,9], operator.ne, 2,True),\n",
        "  Constraint([6,10], operator.ne, 2,True),\n",
        "  Constraint([7,8], operator.ne, 2,True),\n",
        "  Constraint([7,9], operator.ne, 2,True),\n",
        "  Constraint([7,10], operator.ne, 2,True),\n",
        "  Constraint([8,9], operator.ne, 2,True),\n",
        "  Constraint([8,10], operator.ne, 2,True),\n",
        "  Constraint([9,10], operator.ne, 2,True),\n",
        "\n",
        "  # alldiff drink 11,12,13,14,15\n",
        "  Constraint([11,12], operator.ne, 2,True),\n",
        "  Constraint([11,13], operator.ne, 2,True),\n",
        "  Constraint([11,14], operator.ne, 2,True),\n",
        "  Constraint([11,15], operator.ne, 2,True),\n",
        "  Constraint([12,13], operator.ne, 2,True),\n",
        "  Constraint([12,14], operator.ne, 2,True),\n",
        "  Constraint([12,15], operator.ne, 2,True),\n",
        "  Constraint([13,14], operator.ne, 2,True),\n",
        "  Constraint([13,15], operator.ne, 2,True),\n",
        "  Constraint([14,15], operator.ne, 2,True),\n",
        "\n",
        "  # alldiff country 16,17,18,19,20\n",
        "  Constraint([16,17], operator.ne, 2,True),\n",
        "  Constraint([16,18], operator.ne, 2,True),\n",
        "  Constraint([16,19], operator.ne, 2,True),\n",
        "  Constraint([16,20], operator.ne, 2,True),\n",
        "  Constraint([17,18], operator.ne, 2,True),\n",
        "  Constraint([17,19], operator.ne, 2,True),\n",
        "  Constraint([17,20], operator.ne, 2,True),\n",
        "  Constraint([18,19], operator.ne, 2,True),\n",
        "  Constraint([18,20], operator.ne, 2,True),\n",
        "  Constraint([19,20], operator.ne, 2,True),\n",
        "\n",
        "  #alldiff brand 21,22,23,24,25\n",
        "  Constraint([21,22], operator.ne, 2,True),\n",
        "  Constraint([21,23], operator.ne, 2,True),\n",
        "  Constraint([21,24], operator.ne, 2,True),\n",
        "  Constraint([21,25], operator.ne, 2,True),\n",
        "  Constraint([22,23], operator.ne, 2,True),\n",
        "  Constraint([22,24], operator.ne, 2,True),\n",
        "  Constraint([22,25], operator.ne, 2,True),\n",
        "  Constraint([23,24], operator.ne, 2,True),\n",
        "  Constraint([23,25], operator.ne, 2,True),\n",
        "  Constraint([24,25], operator.ne, 2,True),\n",
        "\n",
        "\n",
        "  #############################\n",
        "    Constraint([16,1], operator.eq, 2,True),\n",
        "    Constraint([17,6], operator.eq, 2,True),\n",
        "    Constraint([11,2], operator.eq, 2,True),\n",
        "    Constraint([18,12], operator.eq, 2,True),\n",
        "    Constraint([2,3], operator.gt, 2,False),\n",
        "    Constraint([2,3], eqDist1, 2,True),\n",
        "    Constraint([21,7], operator.eq, 2,True),\n",
        "    Constraint([22,4], operator.eq, 2,True),\n",
        "    Constraint([13], eqConst, 1,False ,parameters = [3]),\n",
        "    Constraint([19], eqConst, 1,False, parameters = [1]),\n",
        "    Constraint([23,8], eqDist1, 2,True),\n",
        "    Constraint([22,9], eqDist1, 2,True),\n",
        "    Constraint([24,14], operator.eq,2,True),\n",
        "    Constraint([20,25], operator.eq,2, True),\n",
        "    Constraint([19,5], eqDist1, 2,True),\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Borigin = Basis(problem_data, language)\n",
        "Borigin.build()\n",
        "Borigin = Borigin.removeDuplicates()\n",
        "lbasis = len(Borigin.constraints)\n",
        "\n",
        "\n",
        "Targetorigin = Network(problem_data , constraints)\n",
        "Targetorigin = Targetorigin.removeDuplicates()\n",
        "ltarget = len(Targetorigin.constraints)\n",
        "\n",
        "\n",
        "for i in range(10):\n",
        "\n",
        "\n",
        "\n",
        "  nlpConstraints = nlpConstraints10files[i]\n",
        "  coords = deepcopy(coordsALL)\n",
        "\n",
        "  B = deepcopy(Borigin)\n",
        "  Bbk = deepcopy(B)\n",
        "  Target = deepcopy(Targetorigin)\n",
        "\n",
        "  Qc = 0\n",
        "  Qa = 0\n",
        "\n",
        "  nbExamplesa = 0\n",
        "  nbExamplesc = 0\n",
        "\n",
        "\n",
        "  Exampletimes = []\n",
        "  Examplesizes = []\n",
        "\n",
        "  ClassifyTimes = []\n",
        "  NERTimes = []\n",
        "\n",
        "\n",
        "  nbWhatIsWrongQueriesA = 0\n",
        "  nbWhatIsWrongQueriesC = 0\n",
        "\n",
        "  nbConfirmationQueriesA = 0\n",
        "  nbConfirmationQueriesC = 0\n",
        "\n",
        "  nbReformulationQueriesA = 0\n",
        "  nbReformulationQueriesC = 0\n",
        "\n",
        "  oldsize = 0\n",
        "  newsize = 0\n",
        "\n",
        "  oldsizeexamples = 0\n",
        "  newsizeexamples = 0\n",
        "\n",
        "  oldsizenlpqueries1 = 0\n",
        "  newsizenlpqueries1 = 0\n",
        "\n",
        "  oldsizenlpqueries2 = 0\n",
        "  newsizenlpqueries2 = 0\n",
        "\n",
        "  oldsizenlpqueries3 = 0\n",
        "  newsizenlpqueries3 = 0\n",
        "\n",
        "  waittimes = []\n",
        "\n",
        "\n",
        "  ttts = time.time()\n",
        "  maxwaittimeold = time.time()\n",
        "\n",
        "\n",
        "  byNLP = 0\n",
        "  byQuAcq = 0\n",
        "\n",
        "  res = QuAcq2(B)\n",
        "  ttte = time.time() - ttts\n",
        "\n",
        "  averageGenerationTime = sum(Exampletimes)/len(Exampletimes)\n",
        "  averageExampleSize = sum(Examplesizes) / len(Examplesizes)\n",
        "\n",
        "  cumulativeWaitingUntilLearning = sum(Exampletimes[:nbExamplesa])\n",
        "  totalCumulativeWaiting = sum(Exampletimes[:nbExamplesc])\n",
        "\n",
        "\n",
        "  ressss.append({\n",
        "        \"Asolution\":res[1].solve(),\n",
        "        \"isAccepted\":Target.isAccepted(res[1].solve())[0],\n",
        "        \"TargetEquivalentToLearned\": Target.isEquivalentTo(res[1]),\n",
        "        \"Learned\": res[1],\n",
        "        \"sizeOfL\":len(res[1].constraints),\n",
        "        \"sizeOfBasis\":lbasis,\n",
        "        \"sizeOfTarget\":ltarget,\n",
        "        \"QA\":Qa,\n",
        "        \"QC\":Qc,\n",
        "        \"nbWhatIsWrongQueriesA\":nbWhatIsWrongQueriesA,\n",
        "        \"nbWhatIsWrongQueriesC\":nbWhatIsWrongQueriesC,\n",
        "        \"nbConfirmationQueriesA\":nbConfirmationQueriesA,\n",
        "        \"nbConfirmationQueriesC\":nbConfirmationQueriesC,\n",
        "        \"nbReformulationQueriesA\":nbReformulationQueriesA,\n",
        "        \"nbReformulationQueriesC\":nbReformulationQueriesC,\n",
        "        \"nbExamplesa\":nbExamplesa,\n",
        "        \"nbExamplesc\":nbExamplesc,\n",
        "        \"tmax\":max(waittimes),\n",
        "        \"t\":averageGenerationTime,\n",
        "        \"averageExampleSize\":averageExampleSize,\n",
        "        \"timeA\":cumulativeWaitingUntilLearning,\n",
        "        \"timeC\":totalCumulativeWaiting,\n",
        "        \"runningTime\":ttte,\n",
        "        \"byQuAcq\": byQuAcq,\n",
        "        \"byNLP\": byNLP,\n",
        "        \"TotalClassifyTimes\" : sum(ClassifyTimes),\n",
        "        \"AverageClassifyTimes\" : sum(ClassifyTimes)/len(ClassifyTimes),\n",
        "        \"TotalNERTimes\" : sum(NERTimes),\n",
        "        \"AverageNERTimes\" : sum(NERTimes)/len(NERTimes)\n",
        "    })\n",
        "\n",
        "  print(\"\\n\\n\\n Another entry was added to ressss, filenb== \",str(i+1),\"\\n\\n\\n\")"
      ],
      "metadata": {
        "id": "0rJOGddZjQvt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "with open(\"reszebra10runs10files.txt\", \"wb\") as f:\n",
        "  pickle.dump(ressss,f)\n"
      ],
      "metadata": {
        "id": "nObadu2Zju7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XjuA6xaxRqsZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "zzVaWjnlfb1I",
        "jabFMsmEhKBV",
        "aXwibIv4hONW",
        "s3uK3M_AhVHo",
        "JMi-6KSkhZvi",
        "YKjgLz63hZjC",
        "RGeBS2ePhZXR",
        "aVvmtYoCpBcD",
        "pqAFjDQmpBK1",
        "Kz6vcOkOILR1",
        "-jDUuzVs1kdT",
        "Czg3LXM51kG0",
        "lngFHC6GqPy7",
        "ti2rEXqCXR8k",
        "mRTZvDY_XWe_",
        "Pp4pJYa1XZ9e",
        "FFWAJzz7sfyu",
        "etxz10Ag6jw2",
        "6KBFyBPL9LWN"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5da4096a24ae4691a3d20bcca9278272": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c8b2e4e980f04257b9876af1d5f4d365",
              "IPY_MODEL_1bf04e4767c24979aa93d8da6b5cfb6a",
              "IPY_MODEL_b8fcefa300c0407aa69878e6405bbe13"
            ],
            "layout": "IPY_MODEL_2eabec30ec5346dfb53861b1984ed398"
          }
        },
        "c8b2e4e980f04257b9876af1d5f4d365": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f4fc4579be3347aeb32c155d58e4bf36",
            "placeholder": "​",
            "style": "IPY_MODEL_bc56505050b74125a0fb11b042b30b06",
            "value": "Downloading (…)solve/main/vocab.txt: 100%"
          }
        },
        "1bf04e4767c24979aa93d8da6b5cfb6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ab5f5a61e7ff420e825b3f012d0e0259",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_af408fcfb7744de08db2f7babcd4d154",
            "value": 231508
          }
        },
        "b8fcefa300c0407aa69878e6405bbe13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed968e3a84874874afb11cb7c1941857",
            "placeholder": "​",
            "style": "IPY_MODEL_8325c4009258417fb79517f9dab44ae9",
            "value": " 232k/232k [00:00&lt;00:00, 476kB/s]"
          }
        },
        "2eabec30ec5346dfb53861b1984ed398": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f4fc4579be3347aeb32c155d58e4bf36": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc56505050b74125a0fb11b042b30b06": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ab5f5a61e7ff420e825b3f012d0e0259": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af408fcfb7744de08db2f7babcd4d154": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ed968e3a84874874afb11cb7c1941857": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8325c4009258417fb79517f9dab44ae9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "65fee762246148c393bce4667f6c9397": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_964b43c83d1d493087ec6ce35a854689",
              "IPY_MODEL_1495e1f5cfb5412d98ea261be651bcc1",
              "IPY_MODEL_561b76c2616743f192632ab825c16056"
            ],
            "layout": "IPY_MODEL_491cc9441174445d9a00566e9628245c"
          }
        },
        "964b43c83d1d493087ec6ce35a854689": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_63c12f8a94d8446fb569dd238be1cc6d",
            "placeholder": "​",
            "style": "IPY_MODEL_cf2f21f3dc11484c947774ca097dc895",
            "value": "Downloading (…)okenizer_config.json: 100%"
          }
        },
        "1495e1f5cfb5412d98ea261be651bcc1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7c265db26c18465587a074d2a1bb1b91",
            "max": 28,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b95a569e4e404ebc8bdbe77aa5d581f9",
            "value": 28
          }
        },
        "561b76c2616743f192632ab825c16056": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7c5bde83780e438a9d1e96919b16d9ba",
            "placeholder": "​",
            "style": "IPY_MODEL_12ce4959d8b64323820af60036dcc310",
            "value": " 28.0/28.0 [00:00&lt;00:00, 1.72kB/s]"
          }
        },
        "491cc9441174445d9a00566e9628245c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "63c12f8a94d8446fb569dd238be1cc6d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf2f21f3dc11484c947774ca097dc895": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7c265db26c18465587a074d2a1bb1b91": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b95a569e4e404ebc8bdbe77aa5d581f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7c5bde83780e438a9d1e96919b16d9ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "12ce4959d8b64323820af60036dcc310": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "edadf3227c074e01af7471fe04c2063f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1becd7b8b8984db68a9d7230ccec62a1",
              "IPY_MODEL_e693dfe2f25b4e0a8916ab0b1d656c63",
              "IPY_MODEL_6474146c3499474c86093d1eb6e4a642"
            ],
            "layout": "IPY_MODEL_61c18dd60c0a4931ac5275fa9d511075"
          }
        },
        "1becd7b8b8984db68a9d7230ccec62a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4b233aca09d2451398339fdf68e56b7a",
            "placeholder": "​",
            "style": "IPY_MODEL_b87676ae6c11497a97737e6982342506",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "e693dfe2f25b4e0a8916ab0b1d656c63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7ffb368a12c44ea48026df7f1f22e74e",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6007dce4162949438efa923fc059e12b",
            "value": 570
          }
        },
        "6474146c3499474c86093d1eb6e4a642": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ba92eefb1d5f4ab2a17144c168e50110",
            "placeholder": "​",
            "style": "IPY_MODEL_26d16d1fb7714e869a7574aa822ab522",
            "value": " 570/570 [00:00&lt;00:00, 42.6kB/s]"
          }
        },
        "61c18dd60c0a4931ac5275fa9d511075": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b233aca09d2451398339fdf68e56b7a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b87676ae6c11497a97737e6982342506": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7ffb368a12c44ea48026df7f1f22e74e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6007dce4162949438efa923fc059e12b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ba92eefb1d5f4ab2a17144c168e50110": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26d16d1fb7714e869a7574aa822ab522": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a957ca83c4fd48c5a5da2384fbc6afd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bf5b740c76094c27a40e5b5ffa4e5923",
              "IPY_MODEL_5fd1066b1f2e490dae7fe7956a70b945",
              "IPY_MODEL_5294c7640a7147548782deeea8ff2a3c"
            ],
            "layout": "IPY_MODEL_c6626b7e561348be868629d9ee5fd286"
          }
        },
        "bf5b740c76094c27a40e5b5ffa4e5923": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_343861c4eb7a486fb665fa63aaae697d",
            "placeholder": "​",
            "style": "IPY_MODEL_95b7aeaa7c9748278f3dfe2200233d8a",
            "value": "Downloading model.safetensors: 100%"
          }
        },
        "5fd1066b1f2e490dae7fe7956a70b945": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef1109521d064012ac8d0df67448ee34",
            "max": 440449768,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3575b6f569b9479b8cee46ea065962b1",
            "value": 440449768
          }
        },
        "5294c7640a7147548782deeea8ff2a3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9b2f183d9d9849ab8370c790c6f2061c",
            "placeholder": "​",
            "style": "IPY_MODEL_7cb0800845ee42afab867ba000d8194b",
            "value": " 440M/440M [00:01&lt;00:00, 281MB/s]"
          }
        },
        "c6626b7e561348be868629d9ee5fd286": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "343861c4eb7a486fb665fa63aaae697d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "95b7aeaa7c9748278f3dfe2200233d8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ef1109521d064012ac8d0df67448ee34": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3575b6f569b9479b8cee46ea065962b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9b2f183d9d9849ab8370c790c6f2061c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7cb0800845ee42afab867ba000d8194b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}